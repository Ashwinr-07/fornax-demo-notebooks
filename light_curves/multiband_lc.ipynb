{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make multiwavelength light curves using archival data\n",
    "\n",
    "### Summary:\n",
    " - model plots after van Velzen et al. 2021, https://arxiv.org/pdf/2111.09391.pdf\n",
    " \n",
    "### Input:\n",
    " - a catalog of CLAGN from the literature\n",
    "\n",
    "### Output:\n",
    " - an archival optical + IR + neutrino light curve\n",
    " \n",
    "### Technical Goals:\n",
    " - should be able to run from a clean checkout from github\n",
    " - should be able to automatically download all catalogs & images used\n",
    " - need to have all photometry in the same physical unit\n",
    " - need to have a data structure that is easy to use but holds light curve information (time and units) and is extendable to ML applications\n",
    " - need to have a curated list of catalogs to search for photometry that is generalizeable to other input catalogs\n",
    " \n",
    "### Authors:\n",
    "IPAC SP team\n",
    "\n",
    "### Acknowledgements:\n",
    "Suvi Gezari, Antara Basu-zych,\n",
    "MAST, HEASARC, & IRSA Fornax teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import axs\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import requests\n",
    "import seaborn as sns\n",
    "\n",
    "from astroquery.ipac.ned import Ned\n",
    "from astroquery.heasarc import Heasarc\n",
    "from astroquery.gaia import Gaia\n",
    "\n",
    "from astropy.coordinates import SkyCoord\n",
    "import astropy.units as u\n",
    "from astropy.table import Table, vstack, hstack, unique\n",
    "from astropy.io import ascii\n",
    "\n",
    "\n",
    "try: # Python 3.x\n",
    "    from urllib.parse import quote as urlencode\n",
    "    from urllib.request import urlretrieve\n",
    "except ImportError:  # Python 2.x\n",
    "    from urllib import pathname2url as urlencode\n",
    "    from urllib import urlretrieve\n",
    "\n",
    "try: # Python 3.x\n",
    "    import http.client as httplib \n",
    "except ImportError:  # Python 2.x\n",
    "    import httplib   \n",
    "\n",
    "!pip install lightkurve --upgrade\n",
    "import lightkurve as lk\n",
    "\n",
    "!pip install ztfquery\n",
    "from ztfquery import lightcurve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define the Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the following paper to make a sample of CLAGN: https://iopscience.iop.org/article/10.3847/1538-4357/aaca3a \n",
    "\n",
    "# This sample can later be switched out to a differen/larger sample of \"interesting\" targets\n",
    "\n",
    "#use ADS to find the refcode for this paper\n",
    "CLAGN = Ned.query_refcode('2018ApJ...862..109Y')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLAGN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the best data structure for this work?\n",
    " - list of requirements is being kept here: https://github.com/fornax-navo/fornax-demo-notebooks/issues/69 \n",
    " - some things to keep an eye on as other people are actively working on this field\n",
    "     - astropy has a light curve class\n",
    "         -would require development work to make this work for multiwavelength application\n",
    "     - LINCC people are interested in this and might have some suggestions on a 6mo. timescale\n",
    "     - xarray\n",
    "     - pandas pint has units support but also has a warning that it doesn't yet work perfectly\n",
    "     - lightKurve is not suitable for this application\n",
    "     - sunpy is also not suitable for this application\n",
    "\n",
    "### Since there is nothing perfectly ready now, we need to go with something practical for the time being\n",
    " - instead of one large dataframe with the multiwavelength information, we could keep them as seperate astropy light curves for each band, do the feature extraction on each light curve and keep the features in one large dataframe. - how would we link targets between bands?\n",
    " - ZTF keeps the light curve info as multidimensional arrays in pandas columns - this works out of the box but doesn't have unit support so we just need to do that manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(CLAGN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Build a list of skycoords from target ra and dec #####\n",
    "coords_list = [\n",
    "    SkyCoord(ra, dec, frame='icrs', unit='deg')\n",
    "    for ra, dec in zip(CLAGN['RA'], CLAGN['DEC'])\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Find light curves for these targets in NASA catalogs\n",
    "- look at NAVO use cases to get help with tools to do this - although they mostly use pyvo\n",
    "- deciding up front to use astroquery instead of pyvo\n",
    "    - astroquery is apparently more user friendly\n",
    "- data access concerns:\n",
    "    - can't ask the archives to search their entire holdings\n",
    "        - not good enough meta data\n",
    "        - not clear that the data is all vetted and good enough to include for science\n",
    "        - all catalogs have differently named columns so how would we know which columns to keep\n",
    "    - instead work with a curated list of catalogs for each archive\n",
    "        - focus on general surveys\n",
    "        - try to ensure that this list is also appropriate for a generalization of this use case to other input catalogs\n",
    "        - could astroquery.NED be useful in finding a generalized curated list\n",
    "- How do we know we have a match that is good enough to include in our light curve\n",
    "     - look at nway for the high energy catalogs\n",
    "     - probably need to generate a table of search radii for each catalog based on bandpass\n",
    "         - need domain knowledge for that\n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 HEASARC: FERMI & Beppo SAX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mission_list = ['FERMIGTRIG', 'SAXGRBMGRB']\n",
    "radius = 0.1*u.degree\n",
    "\n",
    "for ccount, coord in enumerate(coords_list):\n",
    "    #use astroquery to search that position for either a Fermi or Beppo Sax trigger\n",
    "    for mcount, mission in enumerate(mission_list):\n",
    "        try:\n",
    "            results = heasarc.query_region(coord, mission = mission, radius = radius)#, sortvar = 'SEARCH_OFFSET_')\n",
    "            print (\"got a live one\")\n",
    "            #need to figure out what this result would look like and how to add that to the saved data structure\n",
    "        except AttributeError:\n",
    "            print(\"no results at that location for \", mission)\n",
    "\n",
    "\n",
    "#**** These HEASARC searches are returning an attribute error because of an astroquery bug\n",
    "# bug submitted to astroquery Oct 18, waiting for a fix.\n",
    "# if that gets fixed, can probably change this cell "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 IRSA: ZTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python package ztfquery is not a good solution for this because it requires IRSA password\n",
    "#Instead will construct the URL for an API query\n",
    "#https://irsa.ipac.caltech.edu/docs/program_interface/ztf_lightcurve_api.html\n",
    "ztf_radius = 0.000278   #as suggested by Dave Shupe\n",
    "\n",
    "for ccount, coord in enumerate(coords_list):\n",
    "    #doesn't take SkyCoord\n",
    "    ra = CLAGN['RA'][ccount]\n",
    "    dec = CLAGN['DEC'][ccount]\n",
    "    #make the string for the URL query\n",
    "    #ask for all three bands (g, r, i)\n",
    "    #don't want data that is flagged as unusable by the ZTF pipeline\n",
    "    urlstr = 'https://irsa.ipac.caltech.edu/cgi-bin/ZTF/nph_light_curves?POS=CIRCLE %f %f %f&BANDNAME=g,r,i&FORMAT=ipac_table&BAD_CATFLAGS_MASK=32768'%(ra, dec,ztf_radius)\n",
    "\n",
    "    response = requests.get(urlstr)\n",
    "    if response.ok:\n",
    "        ztf_lc = ascii.read(response.text, format='ipac')\n",
    "        #print(count, len(ztf_lc))\n",
    "        #this could be up to 3 light curves because there are 3 filters\n",
    "        #need to sort by filtercode 'zg','zr','zi'\n",
    "        #and store the light curves\n",
    "    else:\n",
    "        print(ccount, \" There is no ZTF light curve at this position\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 IRSA:WISE\n",
    "\n",
    "- Dave Shupe has made a catalog of neowise light curves of half the sky in a parquet file\n",
    "\n",
    "- Pandas is not a good option for working with this catalog because it is so large (2 billion rows?)\n",
    "\n",
    "- Instead we can use AXS to cross match the CLAGN sample with the neowise catalog to find those rows in neowise which correspond to the CLAGN sample. AXS is a part of spark. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#could load the neowise light curves into pandas, but would need to severely\n",
    "# filter the catalog to get it to fit into memory.  Since these targets are all over the sky\n",
    "# it is not obvious how to filter the catalog\n",
    "\n",
    "#Here is one way it could work in Pandas if we had a way to filter significantly before matching\n",
    "#subset = pd.read_parquet('/stage/irsa-data-download10/parquet-work/NEOWISE-R/neowise_lc_half.parquet',\n",
    "#                    engine='pyarrow', \n",
    "#                    filters=[ ('ra', '<', 121) , ('ra', '>', 120) , \n",
    "#                            ('dec', '<', 68) , ('dec', '>', -9),\n",
    "#                            ('cw_w1mpro', '>', 15.0) ])\n",
    "#\n",
    "#len(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start up SPARK\n",
    "os.environ['SPARK_CONF_DIR'] = '/home/jkrick/axs_store/conf_alt'\n",
    "\n",
    "def spark_start(work_dir, database_dir, warehouse_dir):\n",
    "    from pyspark.sql import SparkSession\n",
    "    import os\n",
    "    \n",
    "    spark = (\n",
    "            SparkSession.builder\n",
    "            .appName(\"spark trial\")\n",
    "            .config(\"spark.sql.warehouse.dir\", warehouse_dir)\n",
    "            .config('spark.master', \"local[20]\")\n",
    "            .config('spark.driver.memory', '64G') # 128\n",
    "            .config('spark.executor.memory', '30G')\n",
    "            .config('spark.local.dir', work_dir)\n",
    "            .config('spark.memory.offHeap.enabled', 'true')\n",
    "            .config('spark.memory.offHeap.size', '128G') # 256\n",
    "            .config(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "            .config(\"spark.driver.maxResultSize\", \"60G\")\n",
    "            .config(\"spark.driver.extraJavaOptions\", \n",
    "                    f\"-Dderby.system.home={database_dir}\")\n",
    "            .config(\"spark.sql.hive.metastore.sharedPrefixes\",\n",
    "                    \"org.apache.derby\")\n",
    "            .enableHiveSupport()\n",
    "            .getOrCreate()\n",
    "                    )   \n",
    "\n",
    "    return spark\n",
    "\n",
    "spark_session = spark_start(\n",
    "    \"/stage/irsa-staff-jkrick/spark_work\",\n",
    "    \"/home/jkrick/axs_store\",\n",
    "    \"/stage/irsa-staff-jkrick/sp_axs_warehouse/warehouse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if the one we want is not yet available, add it to the list\n",
    "catalog = axs.AxsCatalog(spark_session)\n",
    "catlist = catalog.list_table_names()\n",
    "\n",
    "if 'neowise_lc_half' not in catlist:\n",
    "    catalog.import_existing_table('neowise_lc_half', \n",
    "        '/stage/irsa-data-download10/parquet-work/NEOWISE-R/neowise_lc_half.parquet',\n",
    "        import_into_spark=True)\n",
    "    \n",
    "\n",
    "#now figure out how to get the CLAGN catalog into AXS\n",
    "#can't go direct from astropy table into AXS, so first to pandas\n",
    "\n",
    "if 'axs_clagn' not in catlist:\n",
    "\n",
    "    pd_CLAGN = CLAGN.to_pandas()\n",
    "\n",
    "    #then pandas to spark dataframe\n",
    "    sp_CLAGN = spark_session.createDataFrame(pd_CLAGN)\n",
    "\n",
    "    #ok, saving below can't handle capital \"RA\" and \"DEC\", so need to change that\n",
    "    #also can't handle column names with spaces in them so need to rename those as well.\n",
    "    sp_CLAGN2 = sp_CLAGN.withColumnRenamed(\"RA\",\"ra\").withColumnRenamed(\"DEC\",\"dec\").withColumnRenamed(\"Object Name\", \"Object_name\").withColumnRenamed(\"Redshift Flag\",\"redshift_flag\").withColumnRenamed(\"Magnitude and Filter\", \"magnitude_and_filter\").withColumnRenamed(\"Photometry Points\",\"photometry_points\").withColumnRenamed(\"Redshift Points\", \"redshift_points\").withColumnRenamed(\"Diameter Points\",\"diameter_points\")\n",
    "\n",
    "    #now save spark to AXS\n",
    "    catalog.save_axs_table(sp_CLAGN2, 'AXS_CLAGN', calculate_zone=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just confirm that worked:\n",
    "#catalog = axs.AxsCatalog(spark_session)\n",
    "#catlist = catalog.list_table_names()\n",
    "#catlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lazy load in the NEOWISE and CLAGN catalog\n",
    "neowise_lc_half = catalog.load('neowise_lc_half')\n",
    "axs_clagn = catalog.load('axs_clagn')\n",
    "\n",
    "#rename column name which is causing problems\n",
    "axs_clagn = axs_clagn.withColumnRenamed(\"No.\", \"objnum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crossmatch CLAGN with NEOWISE\n",
    "\n",
    "neowise_CLAGN = neowise_lc_half.crossmatch(axs_clagn, 2*axs.Constants.ONE_ASEC, return_min = True, include_dist_col = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#lazy evaluation means the cross match won't happen until this cell gets executed\n",
    "#how many matches did we get?\n",
    "neowise_CLAGN.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#now get it into a format that I can handle\n",
    "#this is taking a long time 45min? for 29 rows? for all columns\n",
    "#more like 18 min for limited set of columns\n",
    "\n",
    "#make a smaller version with just the light curve info to save\n",
    "neowise_lc = neowise_CLAGN.select('objnum','Object_name','w1pmag','w1pmagerr','w2pmag','w2pmagerr', 'mjd')\n",
    "\n",
    "#convert to Pandas dataframe\n",
    "pd_neowise_lc = neowise_lc.toPandas()\n",
    "\n",
    "#instead maybe try parquet?  csv doesn't work since there are arrays in the columns\n",
    "#neowise_CLAGN.write.parquet(\"neowise_CLAGN.parquet\")\n",
    "\n",
    "#instead try pulling the data into a pandas dataframe\n",
    "#is this faster? no 1h 58 min. for all columns\n",
    "#pd_neowise_CLAGN = pd.DataFrame.from_records(neowise_CLAGN.collect(), columns=neowise_CLAGN.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to convert those magnitudes into flux to be consistent in data structure.\n",
    "#go to Janskies\n",
    "#using zeropoints from here: https://wise2.ipac.caltech.edu/docs/release/allsky/expsup/sec4_4h.html\n",
    "def convert_WISEtoJanskies(mag, magerr, band):\n",
    "    if band == 'w1':\n",
    "        zpt = 309.54\n",
    "    elif band == 'w2':\n",
    "        zpt = 171.787\n",
    "            \n",
    "    flux_Jy = zpt*(10**(-mag/2.5))\n",
    "    \n",
    "    #calculate the error\n",
    "    magupper = mag + magerr\n",
    "    maglower = mag - magerr\n",
    "    flux_upper = abs(flux - (zpt*(10**(-magupper/2.5))))\n",
    "    flux_lower = abs(flux - (zpt*(10**(-maglower/2.5))))\n",
    "    \n",
    "    fluxerr_Jy = (flux_upper + flux_lower) / 2.0\n",
    "    \n",
    "    return flux_Jy, fluxerr_Jy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_decimal_float(changetype):\n",
    "\n",
    "    #columns which are themselves arrays\n",
    "    changetype['mjd'] = [changetype.mjd[i].astype(float) for i in range(len(changetype))]\n",
    "    changetype['w1mag'] = [changetype.w1pmag[i].astype(float) for i in range(len(changetype))]\n",
    "    changetype['w2mag'] = [changetype.w2pmag[i].astype(float) for i in range(len(changetype))]\n",
    "    changetype['w1error'] = [changetype.w1pmagerr[i].astype(float) for i in range(len(changetype))]\n",
    "    changetype['w2error'] = [changetype.w2pmagerr[i].astype(float) for i in range(len(changetype))]\n",
    "\n",
    "    return changetype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decimals are causing problems when trying to do things with these columns, aka, convert units\n",
    "floats = convert_decimal_float(pd_neowise_lc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(29):\n",
    "    print(i, floats['w2mag'][i].size, floats['w2error'][i].size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ack, errors don't have same lengths as flux arrays!!\n",
    "#well, we need to exclude w2 for now and figure this out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd_neowise_lc.drop(columns = ['w1flux', 'w1fluxerr'], inplace = True)\n",
    "\n",
    "#make new columns with fluxes instead of magnitudes for comparison with other bands\n",
    "w1flux, w1fluxerr = convert_WISEtoJanskies(floats['w1mag'],floats['w1error'] ,'w1')\n",
    "flux_lc = pd_neowise_lc.assign(w1flux = w1flux, w1fluxerr = w1fluxerr)\n",
    "\n",
    "#w2flux, w2fluxerr = convert_WISEtoJanskies(floats['w2mag'],floats['w2error'] ,'w2')\n",
    "#flux_lc = flux_lc.assign(w2flux = w2flux, w2fluxerr = w2fluxerr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flux_lc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_lc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#syntax isn't working and getting data type errors to convert the whole dataframe of arrays \n",
    "#into the multiindex dataframe structure for saving\n",
    "#instead try iterating over rows in df and filling dfsingle from there.\n",
    "#this is a crazy inefficiency, but....\n",
    "pd_neowise_lc.reset_index()\n",
    "for index, row in flux_lc.iterrows():\n",
    "    dfw1 = pd.DataFrame(dict(flux=row.w1flux, err=row.w1fluxerr, time=row.mjd, objectid=row.objnum, band='w1')).set_index([\"objectid\", \"band\", \"time\"])\n",
    "\n",
    "    #then concatenate each individual df together\n",
    "    #first make sure that df has been defined before\n",
    "    try:\n",
    "        df_lc\n",
    "    except NameError:\n",
    "        #df_lc doesn't exist (yet)\n",
    "        df_lc = dfw1\n",
    "    else:\n",
    "        #df_lc exists\n",
    "        df_lc = pd.concat([df_lc, dfw1])#, dfw2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 MAST: Pan-STARRS\n",
    "Code ideas taken from this website: https://ps1images.stsci.edu/ps1_dr2_api.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ps1cone(ra,dec,radius,table=\"mean\",release=\"dr1\",format=\"csv\",columns=None,\n",
    "           baseurl=\"https://catalogs.mast.stsci.edu/api/v0.1/panstarrs\", verbose=False,\n",
    "           **kw):\n",
    "    \"\"\"Do a cone search of the PS1 catalog\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ra (float): (degrees) J2000 Right Ascension\n",
    "    dec (float): (degrees) J2000 Declination\n",
    "    radius (float): (degrees) Search radius (<= 0.5 degrees)\n",
    "    table (string): mean, stack, or detection\n",
    "    release (string): dr1 or dr2\n",
    "    format: csv, votable, json\n",
    "    columns: list of column names to include (None means use defaults)\n",
    "    baseurl: base URL for the request\n",
    "    verbose: print info about request\n",
    "    **kw: other parameters (e.g., 'nDetections.min':2)\n",
    "    \"\"\"\n",
    "    \n",
    "    data = kw.copy()\n",
    "    data['ra'] = ra\n",
    "    data['dec'] = dec\n",
    "    data['radius'] = radius\n",
    "    return ps1search(table=table,release=release,format=format,columns=columns,\n",
    "                    baseurl=baseurl, verbose=verbose, **data)\n",
    "\n",
    "\n",
    "def ps1search(table=\"mean\",release=\"dr1\",format=\"csv\",columns=None,\n",
    "           baseurl=\"https://catalogs.mast.stsci.edu/api/v0.1/panstarrs\", verbose=False,\n",
    "           **kw):\n",
    "    \"\"\"Do a general search of the PS1 catalog (possibly without ra/dec/radius)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    table (string): mean, stack, or detection\n",
    "    release (string): dr1 or dr2\n",
    "    format: csv, votable, json\n",
    "    columns: list of column names to include (None means use defaults)\n",
    "    baseurl: base URL for the request\n",
    "    verbose: print info about request\n",
    "    **kw: other parameters (e.g., 'nDetections.min':2).  Note this is required!\n",
    "    \"\"\"\n",
    "    \n",
    "    data = kw.copy()\n",
    "    if not data:\n",
    "        raise ValueError(\"You must specify some parameters for search\")\n",
    "    checklegal(table,release)\n",
    "    if format not in (\"csv\",\"votable\",\"json\"):\n",
    "        raise ValueError(\"Bad value for format\")\n",
    "    url = f\"{baseurl}/{release}/{table}.{format}\"\n",
    "    if columns:\n",
    "        # check that column values are legal\n",
    "        # create a dictionary to speed this up\n",
    "        dcols = {}\n",
    "        for col in ps1metadata(table,release)['name']:\n",
    "            dcols[col.lower()] = 1\n",
    "        badcols = []\n",
    "        for col in columns:\n",
    "            if col.lower().strip() not in dcols:\n",
    "                badcols.append(col)\n",
    "        if badcols:\n",
    "            raise ValueError('Some columns not found in table: {}'.format(', '.join(badcols)))\n",
    "        # two different ways to specify a list of column values in the API\n",
    "        # data['columns'] = columns\n",
    "        data['columns'] = '[{}]'.format(','.join(columns))\n",
    "\n",
    "# either get or post works\n",
    "#    r = requests.post(url, data=data)\n",
    "    r = requests.get(url, params=data)\n",
    "\n",
    "    if verbose:\n",
    "        print(r.url)\n",
    "    r.raise_for_status()\n",
    "    if format == \"json\":\n",
    "        return r.json()\n",
    "    else:\n",
    "        return r.text\n",
    "\n",
    "\n",
    "def checklegal(table,release):\n",
    "    \"\"\"Checks if this combination of table and release is acceptable\n",
    "    \n",
    "    Raises a VelueError exception if there is problem\n",
    "    \"\"\"\n",
    "    \n",
    "    releaselist = (\"dr1\", \"dr2\")\n",
    "    if release not in (\"dr1\",\"dr2\"):\n",
    "        raise ValueError(\"Bad value for release (must be one of {})\".format(', '.join(releaselist)))\n",
    "    if release==\"dr1\":\n",
    "        tablelist = (\"mean\", \"stack\")\n",
    "    else:\n",
    "        tablelist = (\"mean\", \"stack\", \"detection\")\n",
    "    if table not in tablelist:\n",
    "        raise ValueError(\"Bad value for table (for {} must be one of {})\".format(release, \", \".join(tablelist)))\n",
    "\n",
    "\n",
    "def ps1metadata(table=\"mean\",release=\"dr1\",\n",
    "           baseurl=\"https://catalogs.mast.stsci.edu/api/v0.1/panstarrs\"):\n",
    "    \"\"\"Return metadata for the specified catalog and table\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    table (string): mean, stack, or detection\n",
    "    release (string): dr1 or dr2\n",
    "    baseurl: base URL for the request\n",
    "    \n",
    "    Returns an astropy table with columns name, type, description\n",
    "    \"\"\"\n",
    "    \n",
    "    checklegal(table,release)\n",
    "    url = f\"{baseurl}/{release}/{table}/metadata\"\n",
    "    r = requests.get(url)\n",
    "    r.raise_for_status()\n",
    "    v = r.json()\n",
    "    # convert to astropy table\n",
    "    tab = Table(rows=[(x['name'],x['type'],x['description']) for x in v],\n",
    "               names=('name','type','description'))\n",
    "    return tab\n",
    "\n",
    "\n",
    "def addfilter(dtab):\n",
    "    \"\"\"Add filter name as column in detection table by translating filterID\n",
    "    \n",
    "    This modifies the table in place.  If the 'filter' column already exists,\n",
    "    the table is returned unchanged.\n",
    "    \"\"\"\n",
    "    if 'filter' not in dtab.colnames:\n",
    "        # the filterID value goes from 1 to 5 for grizy\n",
    "        id2filter = np.array(list('grizy'))\n",
    "        dtab['filter'] = id2filter[(dtab['filterID']-1).data]\n",
    "    return dtab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try for panstarrs\n",
    "radius = 1.0/3600.0 # radius = 1 arcsec\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.figure(1,(10,10))\n",
    "\n",
    "        \n",
    "#for all objects\n",
    "for ccount, coord in enumerate(coords_list):\n",
    "    #doesn't take SkyCoord\n",
    "    ra = CLAGN['RA'][ccount]\n",
    "    dec = CLAGN['DEC'][ccount]\n",
    "\n",
    "    #see if there is an object in panSTARRS at this location\n",
    "    results = ps1cone(ra,dec,radius,release='dr2')\n",
    "    tab = ascii.read(results)\n",
    "    \n",
    "    # improve the format\n",
    "    for filter in 'grizy':\n",
    "        col = filter+'MeanPSFMag'\n",
    "        tab[col].format = \".4f\"\n",
    "        tab[col][tab[col] == -999.0] = np.nan\n",
    "        \n",
    "    #in case there is more than one object within 1 arcsec, sort them by match distance\n",
    "    tab.sort('distance')\n",
    "    \n",
    "    #if there is an object at that location\n",
    "    if len(tab) > 0:   \n",
    "        #got a live one\n",
    "        #print( 'for object', ccount + 1, 'there is ',len(tab), 'match in panSTARRS', tab['objID'])\n",
    "\n",
    "        #take the closest match as the best match\n",
    "        objid = tab['objID'][0]\n",
    "        \n",
    "        #setup to pull light curve info\n",
    "        dconstraints = {'objID': objid}\n",
    "        dcolumns = (\"\"\"objID,detectID,filterID,obsTime,ra,dec,psfFlux,psfFluxErr,psfMajorFWHM,psfMinorFWHM,\n",
    "                    psfQfPerfect,apFlux,apFluxErr,infoFlag,infoFlag2,infoFlag3\"\"\").split(',')\n",
    "        # strip blanks and weed out blank and commented-out values\n",
    "        dcolumns = [x.strip() for x in dcolumns]\n",
    "        dcolumns = [x for x in dcolumns if x and not x.startswith('#')]\n",
    "\n",
    "\n",
    "        #get the actual detections and light curve info for this target\n",
    "        dresults = ps1search(table='detection',release='dr2',columns=dcolumns,**dconstraints)\n",
    "        \n",
    "        #sometimes there isn't actually a light curve for the target???\n",
    "        try:\n",
    "            ascii.read(dresults)\n",
    "        except FileNotFoundError:\n",
    "            print(\"There is no light curve\")\n",
    "            #no need to store PanSTARRS data for this one\n",
    "        else:\n",
    "            #There is a light curve for this target\n",
    "            \n",
    "            #fix the column names to include filter names\n",
    "            dtab = addfilter(ascii.read(dresults))\n",
    "            dtab.sort('obsTime')\n",
    "\n",
    "            #here is the light curve mixed from all 5 bands\n",
    "            t_panstarrs = dtab['obsTime']\n",
    "            flux_panstarrs = dtab['psfFlux']  # in Jy\n",
    "            err_panstarrs = dtab['psfFluxErr']\n",
    "            filtername = dtab['filter']\n",
    "            \n",
    "            #put this single object light curves into a pandas multiindex dataframe\n",
    "            dfsingle = pd.DataFrame(dict(flux=flux_panstarrs, err=err_panstarrs, time=t_panstarrs, objectid=ccount + 1, band=filtername)).set_index([\"objectid\", \"band\", \"time\"])\n",
    "            \n",
    "            #then concatenate each individual df together\n",
    "            #first make sure that df has been defined before\n",
    "            try:\n",
    "                df_lc\n",
    "            except NameError:\n",
    "                #df_lc doesn't exist (yet)\n",
    "                df_lc = dfsingle\n",
    "            else:\n",
    "                #df_lc exists\n",
    "                df_lc = pd.concat([df_lc, dfsingle])\n",
    "                \n",
    "            \n",
    "            #plot light curves on same plot just to know they are there?\n",
    "            #not currently working\n",
    "            #xlim = np.array([t.min(),t.max()])\n",
    "            #xlim = xlim + np.array([-1,1])*0.02*(xlim[1]-xlim[0])\n",
    "            #for i, filter in enumerate(\"grizy\"):\n",
    "            #    plt.subplot(511+i)\n",
    "            #    w = np.where(dtab['filter']==filter)\n",
    "            #    plt.plot(t[w],flux[w],'-o')\n",
    "            #    plt.ylabel(filter+' [Jy]')\n",
    "            #    plt.xlim(xlim)\n",
    "            #    #plt.gca().invert_yaxis()\n",
    "            #    if i==0:\n",
    "            #        plt.title(objid)\n",
    "            #plt.xlabel('Time [MJD]')\n",
    "            #plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.set_option('display.max_rows',None)\n",
    "df_lc\n",
    "#pd.reset_option('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 MAST: ATLAS all-sky stellar reference catalog (g, r, i) < 19mag\n",
    " -  MAST has this catalog but it is not clear that it has the individual epoch photometry and it is only accessible with casjobs, not through python notebooks.  \n",
    "\n",
    " https://archive.stsci.edu/hlsp/atlas-refcat2#section-a737bc3e-2d56-4827-9ab4-838fbf8d67c1\n",
    " \n",
    " - if we really want to pursue this, we can put in a MAST helpdesk ticket to see if a) they do have the light curves, and b) they could switch the catalog to a searchable with python version.  There are some ways of accessing casjobs through python (<https://github.com/spacetelescope/notebooks/blob/master/notebooks/MAST/HSC/HCV_CASJOBS/HCV_casjobs_demo.ipynb), but apparently not this particular catalog.  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 MAST: TESS, Kepler and K2\n",
    " - use lightKurve to search all 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radius = 1.0  #arcseconds\n",
    "\n",
    "#for all objects\n",
    "for count, coord in enumerate(coords_list):\n",
    "    print(\"working on object\", count, coord)\n",
    "    \n",
    "    #use lightkurve to search TESS, Kepler and K2\n",
    "    search_result = lk.search_lightcurve(coord, radius = radius)\n",
    "    \n",
    "    #figure out what to do with the results\n",
    "    if len(search_result) < 1:\n",
    "        #there is no data in these missions at this location\n",
    "    else:\n",
    "        #don't know what this looks like because none of these targets has a light curve\n",
    "        #https://docs.lightkurve.org/tutorials/1-getting-started/searching-for-data-products.html\n",
    "        #has a tutorial on how to do this\n",
    "        #might look something like this:\n",
    "        #lc_collection = search_result[*].download_all()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 MAST: HCV\n",
    " - hubble catalog of variables (https://archive.stsci.edu/hlsp/hcv)\n",
    " - follow notebook here to know how to search and download light curves https://archive.stsci.edu/hst/hsc/help/HCV/HCV_API_demo.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Find light curves for these targets in relevant, non-NASA catalogs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaia (Faisst)\n",
    "- astroquery.gaia will presumably work out of the box for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ EXTRACT GAIA DATA FOR OBJECTS ##########\n",
    "\n",
    "## Select Gaia table (DR3)\n",
    "Gaia.MAIN_GAIA_TABLE = \"gaiaedr3.gaia_source\"\n",
    "\n",
    "## Define search radius\n",
    "radius = u.Quantity(20, u.arcsec)\n",
    "\n",
    "## Search and Cross match.\n",
    "# This can be done in a smarter way by matching catalogs on the Gaia server, or grouping the\n",
    "# sources and search a larger area.\n",
    "\n",
    "# get catalog\n",
    "gaia_table = Table()\n",
    "t1 = time.time()\n",
    "for cc,coord in enumerate(coords_list):\n",
    "    print(len(coords_list)-cc , end=\" \")\n",
    "\n",
    "    gaia_search = Gaia.cone_search_async(coordinate=coord, radius=radius , background=True)\n",
    "    gaia_search.get_data()[\"dist\"].unit = \"deg\"\n",
    "    gaia_search.get_data()[\"dist\"] = gaia_search.get_data()[\"dist\"].to(u.arcsec) # Change distance unit from degrees to arcseconds\n",
    "    \n",
    "    \n",
    "    # match\n",
    "    if len(gaia_search.get_data()[\"dist\"]) > 0:\n",
    "        gaia_search.get_data()[\"input_object_name\"] = CLAGN[\"Object Name\"][cc] # add input object name to catalog\n",
    "        sel_min = np.where( (gaia_search.get_data()[\"dist\"] < 1*u.arcsec) & (gaia_search.get_data()[\"dist\"] == np.nanmin(gaia_search.get_data()[\"dist\"]) ) )[0]\n",
    "    else:\n",
    "        sel_min = []\n",
    "        \n",
    "    #print(\"Number of sources matched: {}\".format(len(sel_min)) )\n",
    "    \n",
    "    if len(sel_min) > 0:\n",
    "        gaia_table = vstack( [gaia_table , gaia_search.get_data()[sel_min]] )\n",
    "    else:\n",
    "        gaia_table = vstack( [gaia_table , gaia_search.get_data()[sel_min]] )\n",
    "\n",
    "print(\"\\nSearch completed in {:.2f} seconds\".format((time.time()-t1) ) )\n",
    "print(\"Number of objects mached: {} out of {}.\".format(len(gaia_table),len(CLAGN) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## EXTRACT PHOTOMETRY #########\n",
    "# Note that the fluxes are in e/s, not very useful. However, there are magnitudes (what unit??) but without errors.\n",
    "# We can get the errors from the flux errors?\n",
    "\n",
    "## Define keys (columns) that will be used later. Also add wavelength in angstroms for each filter\n",
    "mag_keys = [\"phot_bp_mean_mag\" , \"phot_g_mean_mag\" , \"phot_rp_mean_mag\"]\n",
    "magerr_keys = [\"phot_bp_mean_mag_error\" , \"phot_g_mean_mag_error\" , \"phot_rp_mean_mag_error\"]\n",
    "flux_keys = [\"phot_bp_mean_flux\" , \"phot_g_mean_flux\" , \"phot_rp_mean_flux\"]\n",
    "fluxerr_keys = [\"phot_bp_mean_flux_error\" , \"phot_g_mean_flux_error\" , \"phot_rp_mean_flux_error\"]\n",
    "mag_lambda = [\"5319.90\" , \"6735.42\" , \"7992.90\"]\n",
    "\n",
    "## Get photometry. Note that this includes only objects that are \n",
    "# matched to the catalog. We have to add the missing ones later.\n",
    "_phot = gaia_table[mag_keys]\n",
    "_err = hstack( [ 2.5/np.log(10) * gaia_table[e]/gaia_table[f] for e,f in zip(fluxerr_keys,flux_keys) ] )\n",
    "gaia_phot2 = hstack( [_phot , _err] )\n",
    "\n",
    "## Clean up (change units and column names)\n",
    "_ = [gaia_phot2.rename_column(f,m) for m,f in zip(magerr_keys,fluxerr_keys)]\n",
    "for key in magerr_keys:\n",
    "    gaia_phot2[key].unit = \"mag\"\n",
    "gaia_phot2[\"input_object_name\"] = gaia_table[\"input_object_name\"].copy()\n",
    "\n",
    "## Also add object for which we don't have photometry.\n",
    "# Add Nan for now, need to think about proper format. Also, there are probably smarter ways to do this.\n",
    "# We do this by matching the object names from the original catalog to the photometry catalog. Then add\n",
    "# an entry [np.nan, ...] if it does not exist. To make life easier, we add a dummy entry as the first\n",
    "# row so we can compy all the \n",
    "gaia_phot = Table( names=gaia_phot2.keys() , dtype=gaia_phot2.dtype )\n",
    "for ii in range(len(CLAGN)):\n",
    "    sel = np.where( CLAGN[\"Object Name\"][ii] == gaia_phot2[\"input_object_name\"] )[0]\n",
    "    if len(sel) > 0:\n",
    "        gaia_phot = vstack([gaia_phot , gaia_phot2[sel] ])\n",
    "    else:\n",
    "        tmp = Table( np.repeat(np.NaN , len(gaia_phot2.keys())) , names=gaia_phot2.keys() , dtype=gaia_phot2.dtype )\n",
    "        gaia_phot = vstack([gaia_phot , tmp ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaia_phot.pprint_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASAS-SN (all sky automated survey for supernovae) has a website that can be manually searched (Faisst)\n",
    "- see if astroquery.vizier can find it\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### icecube has a 2008 - 2018 catalog which we can download and is small (Faisst)\n",
    "- https://icecube.wisc.edu/data-releases/2021/01/all-sky-point-source-icecube-data-years-2008-2018/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Make plots of luminosity as a function of time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data structure is called df_lc\n",
    "#bands we currently have available are WISE & pan-starrs\n",
    "availband = ['w1', 'g', 'r', 'i', 'z','y']\n",
    "obj = 1  #pick one to look at for now\n",
    "fig, ax = plt.subplots(figsize=(9,5))\n",
    "\n",
    "for l in range(len(availband)):\n",
    "    band_lc = df_lc.loc[(obj, availband[l]), :]\n",
    "    band_lc.reset_index(inplace=True)\n",
    "    ax.errorbar(band_lc.time, band_lc.flux, band_lc.err, capsize = 3.0, label = availband[l])\n",
    "    \n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.set_xlabel('Time(MJD)')\n",
    "ax.set_ylabel('Flux(Jy)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image extension: look for archival images of these targets\n",
    "- NASA NAVO use cases should help us to learn how to do this\n",
    "- can use the cutout service now in astropy from the first fornax use case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Extension \n",
    "Consider training a ML model to do light curve classification based on this sample of CLAGN\n",
    " - once we figure out which bands these are likely to be observed in, could then have a optical + IR light curve classifier\n",
    " - what would the features of the light curve be?\n",
    " - what models are reasonable to test as light curve classifiers?\n",
    " - could we make also a sample of TDEs, SNe, flaring AGN? - then train the model to distinguish between these things?\n",
    " - need a sample of non-flaring light curves\n",
    " \n",
    "After training the model:\n",
    " - would then need a sample of optical + IR light curves for \"all\" galaxies = big data to run the model on.\n",
    "\n",
    "Some resources to consider:\n",
    "- https://github.com/dirac-institute/ZTF_Boyajian\n",
    "- https://ui.adsabs.harvard.edu/abs/2022AJ....164...68S/abstract\n",
    "- https://ui.adsabs.harvard.edu/abs/2019ApJ...881L...9F/abstract\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

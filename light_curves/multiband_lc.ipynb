{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make multiwavelength light curves using archival data\n",
    "***\n",
    "\n",
    "## Learning Goals    \n",
    "By the end of this tutorial, you will be able to:\n",
    " - automatically load a catalog of sources\n",
    " - automatically search NASA and non-NASA resources for light curves\n",
    " - store light curves in a Pandas multiindex dataframe\n",
    " - plot all light curves on the same plot\n",
    " \n",
    " \n",
    "## Introduction:\n",
    " - A user has a sample of interesting targets for which they would like to see a plot of available archival light curves.  We start with a small set of changing look AGN from Yang et al., 2018, which are automatically downloaded. Changing look AGN are cases where the broad emission lines appear or disappear (and not just that the flux is variable). We model light curve plots after van Velzen et al. 2021.  We search through a curated list of time-domain NASA holdings as well as non-NASA sources.  HEASARC catalogs used are Fermi and Beppo-Sax, IRSA catalogs used are ZTF and WISE, and MAST catalogs used are Pan-Starrs, TESS, Kepler, and K2.  Non-NASA sources are Gaia and IceCube. This list is hopefully generalized enough to include many types of targets to make this notebook interesting for many types of science.  All of these time-domain archives are searched in an automated fashion using astroquery.\n",
    " - Light curve data storage is a tricky problem.  Currently we are using a multi-index Pandas dataframe, but it is not clear that this is the best choice or the best choice for scaling up.  One downside is that we need to manually track the units of flux and time instead of relying on an astropy storage scheme which would be able to do the units worrying for us.  Astropy does not currently have a good option for multi band light curve storage.\n",
    " - We intend to explore a ML classifier for these changing look AGN light curves.\n",
    " \n",
    "## Input:\n",
    " - a catalog of CLAGN from the literature\n",
    "\n",
    "## Output:\n",
    " - an archival optical + IR + neutrino light curve\n",
    " \n",
    "## Technical Goals:\n",
    " - should be able to run from a clean checkout from github\n",
    " - should be able to automatically download all catalogs & images used\n",
    " - need to have all photometry in the same physical unit\n",
    " - need to have a data structure that is easy to use but holds light curve information (time and units) and is extendable to ML applications\n",
    " - need to have a curated list of catalogs to search for photometry that is generalizeable to other input catalogs\n",
    " \n",
    "## Non-standard Imports:\n",
    "- `astroquery` to interface with archives APIs\n",
    "- `astropy` to work with coordinates/units and data structures\n",
    "- `AXS` (astronomy extensions for Spark) to handle large catalog cross matching\n",
    "- `lightkurve` to search TESSS, Kepler, and K2 archives\n",
    "- `urllib` to handle archive searches with website interface\n",
    "\n",
    "## Authors:\n",
    "IPAC SP team\n",
    "\n",
    "## Acknowledgements:\n",
    "Suvi Gezari, Antara Basu-zych,\\\n",
    "MAST, HEASARC, & IRSA Fornax teams\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import axs\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import json\n",
    "import requests\n",
    "\n",
    "from astroquery.ipac.ned import Ned\n",
    "from astroquery.heasarc import Heasarc\n",
    "from astroquery.gaia import Gaia\n",
    "\n",
    "from astropy.coordinates import SkyCoord, name_resolve\n",
    "import astropy.units as u\n",
    "from astropy.table import Table, vstack, hstack, unique\n",
    "from astropy.io import fits,ascii\n",
    "from astropy.time import Time\n",
    "from astropy.timeseries import TimeSeries\n",
    "\n",
    "\n",
    "try: # Python 3.x\n",
    "    from urllib.parse import quote as urlencode\n",
    "    from urllib.request import urlretrieve\n",
    "except ImportError:  # Python 2.x\n",
    "    from urllib import pathname2url as urlencode\n",
    "    from urllib import urlretrieve\n",
    "\n",
    "#!pip install lightkurve --upgrade\n",
    "import lightkurve as lk\n",
    "\n",
    "#!pip install acstools\n",
    "from acstools import acszpt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "try:\n",
    "    from unTimely_Catalog_tools import unTimelyCatalogExplorer\n",
    "except ImportError:\n",
    "    if not os.path.exists('./unTimely_Catalog_explorer'):\n",
    "        !git clone https://github.com/fkiwy/unTimely_Catalog_explorer.git\n",
    "    sys.path.append('./unTimely_Catalog_explorer')\n",
    "    from unTimely_Catalog_tools import unTimelyCatalogExplorer\n",
    "    \n",
    "import tempfile\n",
    "\n",
    "\n",
    "## From Brigitta\n",
    "class MultibandTimeSeries(TimeSeries):\n",
    "    def __init__(self, *, data=None, time=None, **kwargs):\n",
    "        # using kwargs to swallow all other arguments a TimeSeries/QTable can have,\n",
    "        # but we dont explicitly use. Ideally they are spelt out if we have docstrings here.\n",
    "        # Also using keyword only arguments everywhere to force being explicit.\n",
    "        super().__init__(data=data, time=time, **kwargs)\n",
    "                \n",
    "    def add_band(self, *, time=None, data=None, band_name=\"None\"):\n",
    "        '''Add a time, flux/mag data set and resort the arrays. ``time`` can be a TimeSeries instance'''\n",
    "        if 'time' not in self.colnames:\n",
    "            if isinstance(time, TimeSeries):\n",
    "                super().__init__(time)\n",
    "            else:\n",
    "                super().__init__(data={band_name: data}, time=time)\n",
    "        else:\n",
    "            if time is None:\n",
    "                # this assumes ``band_name`` fluxes are taken at the very same times as the already exitsing bands\n",
    "                # TODO: include checks for sizes and other assumptions\n",
    "                self[band_name] = data\n",
    "                return \n",
    "            elif not isinstance(time, TimeSeries):\n",
    "                # TODO: handle band_name=None case\n",
    "                time = TimeSeries(time=time, data={band_name: data})\n",
    "            super().__init__(vstack([self, time]))\n",
    "            \n",
    "            \n",
    "## MultiIndex Pandas data frame object in which we can append the light curves:\n",
    "class MultiIndexDFObject:\n",
    "    '''\n",
    "    Pandas data frame MultiIndex object. \n",
    "    - add(): append new MultiIndex light curve data frame.\n",
    "    - .data returns the data.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def append(self,x):\n",
    "        try:\n",
    "            self.data\n",
    "        except AttributeError:\n",
    "            self.data = x.copy()\n",
    "        else:\n",
    "            self.data = pd.concat([self.data , x])\n",
    "\n",
    "\n",
    "## Plotting stuff\n",
    "mpl.rcParams['font.size'] = 12\n",
    "mpl.rcParams['axes.labelpad'] = 10\n",
    "mpl.rcParams['xtick.major.pad'] = 7\n",
    "mpl.rcParams['ytick.major.pad'] = 7\n",
    "mpl.rcParams['xtick.minor.visible'] = True\n",
    "mpl.rcParams['ytick.minor.visible'] = True\n",
    "mpl.rcParams['xtick.minor.top'] = True\n",
    "mpl.rcParams['xtick.minor.bottom'] = True\n",
    "mpl.rcParams['ytick.minor.left'] = True\n",
    "mpl.rcParams['ytick.minor.right'] = True\n",
    "mpl.rcParams['xtick.major.size'] = 5\n",
    "mpl.rcParams['ytick.major.size'] = 5\n",
    "mpl.rcParams['xtick.minor.size'] = 3\n",
    "mpl.rcParams['ytick.minor.size'] = 3\n",
    "mpl.rcParams['xtick.direction'] = 'in'\n",
    "mpl.rcParams['ytick.direction'] = 'in'\n",
    "#mpl.rc('text', usetex=True)\n",
    "mpl.rc('font', family='serif')\n",
    "mpl.rcParams['xtick.top'] = True\n",
    "mpl.rcParams['ytick.right'] = True\n",
    "mpl.rcParams['hatch.linewidth'] = 1\n",
    "def_cols = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define the Sample\n",
    " use the following paper to make a sample of CLAGN: https://iopscience.iop.org/article/10.3847/1538-4357/aaca3a \n",
    "\n",
    " This sample can later be switched out to a differen/larger sample of \"interesting\" targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use ADS to find the refcode for this paper (ie., '2018blahblah')\n",
    "#Then use the NED astroquery interface to load the table from the paper with relevant target info\n",
    "#This loads the table into an astropy table\n",
    "\n",
    "CLAGN = Ned.query_refcode('2018ApJ...862..109Y')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#see what is included in this table\n",
    "CLAGN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a list of skycoords from target ra and dec\n",
    "#This is useful for some codes which understand that these are a set of coordinates\n",
    "coords_list = [\n",
    "    SkyCoord(ra, dec, frame='icrs', unit='deg')\n",
    "    for ra, dec in zip(CLAGN['RA'], CLAGN['DEC'])\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here are additional CLAGN samples\n",
    "CLQ = Ned.query_refcode('2022ApJ...933..180G') #from Green et al., 2022\n",
    "\n",
    "green_coords = [\n",
    "    SkyCoord(ra, dec, frame='icrs', unit='deg')\n",
    "    for ra, dec in zip(CLQ['RA'], CLAGN['DEC'])\n",
    "]\n",
    "\n",
    "CSAGN = Ned.query_refcode('2021AJ....162..206S') # from Sanchez-Saez 2021\n",
    "\n",
    "ss_coords = [\n",
    "    SkyCoord(ra, dec, frame='icrs', unit='deg')\n",
    "    for ra, dec in zip(CSAGN['RA'], CSAGN['DEC'])\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use astropy table to get larger sample that neither NED nor astropy can access.\n",
    "CSQ = Table.read('https://academic.oup.com/mnras/article/491/4/4925/5634279', htmldict={'table_id': 5}, format='ascii.html')\n",
    "\n",
    "#get coords from \"name\" column for this\n",
    "graham_coords = []\n",
    "for i in range(len(CSQ)):\n",
    "    coord_str = CSQ['Name\\n            .'][i]\n",
    "    test_str = coord_str[6:8]+ \" \"+ coord_str[8:10]+ \" \" + coord_str[10:14] + \" \" + coord_str[14:17] + \" \" + coord_str[17:19]+ \" \" + coord_str[19:]\n",
    "    graham_coords.append(SkyCoord(test_str, unit=(u.hourangle, u.deg)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#when ready for more than the initial sample of 30\n",
    "#coords_list.extend(green_coords)\n",
    "#coords_list.extend(ss_coords)\n",
    "#coords_list.extend(graham_coords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Initialize Pandas MultiIndex data frame for storing the light curve\n",
    "df_lc = MultiIndexDFObject()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Find light curves for these targets in NASA catalogs\n",
    "- data access concerns:\n",
    "    - can't ask the archives to search their entire holdings\n",
    "        - not good enough meta data\n",
    "        - not clear that the data is all vetted and good enough to include for science\n",
    "        - all catalogs have differently named columns so how would we know which columns to keep\n",
    "    - instead work with a curated list of catalogs for each archive\n",
    "        - focus on general surveys\n",
    "        - try to ensure that this list is also appropriate for a generalization of this use case to other input catalogs\n",
    "        - could astroquery.NED be useful in finding a generalized curated list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 HEASARC: FERMI & Beppo SAX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mission_list = ['FERMIGTRIG', 'SAXGRBMGRB']\n",
    "radius = 0.1*u.degree\n",
    "#for testing\n",
    "for ccount in range(1):\n",
    "    #To get a fermigtrig source\n",
    "    #coord = SkyCoord('03h41m21.2s -89d00m33.0s', frame='icrs')\n",
    "\n",
    "    #to get a bepposax source\n",
    "    coord = SkyCoord('14h32m00.0s -88d00m00.0s', frame='icrs')\n",
    "\n",
    "#for ccount, coord in enumerate(coords_list):\n",
    "    #use astroquery to search that position for either a Fermi or Beppo Sax trigger\n",
    "    for mcount, mission in enumerate(mission_list):\n",
    "        try:\n",
    "            results = Heasarc.query_region(coord, mission = mission, radius = radius)#, sortvar = 'SEARCH_OFFSET_')\n",
    "            #really just need to save the one time of the Gamma ray detection\n",
    "            #time is already in MJD for both catalogs\n",
    "            if mission == 'FERMIGTRIG':\n",
    "                time_mjd = float(results['TRIGGER_TIME'])\n",
    "            else:\n",
    "                time_mjd = float(results['TIME'][0])\n",
    "                \n",
    "            type(time_mjd)\n",
    "\n",
    "            #really just need to mark this spot with a vertical line in the plot\n",
    "            dfsingle = pd.DataFrame(dict(flux=[0.1], err=[0.1], time=[time_mjd], objectid=[ccount + 1], band=[mission])).set_index([\"objectid\", \"band\", \"time\"])\n",
    "            \n",
    "            # Append to existing MultiIndex light curve object\n",
    "            df_lc.append(dfsingle)\n",
    "\n",
    "        except AttributeError:\n",
    "            print(\"no results at that location for \", mission)\n",
    "\n",
    "\n",
    "#**** These HEASARC searches are returning an attribute error because of an astroquery bug\n",
    "# bug submitted to astroquery Oct 18, waiting for a fix.\n",
    "# if that gets fixed, can probably change this cell "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lc.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 IRSA: ZTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#python package ztfquery is not a good solution for this because it requires IRSA password\n",
    "#Instead will construct the URL for an API query\n",
    "#https://irsa.ipac.caltech.edu/docs/program_interface/ztf_lightcurve_api.html\n",
    "ztf_radius = 0.000278   #as suggested by Dave Shupe\n",
    "\n",
    "for ccount, coord in enumerate(coords_list):\n",
    "    #doesn't take SkyCoord\n",
    "    ra = CLAGN['RA'][ccount]\n",
    "    dec = CLAGN['DEC'][ccount]\n",
    "    #make the string for the URL query\n",
    "    #ask for all three bands (g, r, i)\n",
    "    #don't want data that is flagged as unusable by the ZTF pipeline\n",
    "    urlstr = 'https://irsa.ipac.caltech.edu/cgi-bin/ZTF/nph_light_curves?POS=CIRCLE %f %f %f&BANDNAME=g,r,i&FORMAT=ipac_table&BAD_CATFLAGS_MASK=32768'%(ra, dec,ztf_radius)\n",
    "\n",
    "    response = requests.get(urlstr)\n",
    "    if response.ok:\n",
    "        ztf_lc = ascii.read(response.text, format='ipac')\n",
    "        print('object '+str(ccount)+' , unique ztf IDs:'+str(len(np.unique(ztf_lc['oid'])))+',in '+str(len(np.unique(ztf_lc['filtercode'])))+' filters')\n",
    "        # reading in indecies of unique IDs to do coordinate match and find dif magnitudes of same objects \n",
    "        idu,inds = np.unique(ztf_lc['oid'],return_index=True)\n",
    "        #plt.figure(figsize=(6,4))\n",
    "        for nn,idd in enumerate(idu):\n",
    "            sel = (ztf_lc['oid']==idd)\n",
    "            \n",
    "            flux = 10**((ztf_lc['mag'][sel] - 23.9)/(-2.5))  #now in uJy [Based on ztf paper https://arxiv.org/pdf/1902.01872.pdf zeropoint corrections already applied]\n",
    "            magupper = ztf_lc['mag'][sel] + ztf_lc['magerr'][sel]\n",
    "            maglower = ztf_lc['mag'][sel] - ztf_lc['magerr'][sel]\n",
    "            flux_upper = abs(flux - (10**((magupper - 23.9)/(-2.5))))\n",
    "            flux_lower =  abs(flux - (10**((maglower - 23.9)/(-2.5))))\n",
    "            fluxerr = (flux_upper + flux_lower) / 2.0\n",
    "            \n",
    "            flux = flux * (1E-3) # now in mJy\n",
    "            fluxerr = fluxerr * (1E-3) # now in mJy\n",
    "            \n",
    "            #plt.errorbar(np.round(ztf_lc['mjd'][sel],0),flux,yerr=fluxerr,marker='.',linestyle='',label=ztf_lc['filtercode'][sel][0])\n",
    "            #plt.legend()\n",
    "            \n",
    "  \n",
    "            # add to df_lc\n",
    "            dfsingle = pd.DataFrame(dict(flux=flux, err=fluxerr, time=ztf_lc['mjd'][sel], objectid=np.ones_like(ztf_lc['mag'][sel])*(ccount+1), band=ztf_lc['filtercode'][sel])).set_index([\"objectid\", \"band\", \"time\"])\n",
    "            df_lc.append(dfsingle)\n",
    "\n",
    "        #plt.show()\n",
    "    \n",
    "    else:\n",
    "        print(ccount, \" There is no ZTF light curve for object\"+str(ccount))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 IRSA:WISE\n",
    "\n",
    "- use the unTimely catalog which ties together all WISE & NEOWISE 2010 - 2020 epochs.  Specifically it combined all observations at a single epoch to achieve deeper mag limits than individual observations alone.\n",
    "- https://github.com/fkiwy/unTimely_Catalog_explorer\n",
    "- https://iopscience-iop-org.caltech.idm.oclc.org/article/10.3847/1538-3881/aca2ab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to convert those magnitudes into mJy to be consistent in data structure.\n",
    "#using zeropoints from here: https://wise2.ipac.caltech.edu/docs/release/allsky/expsup/sec4_4h.html\n",
    "def convert_WISEtoJanskies(mag, magerr, band):\n",
    "    if band == 'w1':\n",
    "        zpt = 309.54\n",
    "    elif band == 'w2':\n",
    "        zpt = 171.787\n",
    "            \n",
    "    flux_Jy = zpt*(10**(-mag/2.5))\n",
    "    \n",
    "    #calculate the error\n",
    "    magupper = mag + magerr\n",
    "    maglower = mag - magerr\n",
    "    flux_upper = abs(flux_Jy - (zpt*(10**(-magupper/2.5))))\n",
    "    flux_lower = abs(flux_Jy - (zpt*(10**(-maglower/2.5))))\n",
    "    \n",
    "    fluxerr_Jy = (flux_upper + flux_lower) / 2.0\n",
    "    \n",
    "    return flux_Jy*1E3, fluxerr_Jy*1E3  #now in mJy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup the explorer\n",
    "ucx = unTimelyCatalogExplorer(directory=os.getcwd(), cache=True, show_progress=True, timeout=300,\n",
    "                              catalog_base_url='http://unwise.me/data/neo7/untimely-catalog/',\n",
    "                              catalog_index_file='untimely_index-neo7.fits')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "bandlist = ['w1', 'w2']\n",
    "\n",
    "#for ccount in range(1):#enumerate(coords_list):\n",
    "for ccount, coord in enumerate(coords_list):\n",
    "    ra = CLAGN['RA'][ccount]\n",
    "    dec = CLAGN['DEC'][ccount]\n",
    "\n",
    "    #search the untimely catalog\n",
    "    result_table = ucx.search_by_coordinates(ra, dec, box_size=100, cone_radius=1.0, show_result_table_in_browser=False,\n",
    "                                         save_result_table=False)\n",
    "\n",
    "    if (len(result_table) > 0):\n",
    "        #got a live one\n",
    "        for bcount, band in enumerate(bandlist):\n",
    "            #sort by band\n",
    "            mask = result_table['band'] == (bcount + 1)\n",
    "            result_table_band = result_table[mask]\n",
    "    \n",
    "            mag = result_table_band['mag']\n",
    "            magerr = result_table_band['dmag']\n",
    "    \n",
    "            wiseflux, wisefluxerr = convert_WISEtoJanskies(mag,magerr ,band)\n",
    "\n",
    "            time_mjd = result_table_band['mjdmean']\n",
    "            \n",
    "            #plt.figure(figsize=(8, 4))\n",
    "            #plt.errorbar(time_mjd, wiseflux, wisefluxerr)\n",
    "            dfsingle = pd.DataFrame(dict(flux=wiseflux, err=wisefluxerr, time=time_mjd, objectid=ccount + 1, band=band)).set_index([\"objectid\", \"band\", \"time\"])\n",
    "\n",
    "            #then concatenate each individual df together\n",
    "            df_lc.append(dfsingle)\n",
    "            \n",
    "    else:\n",
    "        print(\"There is no WISE light curve for this object\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 MAST: Pan-STARRS\n",
    "Code ideas taken from this website: https://ps1images.stsci.edu/ps1_dr2_api.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ps1cone(ra,dec,radius,table=\"mean\",release=\"dr1\",format=\"csv\",columns=None,\n",
    "           baseurl=\"https://catalogs.mast.stsci.edu/api/v0.1/panstarrs\", verbose=False,\n",
    "           **kw):\n",
    "    \"\"\"Do a cone search of the PS1 catalog\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ra (float): (degrees) J2000 Right Ascension\n",
    "    dec (float): (degrees) J2000 Declination\n",
    "    radius (float): (degrees) Search radius (<= 0.5 degrees)\n",
    "    table (string): mean, stack, or detection\n",
    "    release (string): dr1 or dr2\n",
    "    format: csv, votable, json\n",
    "    columns: list of column names to include (None means use defaults)\n",
    "    baseurl: base URL for the request\n",
    "    verbose: print info about request\n",
    "    **kw: other parameters (e.g., 'nDetections.min':2)\n",
    "    \"\"\"\n",
    "    \n",
    "    data = kw.copy()\n",
    "    data['ra'] = ra\n",
    "    data['dec'] = dec\n",
    "    data['radius'] = radius\n",
    "    return ps1search(table=table,release=release,format=format,columns=columns,\n",
    "                    baseurl=baseurl, verbose=verbose, **data)\n",
    "\n",
    "\n",
    "def ps1search(table=\"mean\",release=\"dr1\",format=\"csv\",columns=None,\n",
    "           baseurl=\"https://catalogs.mast.stsci.edu/api/v0.1/panstarrs\", verbose=False,\n",
    "           **kw):\n",
    "    \"\"\"Do a general search of the PS1 catalog (possibly without ra/dec/radius)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    table (string): mean, stack, or detection\n",
    "    release (string): dr1 or dr2\n",
    "    format: csv, votable, json\n",
    "    columns: list of column names to include (None means use defaults)\n",
    "    baseurl: base URL for the request\n",
    "    verbose: print info about request\n",
    "    **kw: other parameters (e.g., 'nDetections.min':2).  Note this is required!\n",
    "    \"\"\"\n",
    "    \n",
    "    data = kw.copy()\n",
    "    if not data:\n",
    "        raise ValueError(\"You must specify some parameters for search\")\n",
    "    checklegal(table,release)\n",
    "    if format not in (\"csv\",\"votable\",\"json\"):\n",
    "        raise ValueError(\"Bad value for format\")\n",
    "    url = f\"{baseurl}/{release}/{table}.{format}\"\n",
    "    if columns:\n",
    "        # check that column values are legal\n",
    "        # create a dictionary to speed this up\n",
    "        dcols = {}\n",
    "        for col in ps1metadata(table,release)['name']:\n",
    "            dcols[col.lower()] = 1\n",
    "        badcols = []\n",
    "        for col in columns:\n",
    "            if col.lower().strip() not in dcols:\n",
    "                badcols.append(col)\n",
    "        if badcols:\n",
    "            raise ValueError('Some columns not found in table: {}'.format(', '.join(badcols)))\n",
    "        # two different ways to specify a list of column values in the API\n",
    "        # data['columns'] = columns\n",
    "        data['columns'] = '[{}]'.format(','.join(columns))\n",
    "\n",
    "# either get or post works\n",
    "#    r = requests.post(url, data=data)\n",
    "    r = requests.get(url, params=data)\n",
    "\n",
    "    if verbose:\n",
    "        print(r.url)\n",
    "    r.raise_for_status()\n",
    "    if format == \"json\":\n",
    "        return r.json()\n",
    "    else:\n",
    "        return r.text\n",
    "\n",
    "\n",
    "def checklegal(table,release):\n",
    "    \"\"\"Checks if this combination of table and release is acceptable\n",
    "    \n",
    "    Raises a VelueError exception if there is problem\n",
    "    \"\"\"\n",
    "    \n",
    "    releaselist = (\"dr1\", \"dr2\")\n",
    "    if release not in (\"dr1\",\"dr2\"):\n",
    "        raise ValueError(\"Bad value for release (must be one of {})\".format(', '.join(releaselist)))\n",
    "    if release==\"dr1\":\n",
    "        tablelist = (\"mean\", \"stack\")\n",
    "    else:\n",
    "        tablelist = (\"mean\", \"stack\", \"detection\")\n",
    "    if table not in tablelist:\n",
    "        raise ValueError(\"Bad value for table (for {} must be one of {})\".format(release, \", \".join(tablelist)))\n",
    "\n",
    "\n",
    "def ps1metadata(table=\"mean\",release=\"dr1\",\n",
    "           baseurl=\"https://catalogs.mast.stsci.edu/api/v0.1/panstarrs\"):\n",
    "    \"\"\"Return metadata for the specified catalog and table\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    table (string): mean, stack, or detection\n",
    "    release (string): dr1 or dr2\n",
    "    baseurl: base URL for the request\n",
    "    \n",
    "    Returns an astropy table with columns name, type, description\n",
    "    \"\"\"\n",
    "    \n",
    "    checklegal(table,release)\n",
    "    url = f\"{baseurl}/{release}/{table}/metadata\"\n",
    "    r = requests.get(url)\n",
    "    r.raise_for_status()\n",
    "    v = r.json()\n",
    "    # convert to astropy table\n",
    "    tab = Table(rows=[(x['name'],x['type'],x['description']) for x in v],\n",
    "               names=('name','type','description'))\n",
    "    return tab\n",
    "\n",
    "\n",
    "def addfilter(dtab):\n",
    "    \"\"\"Add filter name as column in detection table by translating filterID\n",
    "    \n",
    "    This modifies the table in place.  If the 'filter' column already exists,\n",
    "    the table is returned unchanged.\n",
    "    \"\"\"\n",
    "    if 'filter' not in dtab.colnames:\n",
    "        # the filterID value goes from 1 to 5 for grizy\n",
    "        #id2filter = np.array(list('grizy'))\n",
    "        id2filter = np.array(['panstarrs g','panstarrs r','panstarrs i','panstarrs z','panstarrs y'])\n",
    "        dtab['filter'] = id2filter[(dtab['filterID']-1).data]\n",
    "    return dtab\n",
    "\n",
    "def improve_filter_format(tab):\n",
    "# column names don't include which filter it is\n",
    "    for filter in 'grizy':\n",
    "        col = filter+'MeanPSFMag'\n",
    "        tab[col].format = \".4f\"\n",
    "        tab[col][tab[col] == -999.0] = np.nan\n",
    "            \n",
    "    return(tab)\n",
    "\n",
    "def search_lightcurve(objid):\n",
    "    #setup to pull light curve info\n",
    "    dconstraints = {'objID': objid}\n",
    "    dcolumns = (\"\"\"objID,detectID,filterID,obsTime,ra,dec,psfFlux,psfFluxErr,psfMajorFWHM,psfMinorFWHM,\n",
    "            psfQfPerfect,apFlux,apFluxErr,infoFlag,infoFlag2,infoFlag3\"\"\").split(',')\n",
    "    # strip blanks and weed out blank and commented-out values\n",
    "    dcolumns = [x.strip() for x in dcolumns]\n",
    "    dcolumns = [x for x in dcolumns if x and not x.startswith('#')]\n",
    "\n",
    "\n",
    "    #get the actual detections and light curve info for this target\n",
    "    dresults = ps1search(table='detection',release='dr2',columns=dcolumns,**dconstraints)\n",
    "    \n",
    "    return(dresults)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do a panstarrs search\n",
    "radius = 1.0/3600.0 # radius = 1 arcsec\n",
    "#plt.rcParams.update({'font.size': 14})\n",
    "#plt.figure(1,(10,10))\n",
    "\n",
    "        \n",
    "#for all objects in our catalog\n",
    "for ccount, coord in enumerate(coords_list):\n",
    "    #doesn't take SkyCoord\n",
    "    ra = CLAGN['RA'][ccount]\n",
    "    dec = CLAGN['DEC'][ccount]\n",
    "\n",
    "    #see if there is an object in panSTARRS at this location\n",
    "    results = ps1cone(ra,dec,radius,release='dr2')\n",
    "    tab = ascii.read(results)\n",
    "    \n",
    "    # improve the format of the table\n",
    "    tab = improve_filter_format(tab)\n",
    "        \n",
    "    #in case there is more than one object within 1 arcsec, sort them by match distance\n",
    "    tab.sort('distance')\n",
    "    \n",
    "    #if there is an object at that location\n",
    "    if len(tab) > 0:   \n",
    "        #got a live one\n",
    "        #print( 'for object', ccount + 1, 'there is ',len(tab), 'match in panSTARRS', tab['objID'])\n",
    "\n",
    "        #take the closest match as the best match\n",
    "        objid = tab['objID'][0]\n",
    "        \n",
    "        #get the actual detections and light curve info for this target\n",
    "        dresults = search_lightcurve(objid)\n",
    "        \n",
    "        #sometimes there isn't actually a light curve for the target???\n",
    "        try:\n",
    "            ascii.read(dresults)\n",
    "        except FileNotFoundError:\n",
    "            print(\"There is no light curve\")\n",
    "            #no need to store PanSTARRS data for this one\n",
    "        else:\n",
    "            #There is a light curve for this target\n",
    "            \n",
    "            #fix the column names to include filter names\n",
    "            dtab = addfilter(ascii.read(dresults))\n",
    "            dtab.sort('obsTime')\n",
    "\n",
    "            #here is the light curve mixed from all 5 bands\n",
    "            t_panstarrs = dtab['obsTime']\n",
    "            flux_panstarrs = dtab['psfFlux']*1E3  # in mJy\n",
    "            err_panstarrs = dtab['psfFluxErr'] *1E3\n",
    "            filtername = dtab['filter']\n",
    "            \n",
    "            #put this single object light curves into a pandas multiindex dataframe\n",
    "            dfsingle = pd.DataFrame(dict(flux=flux_panstarrs, err=err_panstarrs, time=t_panstarrs, objectid=ccount + 1, band=filtername)).set_index([\"objectid\", \"band\", \"time\"])\n",
    "            \n",
    "            #then concatenate each individual df together\n",
    "            df_lc.append(dfsingle)\n",
    "                \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.set_option('display.max_rows',None)\n",
    "df_lc.data\n",
    "#pd.reset_option('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 MAST: ATLAS all-sky stellar reference catalog (g, r, i) < 19mag\n",
    " -  MAST has this catalog but it is not clear that it has the individual epoch photometry and it is only accessible with casjobs, not through python notebooks.  \n",
    "\n",
    " https://archive.stsci.edu/hlsp/atlas-refcat2#section-a737bc3e-2d56-4827-9ab4-838fbf8d67c1\n",
    " \n",
    " - if we really want to pursue this, we can put in a MAST helpdesk ticket to see if a) they do have the light curves, and b) they could switch the catalog to a searchable with python version.  There are some ways of accessing casjobs through python (<https://github.com/spacetelescope/notebooks/blob/master/notebooks/MAST/HSC/HCV_CASJOBS/HCV_casjobs_demo.ipynb), but apparently not this particular catalog.  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 MAST: TESS, Kepler and K2\n",
    " - use `lightKurve` to search all 3 missions and download light curves\n",
    " - TESS, Kepler, and K2 report flux in units of electrons/s \n",
    "     - there is no good way to convert this to anything more useful because the bandpasses are very wide and nonstandard\n",
    "     - could take it into magnitudes, but that doesn't help get it back to flux density\n",
    "     - really we don't care about absolute scale, but want to scale the light curve to be on the same plot as other light curves\n",
    "     - save as electron/s now and think about how to scale when plotting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_filternames(search_result, numlc):\n",
    "                \n",
    "    filtername = str(search_result[numlc].mission)\n",
    "    #clean this up a bit so all Kepler quarters etc., get the same filtername\n",
    "    #we don't need to track the individual names for the quarters, just need to know which mission it is\n",
    "    if 'Kepler' in filtername:\n",
    "        filtername = 'Kepler'\n",
    "    if 'TESS' in filtername:\n",
    "        filtername = 'TESS'\n",
    "    if 'K2' in filtername:\n",
    "        filtername = 'K2'\n",
    "    return(filtername)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radius = 1.0  #arcseconds\n",
    "\n",
    "#for all objects\n",
    "#for ccount, coord in enumerate(coords_list):\n",
    "#for testing, this has 79 light curves between the three missions.\n",
    "for ccount in range(1):\n",
    "    coord = '19:02:43.1 +50:14:28.7'\n",
    "    \n",
    "    #use lightkurve to search TESS, Kepler and K2\n",
    "    search_result = lk.search_lightcurve(coord, radius = radius)\n",
    "    \n",
    "    #figure out what to do with the results\n",
    "    if len(search_result) < 1:\n",
    "        #there is no data in these missions at this location\n",
    "        print(ccount, ': no match')\n",
    "    else:\n",
    "        #https://docs.lightkurve.org/tutorials/1-getting-started/searching-for-data-products.html\n",
    "        print(ccount, 'got a live one')\n",
    "        #download all of the returned light curves from TESS, Kepler, and K2\n",
    "        lc_collection = search_result.download_all()\n",
    "\n",
    "        #can't get the whole collection directly into pandas multiindex\n",
    "        #pull out inidividual light curves, convert to uniform units, and put them in pandas\n",
    "        for numlc in range(len(search_result)):\n",
    "            \n",
    "            lc = lc_collection[numlc]  #for testing 0 is Kepler, #69 is TESS\n",
    "\n",
    "            #convert to Pandas\n",
    "            lcdf = lc.to_pandas().reset_index()\n",
    "        \n",
    "            #convert time to mjd\n",
    "            time_lc = lcdf.time #in units of time - 2457000 BTJD days\n",
    "            time_lc= time_lc + 2457000 - 2400000.5 #now in MJD days within a few minutes (except for the barycenter correction)\n",
    "\n",
    "            flux_lc = lcdf.flux #in electron/s\n",
    "            fluxerr_lc = lcdf.flux_err #in electron/s\n",
    "\n",
    "            #record band name\n",
    "            filtername = clean_filternames(search_result, numlc)\n",
    "\n",
    "            #put this single object light curves into a pandas multiindex dataframe\n",
    "            # fluxes are in units of electrons/s and will be scaled to fit the other fluxes when plotting\n",
    "            dfsingle = pd.DataFrame(dict(flux=flux_lc, err=fluxerr_lc, time=time_lc, objectid=ccount + 1, band=filtername)).set_index([\"objectid\", \"band\", \"time\"])\n",
    "\n",
    "            #then concatenate each individual df together\n",
    "            df_lc.append(dfsingle)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 MAST: HCV\n",
    " - hubble catalog of variables (https://archive.stsci.edu/hlsp/hcv)\n",
    " - follow notebook here to know how to search and download light curves https://archive.stsci.edu/hst/hsc/help/HCV/HCV_API_demo.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hscapiurl = \"https://catalogs.mast.stsci.edu/api/v0.1/hsc\"\n",
    "\n",
    "def hcvcone(ra,dec,radius,table=\"hcvsummary\",release=\"v3\",format=\"csv\",magtype=\"magaper2\",\n",
    "            columns=None, baseurl=hscapiurl, verbose=False,\n",
    "            **kw):\n",
    "    \"\"\"Do a cone search of the HSC catalog (including the HCV)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ra (float): (degrees) J2000 Right Ascension\n",
    "    dec (float): (degrees) J2000 Declination\n",
    "    radius (float): (degrees) Search radius (<= 0.5 degrees)\n",
    "    table (string): hcvsummary, hcv, summary, detailed, propermotions, or sourcepositions\n",
    "    release (string): v3 or v2\n",
    "    magtype (string): magaper2 or magauto (only applies to summary table)\n",
    "    format: csv, votable, json\n",
    "    columns: list of column names to include (None means use defaults)\n",
    "    baseurl: base URL for the request\n",
    "    verbose: print info about request\n",
    "    **kw: other parameters (e.g., 'numimages.gte':2)\n",
    "    \"\"\"\n",
    "    \n",
    "    data = kw.copy()\n",
    "    data['ra'] = ra\n",
    "    data['dec'] = dec\n",
    "    data['radius'] = radius\n",
    "    return hcvsearch(table=table,release=release,format=format,magtype=magtype,\n",
    "                     columns=columns,baseurl=baseurl,verbose=verbose,**data)\n",
    "\n",
    "\n",
    "def hcvsearch(table=\"hcvsummary\",release=\"v3\",magtype=\"magaper2\",format=\"csv\",\n",
    "              columns=None, baseurl=hscapiurl, verbose=False,\n",
    "           **kw):\n",
    "    \"\"\"Do a general search of the HSC catalog (possibly without ra/dec/radius)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    table (string): hcvsummary, hcv, summary, detailed, propermotions, or sourcepositions\n",
    "    release (string): v3 or v2\n",
    "    magtype (string): magaper2 or magauto (only applies to summary table)\n",
    "    format: csv, votable, json\n",
    "    columns: list of column names to include (None means use defaults)\n",
    "    baseurl: base URL for the request\n",
    "    verbose: print info about request\n",
    "    **kw: other parameters (e.g., 'numimages.gte':2).  Note this is required!\n",
    "    \"\"\"\n",
    "    \n",
    "    data = kw.copy()\n",
    "    if not data:\n",
    "        raise ValueError(\"You must specify some parameters for search\")\n",
    "    if format not in (\"csv\",\"votable\",\"json\"):\n",
    "        raise ValueError(\"Bad value for format\")\n",
    "    url = \"{}.{}\".format(cat2url(table,release,magtype,baseurl=baseurl),format)\n",
    "    if columns:\n",
    "        # check that column values are legal\n",
    "        # create a dictionary to speed this up\n",
    "        dcols = {}\n",
    "        for col in hcvmetadata(table,release,magtype)['name']:\n",
    "            dcols[col.lower()] = 1\n",
    "        badcols = []\n",
    "        for col in columns:\n",
    "            if col.lower().strip() not in dcols:\n",
    "                badcols.append(col)\n",
    "        if badcols:\n",
    "            raise ValueError('Some columns not found in table: {}'.format(', '.join(badcols)))\n",
    "        # two different ways to specify a list of column values in the API\n",
    "        # data['columns'] = columns\n",
    "        data['columns'] = '[{}]'.format(','.join(columns))\n",
    "\n",
    "    # either get or post works\n",
    "    # r = requests.post(url, data=data)\n",
    "    r = requests.get(url, params=data)\n",
    "\n",
    "    if verbose:\n",
    "        print(r.url)\n",
    "    r.raise_for_status()\n",
    "    if format == \"json\":\n",
    "        return r.json()\n",
    "    else:\n",
    "        return r.text\n",
    "\n",
    "\n",
    "def hcvmetadata(table=\"hcvsummary\",release=\"v3\",magtype=\"magaper2\",baseurl=hscapiurl):\n",
    "    \"\"\"Return metadata for the specified catalog and table\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    table (string): hcvsummary, hcv, summary, detailed, propermotions, or sourcepositions\n",
    "    release (string): v3 or v2\n",
    "    magtype (string): magaper2 or magauto (only applies to summary table)\n",
    "    baseurl: base URL for the request\n",
    "    \n",
    "    Returns an astropy table with columns name, type, description\n",
    "    \"\"\"\n",
    "    url = \"{}/metadata\".format(cat2url(table,release,magtype,baseurl=baseurl))\n",
    "    r = requests.get(url)\n",
    "    r.raise_for_status()\n",
    "    v = r.json()\n",
    "    # convert to astropy table\n",
    "    tab = Table(rows=[(x['name'],x['type'],x['description']) for x in v],\n",
    "               names=('name','type','description'))\n",
    "    return tab\n",
    "\n",
    "\n",
    "def cat2url(table=\"hcvsummary\",release=\"v3\",magtype=\"magaper2\",baseurl=hscapiurl):\n",
    "    \"\"\"Return URL for the specified catalog and table\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    table (string): hcvsummary, hcv, summary, detailed, propermotions, or sourcepositions\n",
    "    release (string): v3 or v2\n",
    "    magtype (string): magaper2 or magauto (only applies to summary table)\n",
    "    baseurl: base URL for the request\n",
    "    \n",
    "    Returns a string with the base URL for this request\n",
    "    \"\"\"\n",
    "    checklegal_hcv(table,release,magtype)\n",
    "    if table == \"summary\":\n",
    "        url = \"{baseurl}/{release}/{table}/{magtype}\".format(**locals())\n",
    "    else:\n",
    "        url = \"{baseurl}/{release}/{table}\".format(**locals())\n",
    "    return url\n",
    "\n",
    "\n",
    "def checklegal_hcv(table,release,magtype):\n",
    "    \"\"\"Checks if this combination of table, release and magtype is acceptable\n",
    "    \n",
    "    Raises a ValueError exception if there is problem\n",
    "    \"\"\"\n",
    "    \n",
    "    releaselist = (\"v2\", \"v3\")\n",
    "    if release not in releaselist:\n",
    "        raise ValueError(\"Bad value for release (must be one of {})\".format(\n",
    "            ', '.join(releaselist)))\n",
    "    if release==\"v2\":\n",
    "        tablelist = (\"summary\", \"detailed\")\n",
    "    else:\n",
    "        tablelist = (\"summary\", \"detailed\", \"propermotions\", \"sourcepositions\",\n",
    "                    \"hcvsummary\", \"hcv\")\n",
    "    if table not in tablelist:\n",
    "        raise ValueError(\"Bad value for table (for {} must be one of {})\".format(\n",
    "            release, \", \".join(tablelist)))\n",
    "    if table == \"summary\":\n",
    "        magtypelist = (\"magaper2\", \"magauto\")\n",
    "        if magtype not in magtypelist:\n",
    "            raise ValueError(\"Bad value for magtype (must be one of {})\".format(\n",
    "                \", \".join(magtypelist)))\n",
    "\n",
    "\n",
    "def resolve(name):\n",
    "    \"\"\"Get the RA and Dec for an object using the MAST name resolver\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    name (str): Name of object\n",
    "\n",
    "    Returns RA, Dec tuple with position\n",
    "    \"\"\"\n",
    "\n",
    "    resolverRequest = {'service':'Mast.Name.Lookup',\n",
    "                       'params':{'input':name,\n",
    "                                 'format':'json'\n",
    "                                },\n",
    "                      }\n",
    "    resolvedObjectString = mastQuery(resolverRequest)\n",
    "    resolvedObject = json.loads(resolvedObjectString)\n",
    "    # The resolver returns a variety of information about the resolved object, \n",
    "    # however for our purposes all we need are the RA and Dec\n",
    "    try:\n",
    "        objRa = resolvedObject['resolvedCoordinate'][0]['ra']\n",
    "        objDec = resolvedObject['resolvedCoordinate'][0]['decl']\n",
    "    except IndexError as e:\n",
    "        raise ValueError(\"Unknown object '{}'\".format(name))\n",
    "    return (objRa, objDec)\n",
    "\n",
    "\n",
    "def mastQuery(request, url='https://mast.stsci.edu/api/v0/invoke'):\n",
    "    \"\"\"Perform a MAST query.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    request (dictionary): The MAST request json object\n",
    "    url (string): The service URL\n",
    "\n",
    "    Returns the returned data content\n",
    "    \"\"\"\n",
    "    \n",
    "    # Encoding the request as a json string\n",
    "    requestString = json.dumps(request)\n",
    "    r = requests.post(url, data={'request': requestString})\n",
    "    r.raise_for_status()\n",
    "    return r.text\n",
    "\n",
    "def convertACSmagtoflux(date, filterstring, mag, magerr):\n",
    "    \n",
    "    #date is currently in MJD and needs to be in ISO Format (YYYY-MM-DD)\n",
    "    #use astropy to handle this properly\n",
    "    t = Time(date, format = 'mjd')\n",
    "    \n",
    "    #t.iso has this, but also the hrs, min, sec, so need to truncate those\n",
    "    tiso = t.iso[0:10]\n",
    "    q = acszpt.Query(date=tiso, detector='WFC', filt=filterstring)\n",
    "    zpt_table = q.fetch()\n",
    "    print(zpt_table)\n",
    "    zpt = zpt_table['VEGAmag'].value\n",
    "    \n",
    "    #ACS provides conversion to erg/s/cm^2/angstrom\n",
    "    flux = 10**((mag - zpt)/(-2.5))  #now in erg/s/cm^2/angstrom\n",
    "    #calculate the error\n",
    "    magupper = mag + magerr\n",
    "    maglower = mag - magerr\n",
    "    flux_upper = abs(flux - (10**((magupper - zpt)/(-2.5))))\n",
    "    flux_lower =  abs(flux - (10**((maglower - zpt)/(-2.5))))\n",
    "    \n",
    "    fluxerr = (flux_upper + flux_lower) / 2.0\n",
    "\n",
    "    #now convert from erg/s/cm^2/angstrom to Jy\n",
    "    #first convert angstrom to Hz using c\n",
    "    c = 3E8\n",
    "    flux = flux * c /(1E-10) #now in erg/s/cm^2/Hz\n",
    "    fluxerr = fluxerr * c /(1E-10) #now in erg/s/cm^2/Hz\n",
    "\n",
    "    flux = flux * (1E-23) # now in Jy\n",
    "    \n",
    "    fluxerr = fluxerr * (1E-23) # now in Jy\n",
    "\n",
    "    return flux, fluxerr\n",
    "            \n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do an HCV search\n",
    "radius = 1.0/3600.0 # radius = 1 arcsec\n",
    "\n",
    "for ccount, coord in enumerate(coords_list):\n",
    "\n",
    "    #doesn't take SkyCoord\n",
    "    ra = CLAGN['RA'][ccount]\n",
    "    dec = CLAGN['DEC'][ccount]\n",
    "\n",
    "    #IC 1613 from the demo for testing\n",
    "    #ra = 16.19913\n",
    "    #dec = 2.11778\n",
    "    \n",
    "    #look in the summary table for anything within a radius of our targets\n",
    "    tab = hcvcone(ra,dec,radius,table=\"hcvsummary\")\n",
    "    if tab == '':\n",
    "        print (ccount, 'no matches')\n",
    "    else:\n",
    "        print(ccount, 'got a live one')\n",
    "        \n",
    "        tab = ascii.read(tab)\n",
    "                \n",
    "        matchid = tab['MatchID'][0]  #take the first one, assuming it is the nearest match\n",
    "        \n",
    "        #just pulling one filter for an example (more filters are available)\n",
    "        try:\n",
    "            src_814 = ascii.read(hcvsearch(table='hcv',MatchID=matchid,Filter='ACS_F814W'))\n",
    "        except FileNotFoundError:\n",
    "            #that filter doesn't exist for this target\n",
    "            print(\"no F814W filter info for this target\")\n",
    "        else:        \n",
    "            time_814 = src_814['MJD']\n",
    "            mag_814 = src_814['CorrMag']  #need to convert this to flux\n",
    "            magerr_814 = src_814['MagErr']\n",
    "\n",
    "            filterstring = 'F814W'\n",
    "\n",
    "            #uggg, ACS has time dependent flux zero points.....\n",
    "            #going to cheat for now and only use one time, but could imagine this as a loop\n",
    "            #https://www.stsci.edu/hst/instrumentation/acs/data-analysis/zeropoints\n",
    "            flux, fluxerr = convertACSmagtoflux(time_814[0], filterstring, mag_814, magerr_814)\n",
    "            flux = flux *1E3 #convert to mJy\n",
    "            fluxerr = fluxerr*1E3 #convert to mJy\n",
    "            \n",
    "            #put this single object light curves into a pandas multiindex dataframe\n",
    "            dfsingle_814 = pd.DataFrame(dict(flux=flux, err=fluxerr, time=time_814, objectid=ccount + 1, band='F814W')).set_index([\"objectid\", \"band\", \"time\"])\n",
    "\n",
    "            #then concatenate each individual df together\n",
    "            df_lc.append(dfsingle_814)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Find light curves for these targets in relevant, non-NASA catalogs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaia (Faisst)\n",
    "- astroquery.gaia will presumably work out of the box for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "############ EXTRACT GAIA DATA FOR OBJECTS ##########\n",
    "## Note: This is very slow. Can probably make faster with direct SQL search?\n",
    "\n",
    "## Select Gaia table (DR3)\n",
    "Gaia.MAIN_GAIA_TABLE = \"gaiaedr3.gaia_source\"\n",
    "\n",
    "## Define search radius\n",
    "radius = u.Quantity(20, u.arcsec)\n",
    "\n",
    "## Search and Cross match.\n",
    "# This can be done in a smarter way by matching catalogs on the Gaia server, or grouping the\n",
    "# sources and search a larger area.\n",
    "\n",
    "# get catalog\n",
    "gaia_table = Table()\n",
    "t1 = time.time()\n",
    "for cc,coord in enumerate(coords_list):\n",
    "    print(len(coords_list)-cc , end=\" \")\n",
    "\n",
    "    gaia_search = Gaia.cone_search_async(coordinate=coord, radius=radius , background=True)\n",
    "    gaia_search.get_data()[\"dist\"].unit = \"deg\"\n",
    "    gaia_search.get_data()[\"dist\"] = gaia_search.get_data()[\"dist\"].to(u.arcsec) # Change distance unit from degrees to arcseconds\n",
    "    \n",
    "    \n",
    "    # match\n",
    "    if len(gaia_search.get_data()[\"dist\"]) > 0:\n",
    "        gaia_search.get_data()[\"input_object_name\"] = CLAGN[\"Object Name\"][cc] # add input object name to catalog\n",
    "        gaia_search.get_data()[\"input_object_id\"] = CLAGN[\"No.\"][cc] # add input object name to catalog\n",
    "        sel_min = np.where( (gaia_search.get_data()[\"dist\"] < 1*u.arcsec) & (gaia_search.get_data()[\"dist\"] == np.nanmin(gaia_search.get_data()[\"dist\"]) ) )[0]\n",
    "    else:\n",
    "        sel_min = []\n",
    "        \n",
    "    #print(\"Number of sources matched: {}\".format(len(sel_min)) )\n",
    "    \n",
    "    if len(sel_min) > 0:\n",
    "        gaia_table = vstack( [gaia_table , gaia_search.get_data()[sel_min]] )\n",
    "    else:\n",
    "        gaia_table = vstack( [gaia_table , gaia_search.get_data()[sel_min]] )\n",
    "\n",
    "print(\"\\nSearch completed in {:.2f} seconds\".format((time.time()-t1) ) )\n",
    "print(\"Number of objects mached: {} out of {}.\".format(len(gaia_table),len(CLAGN) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "########## EXTRACT PHOTOMETRY #########\n",
    "# Once we matched the objects, we have to extract the photometry for them. Here we extract\n",
    "# the mean photometry (later we will do the time series).\n",
    "# Note that the fluxes are in e/s, not very useful. However, there are magnitudes (what unit??) but without errors.\n",
    "# We can get the errors from the flux errors?\n",
    "# Also note that we should include the source_id in order to search for epoch photometry\n",
    "\n",
    "## Define keys (columns) that will be used later. Also add wavelength in angstroms for each filter\n",
    "other_keys = [\"source_id\",\"phot_g_n_obs\",\"phot_rp_n_obs\",\"phot_bp_n_obs\"] # some other useful info\n",
    "mag_keys = [\"phot_bp_mean_mag\" , \"phot_g_mean_mag\" , \"phot_rp_mean_mag\"]\n",
    "magerr_keys = [\"phot_bp_mean_mag_error\" , \"phot_g_mean_mag_error\" , \"phot_rp_mean_mag_error\"]\n",
    "flux_keys = [\"phot_bp_mean_flux\" , \"phot_g_mean_flux\" , \"phot_rp_mean_flux\"]\n",
    "fluxerr_keys = [\"phot_bp_mean_flux_error\" , \"phot_g_mean_flux_error\" , \"phot_rp_mean_flux_error\"]\n",
    "mag_lambda = [\"5319.90\" , \"6735.42\" , \"7992.90\"]\n",
    "\n",
    "## Get photometry. Note that this includes only objects that are \n",
    "# matched to the catalog. We have to add the missing ones later.\n",
    "_phot = gaia_table[mag_keys]\n",
    "_err = hstack( [ 2.5/np.log(10) * gaia_table[e]/gaia_table[f] for e,f in zip(fluxerr_keys,flux_keys) ] )\n",
    "gaia_phot2 = hstack( [_phot , _err] )\n",
    "\n",
    "## Clean up (change units and column names)\n",
    "_ = [gaia_phot2.rename_column(f,m) for m,f in zip(magerr_keys,fluxerr_keys)]\n",
    "for key in magerr_keys:\n",
    "    gaia_phot2[key].unit = \"mag\"\n",
    "gaia_phot2[\"input_object_name\"] = gaia_table[\"input_object_name\"].copy()\n",
    "\n",
    "## Add Some other useful information\n",
    "for key in other_keys:\n",
    "    gaia_phot2[key] = gaia_table[key]\n",
    "\n",
    "\n",
    "## Also add object for which we don't have photometry.\n",
    "# Add Nan for now, need to think about proper format. Also, there are probably smarter ways to do this.\n",
    "# We do this by matching the object names from the original catalog to the photometry catalog. Then add\n",
    "# an entry [np.nan, ...] if it does not exist. To make life easier, we add a dummy entry as the first\n",
    "# row so we can compy all the \n",
    "gaia_phot = Table( names=gaia_phot2.keys() , dtype=gaia_phot2.dtype )\n",
    "for ii in range(len(CLAGN)):\n",
    "    sel = np.where( CLAGN[\"Object Name\"][ii] == gaia_phot2[\"input_object_name\"] )[0]\n",
    "    if len(sel) > 0:\n",
    "        gaia_phot = vstack([gaia_phot , gaia_phot2[sel] ])\n",
    "    else:\n",
    "        tmp = Table( np.repeat(np.NaN , len(gaia_phot2.keys())) , names=gaia_phot2.keys() , dtype=gaia_phot2.dtype )\n",
    "        gaia_phot = vstack([gaia_phot , tmp ])\n",
    "        \n",
    "## Some cleanup:\n",
    "gaia_phot[\"source_id\"][gaia_phot[\"source_id\"] < 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "######## EXTRACT LIGHT CURVES ##########\n",
    "# Now since we have matched the objects to the Gaia catalog, we can also extract the full light curves using\n",
    "# the Gaia IDs.\n",
    "\n",
    "## Log in (apparently not necessary for small queries) =========\n",
    "#Gaia.login(user=None , password=None)\n",
    "\n",
    "\n",
    "## For each of the objects, request the EPOCH_PHOTOMETRY from the Gaia DataLink Service =======\n",
    "# We first define some function:\n",
    "# 1. Gaia_retrieve_EPOCH_PHOTOMETRY: to retrieve the epoch photometry for a given Gaia (DR3) ID\n",
    "# 2. Gaia_mk_lightcurves: to create light curves from the table downloaded with DataLink\n",
    "#                         from the Gaia server using the function Gaia_retrieve_EPOCH_PHOTOMETRY()\n",
    "# 3. Gaia_mk_MultibandTimeSeries: creates a MultibandTimeSeries object (not really used)\n",
    "# 4. Gaia_mk_MultiIndex: creates a Pandas MultiIndex data frame, which can be added to the existing\n",
    "#                        data frame containing the other observations.\n",
    "\n",
    "\n",
    "## Define function to retrieve epoch photometry\n",
    "def Gaia_retrieve_EPOCH_PHOTOMETRY(ids, verbose):\n",
    "    '''\n",
    "    Function to retrieve EPOCH_PHOTOMETRY (or actually any) catalog product for Gaia\n",
    "    entries using the DataLink. Note that the IDs need to be DR3 source_id and needs to be a list.\n",
    "    \n",
    "    Code fragments taken from: https://www.cosmos.esa.int/web/gaia-users/archive/datalink-products\n",
    "    \n",
    "    INPUT:\n",
    "        - ids: List of Gaia DR3 source IDs (source_id). \n",
    "    \n",
    "    OUTPUT:\n",
    "        - Dictionary (key = source_id) with a table of photometry as a function of time\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    ## Some Definitions\n",
    "    retrieval_type = 'EPOCH_PHOTOMETRY'# Options are: 'EPOCH_PHOTOMETRY', 'MCMC_GSPPHOT', 'MCMC_MSC', 'XP_SAMPLED', 'XP_CONTINUOUS', 'RVS', 'ALL'\n",
    "    data_structure = 'INDIVIDUAL'   # Options are: 'INDIVIDUAL', 'COMBINED', 'RAW'\n",
    "    data_release   = 'Gaia DR3'     # Options are: 'Gaia DR3' (default), 'Gaia DR2'\n",
    "\n",
    "    ## Get the files\n",
    "    datalink = Gaia.load_data(ids=ids,\n",
    "                              data_release = data_release,\n",
    "                              retrieval_type=retrieval_type,\n",
    "                              data_structure = data_structure, verbose = False, output_file = None , overwrite_output_file=True)\n",
    "    dl_keys  = list(datalink.keys())\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'The following Datalink products have been downloaded:')\n",
    "        for dl_key in dl_keys:\n",
    "            print(f' * {dl_key}')\n",
    "    \n",
    "    ## Extract the info\n",
    "    prod_tab = dict() # Dictionary to save the light curves. The key is the source_id\n",
    "    for dd in ids:\n",
    "        if verbose: print(\"{}: \".format(dd) , end=\" \")\n",
    "        this_dl_key = 'EPOCH_PHOTOMETRY-Gaia DR3 {}.xml'.format(dd)\n",
    "        if this_dl_key in datalink.keys():\n",
    "            prod_tab[str(dd)] = datalink[this_dl_key][0].to_table()\n",
    "            if verbose: print(\"found\")\n",
    "        else:\n",
    "            pass\n",
    "            if verbose: print(\"not found\")\n",
    "    \n",
    "    return(prod_tab)\n",
    "\n",
    "## Define function to extract light curve from product table.\n",
    "def Gaia_mk_lightcurves(prod_tab):\n",
    "    '''\n",
    "    This function creates light curves from the table downloaded with DataLink from the Gaia server.\n",
    "    \n",
    "    INPUT:\n",
    "        - prod_tab: product table downloaded via datalink, produced by `Gaia_retrieve_EPOCH_PHOTOMETRY()`.\n",
    "        \n",
    "    OUTPUT:\n",
    "        - A dictionary (key = source_id) including a dictionary of light curves for bands \"G\", \"BP\", \"RP\". Each\n",
    "        of them includes a time stamp (`time_jd` and `time_isot`) a magnitude (`mag`) and magnitude error (`magerr`).\n",
    "    '''\n",
    "    \n",
    "    bands = [\"G\",\"BP\",\"RP\"]\n",
    "    output = dict()\n",
    "    for ii,key in enumerate(list(prod_tab.keys()) ):\n",
    "        print(key)\n",
    "    \n",
    "        output[str(key)] = dict()\n",
    "        for band in bands:\n",
    "            sel_band = np.where( (prod_tab[key][\"band\"] == band) & (prod_tab[key][\"rejected_by_photometry\"] == False) )[0]\n",
    "            print(\"Number of entries for band {}: {}\".format(band , len(sel_band)))\n",
    "            \n",
    "            time_jd = prod_tab[key][sel_band][\"time\"] + 2455197.5 # What unit???\n",
    "            time_isot = Time(time_jd , format=\"jd\").isot\n",
    "            mag = prod_tab[key][sel_band][\"mag\"]\n",
    "            magerr = 2.5/np.log(10) * prod_tab[key][sel_band][\"flux_error\"]/prod_tab[key][sel_band][\"flux\"]\n",
    "            \n",
    "            output[str(key)][band] = Table([time_jd , time_isot , mag , magerr] , names=[\"time_jd\",\"time_isot\",\"mag\",\"magerr\"] ,\n",
    "                                           dtype = [float , str , float , float], units=[u.d , \"\"  , u.mag , u.mag])\n",
    "            \n",
    "    return(output)\n",
    "\n",
    "## Function to add light curves into Multi-Band Time Series object\n",
    "def Gaia_mk_MultibandTimeSeries(epoch_phot):\n",
    "    '''\n",
    "    Creates MultibandTimeSeries object from epoch photometry lightcurves\n",
    "    \n",
    "    INPUT\n",
    "        - epoch_phot: Epoch photometry light curve (see `Gaia_mk_lightcurves`)\n",
    "        \n",
    "    OUTPUT\n",
    "        - Dictionary of MultibandTimeSeries light curves (for each source_id)\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    \n",
    "    ## For each source, create a MultibandTimeSeries object and add the bands.\n",
    "    bands = [\"G\",\"BP\",\"RP\"]\n",
    "    out = dict()\n",
    "    for key in epoch_phot.keys():\n",
    "        \n",
    "        # Initialize\n",
    "        ts = MultibandTimeSeries()\n",
    "        \n",
    "        # Add bands\n",
    "        for band in bands:\n",
    "            ts.add_band(time = Time(epoch_phot[str(key)][band][\"time_jd\"] , format=\"jd\") ,\n",
    "                 data = epoch_phot[str(key)][band][\"mag\"],\n",
    "                 band_name=band\n",
    "                )\n",
    "        out[key] = ts\n",
    "        \n",
    "    return(out)\n",
    "\n",
    "\n",
    "def Gaia_mk_MultiIndex(data , gaia_phot , gaia_epoch_phot , verbose):\n",
    "    '''\n",
    "    Creates a MultiIndex Pandas Dataframe for the Gaia observations. Specifically, it \n",
    "    returns the epoch photometry as a function of time. For sources without Gaia epoch\n",
    "    photometry, it just returns the mean photometry a epoch 2015-09-24T19:40:33.468, which\n",
    "    is the average epoch of the observations of sources with multi-epoch photometry.\n",
    "    \n",
    "    INPUT:\n",
    "    - data: the catalog with the source IDs and names (here: CLAGN)\n",
    "    - gaia_phot: The Gaia mean photometry (will be linked by object ID in 'data' catalog)\n",
    "    - gaia_epoch_phot: The Gaia epoch photometry (is a dictionary created by 'Gaia_mk_lightcurves()' function)\n",
    "    - verbose: verbosity level (0=silent)\n",
    "    \n",
    "    OUTPUT:\n",
    "    A Pandas data frame with indices (\"objectid\",\"band\",\"time\"). The data frame contains flux and flux error,\n",
    "    both in mJy. The output can be appended to another lightcurve Pandas data\n",
    "    frame via df_lc_object.append(df_lc)\n",
    "    '''\n",
    "\n",
    "    for ii in range(len(data)):\n",
    "        print(\"{} matched to: \".format( data[\"Object Name\"][ii])  , end=\" \")\n",
    "\n",
    "        ## Check if this object has a Gaia light curve:\n",
    "\n",
    "        # get Gaia source_id\n",
    "        sel = np.where(data[\"Object Name\"][ii] == gaia_phot[\"input_object_name\"])[0]\n",
    "        if len(sel) > 0:\n",
    "            source_id = gaia_phot[\"source_id\"][sel[0]]\n",
    "            print(source_id , end=\" \")\n",
    "\n",
    "            if str(source_id) in gaia_epoch_phot.keys(): # Match to Gaia multi-epoch catalog\n",
    "                print(\"Has Gaia epoch photometry\")\n",
    "\n",
    "                for band in [\"G\",\"BP\",\"RP\"]:\n",
    "\n",
    "                    # get data\n",
    "                    d = gaia_epoch_phot[str(source_id)][band][\"time_isot\"]\n",
    "                    t = Time(d , format=\"isot\") # convert to time object\n",
    "                    y = gaia_epoch_phot[str(source_id)][band][\"mag\"]\n",
    "                    dy = gaia_epoch_phot[str(source_id)][band][\"magerr\"]\n",
    "\n",
    "                    # compute flux and flux error in mJy\n",
    "                    y2 = 10**(-0.4*(y - 23.9))/1e3 # in mJy\n",
    "                    dy2 = dy / 2.5 * np.log(10) * y2 # in mJy\n",
    "\n",
    "                    # create single instance\n",
    "                    dfsingle = pd.DataFrame(\n",
    "                                dict(flux=np.asarray(y2), # in mJy\n",
    "                                 err=np.asarray(dy2), # in mJy\n",
    "                                 time=t.mjd, # in MJD\n",
    "                                 #objectid=gaia_phot[\"input_object_name\"][sel],\n",
    "                                 objectid=np.repeat(ii+1, len(y)),\n",
    "                                 band=\"Gaia {}\".format(band.lower())\n",
    "                                                )\n",
    "                                           ).set_index([\"objectid\", \"band\", \"time\"])\n",
    "\n",
    "                    # add to table\n",
    "                    try:\n",
    "                        df_lc\n",
    "                    except NameError:\n",
    "                        #df_lc doesn't exist (yet)\n",
    "                        df_lc = dfsingle.copy()\n",
    "                    else:\n",
    "                        #df_lc_gaia exists\n",
    "                        df_lc = pd.concat([df_lc, dfsingle])\n",
    "\n",
    "            else: # No match to Gaia multi-epoch catalog: use single epoch photometry\n",
    "                print(\"No Gaia epoch photometry, append single epoch photometry \")\n",
    "\n",
    "                for band in [\"G\",\"BP\",\"RP\"]:\n",
    "\n",
    "                    # get data\n",
    "                    t = Time(\"2015-09-24T19:40:33.468\" , format=\"isot\") # just random date: FIXME: NEED TO GET ACTUAL OBSERVATION TIME!\n",
    "                    y = gaia_phot[\"phot_{}_mean_mag\".format(band.lower())][sel]\n",
    "                    dy = gaia_phot[\"phot_{}_mean_mag_error\".format(band.lower())][sel]\n",
    "\n",
    "                    # compute flux and flux error in mJy\n",
    "                    y2 = 10**(-0.4*(y - 23.9))/1e3 # in mJy\n",
    "                    dy2 = dy / 2.5 * np.log(10) * y2 # in mJy\n",
    "\n",
    "                    # create single instance\n",
    "                    dfsingle = pd.DataFrame(\n",
    "                                dict(flux=np.asarray(y2), # in mJy\n",
    "                                 err=np.asarray(dy2), # in mJy\n",
    "                                 time=t.mjd, # in MJD\n",
    "                                 #objectid=gaia_phot[\"input_object_name\"][sel],\n",
    "                                 objectid=np.repeat(ii+1, len(y)),\n",
    "                                 band=\"Gaia {}\".format(band.lower())\n",
    "                                    )\n",
    "                    ).set_index([\"objectid\", \"band\", \"time\"])\n",
    "\n",
    "                    # add to table\n",
    "                    try:\n",
    "                        df_lc\n",
    "                    except NameError:\n",
    "                        #df_lc doesn't exist (yet)\n",
    "                        df_lc = dfsingle.copy()\n",
    "                    else:\n",
    "                        #df_lc_gaia exists\n",
    "                        df_lc = pd.concat([df_lc, dfsingle])\n",
    "\n",
    "        else: # no match to Gaia\n",
    "            print(\"none\")\n",
    "            \n",
    "    return(df_lc)\n",
    "\n",
    "## Run search\n",
    "ids = list(gaia_phot[\"source_id\"])\n",
    "prod_tab = Gaia_retrieve_EPOCH_PHOTOMETRY(ids=ids , verbose=False)\n",
    "\n",
    "## Create light curves\n",
    "gaia_epoch_phot = Gaia_mk_lightcurves(prod_tab)\n",
    "\n",
    "## Create MultiBandTimeSeries photometry object\n",
    "gaia_multibandTS_phot = Gaia_mk_MultibandTimeSeries(epoch_phot = gaia_epoch_phot)\n",
    "\n",
    "## Create Gaia Pandas MultiIndex object and append to existing data frame.\n",
    "df_lc_gaia = Gaia_mk_MultiIndex(data=CLAGN , gaia_phot=gaia_phot , gaia_epoch_phot=gaia_epoch_phot , verbose = 1)\n",
    "\n",
    "## Append to existing MultiIndex object (if exists)\n",
    "#df_lc = MultiIndexDFObject()\n",
    "df_lc.append(df_lc_gaia)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MAKE FIGURE OF ONE LIGHT CURVE FOR GAIA ###\n",
    "\n",
    "## First get the ids/names of sources that have Gaia multi-epoch observations.\n",
    "object_ids = list(df_lc.data.index.levels[0]) # get list of objectids in multiIndex table\n",
    "#print(object_ids)\n",
    "\n",
    "fig = plt.figure(figsize=(12,4))\n",
    "plt.subplots_adjust(wspace=0.3)\n",
    "axs = [ fig.add_subplot(1,3,ii+1) for ii in range(3) ]\n",
    "cmap = plt.get_cmap(\"Spectral\")\n",
    "\n",
    "for dd in object_ids:\n",
    "    try:\n",
    "    \n",
    "        for bb,band in enumerate([\"G\",\"BP\",\"RP\"]):\n",
    "        \n",
    "            this_tab = df_lc.data.loc[dd,\"Gaia {}\".format(band.lower()),:].reset_index(inplace=False)\n",
    "            t = Time(this_tab[\"time\"] , format=\"mjd\") # convert to time object\n",
    "            #axs[bb].plot(t.mjd , this_tab[\"flux\"] , \"-\" , linewidth=1 , markersize=0.1)\n",
    "            axs[bb].errorbar(t.mjd , this_tab[\"flux\"] , yerr=this_tab[\"err\"] , fmt=\"-o\",linewidth=0.5 , markersize=3 , label=\"{}\".format(dd))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "for ii in range(3):\n",
    "    axs[ii].set_title(np.asarray([\"G\",\"BP\",\"RP\"])[ii])\n",
    "    axs[ii].legend(fontsize=6 , ncol=3)\n",
    "    axs[ii].set_xlabel(\"MJD (Days)\" , fontsize=10)\n",
    "    axs[ii].set_ylabel(r\"Flux ($\\mu$Jy)\", fontsize=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lc.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASAS-SN (all sky automated survey for supernovae) has a website that can be manually searched (Faisst)\n",
    "- see if astroquery.vizier can find it\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### icecube has a 2008 - 2018 catalog which we can download and is small (Faisst)\n",
    "- https://icecube.wisc.edu/data-releases/2021/01/all-sky-point-source-icecube-data-years-2008-2018/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Make plots of luminosity as a function of time\n",
    "- model plots after https://arxiv.org/pdf/2111.09391.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#pick one to look at for now\n",
    "obj = 15\n",
    "singleobj = df_lc.data.loc[(obj),:,:]\n",
    "\n",
    "#set up for plotting\n",
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "\n",
    "#first check to see which bands we have in the dataframe\n",
    "availband = singleobj.index.unique('band')\n",
    "max_list = []\n",
    "\n",
    "#plot the bands one at a time\n",
    "for l in range(len(availband)):\n",
    "    band_lc = singleobj.loc[(obj, availband[l]), :]\n",
    "    #band_lc = singleobj.loc[availband[l], :] # above line doesn't work for me [ALF]\n",
    "    band_lc.reset_index(inplace = True)\n",
    "\n",
    "    #first clean dataframe to remove erroneous rows\n",
    "    band_lc_clean = band_lc[band_lc['time'] < 65000]\n",
    "\n",
    "    #before plotting need to scale the Kepler, K2, and TESS fluxes to the other available fluxes\n",
    "    if availband[l] in ['Kepler', 'K2', 'TESS']:\n",
    "        #find the maximum value of 'other bands'\n",
    "        max_electrons = max(band_lc_clean.flux)\n",
    "        factor = np.mean(max_list)/ max_electrons\n",
    "        ax.errorbar(band_lc_clean.time, band_lc_clean.flux * factor, band_lc_clean.err* factor, capsize = 3.0,label = availband[l])\n",
    "    elif availband[l] in ['zg','zr','zi']:\n",
    "        max_list.append(max(band_lc_clean.flux)) \n",
    "        ax.errorbar(band_lc_clean.time, band_lc_clean.flux, band_lc_clean.err, capsize = 1.0,marker='.',linestyle='', label = 'ZTF '+str(availband[l]))\n",
    "\n",
    "    else:\n",
    "        max_list.append(max(band_lc_clean.flux)) \n",
    "        ax.errorbar(band_lc_clean.time, band_lc_clean.flux, band_lc_clean.err, capsize = 3.0, label = availband[l])\n",
    "\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.set_xlabel('Time(MJD)')\n",
    "ax.set_ylabel('Flux(mJy)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Extension \n",
    "Consider training a ML model to do light curve classification based on this sample of CLAGN\n",
    " - once we figure out which bands these are likely to be observed in, could then have a optical + IR light curve classifier\n",
    " - what would the features of the light curve be?\n",
    " - what models are reasonable to test as light curve classifiers?\n",
    " - could we make also a sample of TDEs, SNe, flaring AGN? - then train the model to distinguish between these things?\n",
    " - need a sample of non-flaring light curves\n",
    " \n",
    "After training the model:\n",
    " - would then need a sample of optical + IR light curves for \"all\" galaxies = big data to run the model on.\n",
    "\n",
    "Some resources to consider:\n",
    "- https://github.com/dirac-institute/ZTF_Boyajian\n",
    "- https://ui.adsabs.harvard.edu/abs/2022AJ....164...68S/abstract\n",
    "- https://ui.adsabs.harvard.edu/abs/2019ApJ...881L...9F/abstract\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citations\n",
    "\n",
    "to the various codes used (astroquery etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make multiwavelength light curves using archival data\n",
    "***\n",
    "\n",
    "## Learning Goals    \n",
    "By the end of this tutorial, you will be able to:\n",
    " - automatically load a catalog of sources\n",
    " - automatically search NASA and non-NASA resources for light curves\n",
    " - store light curves in a Pandas multiindex dataframe\n",
    " - plot all light curves on the same plot\n",
    " \n",
    " \n",
    "## Introduction:\n",
    " - A user has a sample of interesting targets for which they would like to see a plot of available archival light curves.  We start with a small set of changing look AGN from Yang et al., 2018, which are automatically downloaded. Changing look AGN are cases where the broad emission lines appear or disappear (and not just that the flux is variable). We model light curve plots after van Velzen et al. 2021.  We search through a curated list of time-domain NASA holdings as well as non-NASA sources.  HEASARC catalogs used are Fermi and Beppo-Sax, IRSA catalogs used are ZTF and WISE, and MAST catalogs used are Pan-Starrs, TESS, Kepler, and K2.  Non-NASA sources are Gaia and IceCube. This list is hopefully generalized enough to include many types of targets to make this notebook interesting for many types of science.  All of these time-domain archives are searched in an automated fashion using astroquery.\n",
    " - Light curve data storage is a tricky problem.  Currently we are using a multi-index Pandas dataframe, but it is not clear that this is the best choice or the best choice for scaling up.  One downside is that we need to manually track the units of flux and time instead of relying on an astropy storage scheme which would be able to do the units worrying for us.  Astropy does not currently have a good option for multi band light curve storage.\n",
    " - We intend to explore two possible extensions to this project.  The first is an image extension where we would search the archives for images of these sources.  The second, is exploring a ML classifier for these changing look AGN light curves.\n",
    " \n",
    "## Input:\n",
    " - a catalog of CLAGN from the literature\n",
    "\n",
    "## Output:\n",
    " - an archival optical + IR + neutrino light curve\n",
    " \n",
    "## Technical Goals:\n",
    " - should be able to run from a clean checkout from github\n",
    " - should be able to automatically download all catalogs & images used\n",
    " - need to have all photometry in the same physical unit\n",
    " - need to have a data structure that is easy to use but holds light curve information (time and units) and is extendable to ML applications\n",
    " - need to have a curated list of catalogs to search for photometry that is generalizeable to other input catalogs\n",
    " \n",
    "## Non-standard Imports:\n",
    "- `astroquery` to interface with archives APIs\n",
    "- `astropy` to work with coordinates/units and data structures\n",
    "- `AXS` (astronomy extensions for Spark) to handle large catalog cross matching\n",
    "- `lightkurve` to search TESSS, Kepler, and K2 archives\n",
    "- `urllib` to handle archive searches with website interface\n",
    "\n",
    "## Authors:\n",
    "IPAC SP team\n",
    "\n",
    "## Acknowledgements:\n",
    "Suvi Gezari, Antara Basu-zych,\\\n",
    "MAST, HEASARC, & IRSA Fornax teams\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import axs\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import requests\n",
    "\n",
    "from astroquery.ipac.ned import Ned\n",
    "from astroquery.heasarc import Heasarc\n",
    "from astroquery.gaia import Gaia\n",
    "\n",
    "from astropy.coordinates import SkyCoord\n",
    "import astropy.units as u\n",
    "from astropy.table import Table, vstack, hstack, unique\n",
    "from astropy.io import ascii\n",
    "from astropy.time import Time\n",
    "\n",
    "\n",
    "try: # Python 3.x\n",
    "    from urllib.parse import quote as urlencode\n",
    "    from urllib.request import urlretrieve\n",
    "except ImportError:  # Python 2.x\n",
    "    from urllib import pathname2url as urlencode\n",
    "    from urllib import urlretrieve\n",
    "\n",
    "!pip install lightkurve --upgrade\n",
    "import lightkurve as lk\n",
    "\n",
    "!pip install acstools\n",
    "from acstools import acszpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define the Sample\n",
    " use the following paper to make a sample of CLAGN: https://iopscience.iop.org/article/10.3847/1538-4357/aaca3a \n",
    "\n",
    " This sample can later be switched out to a differen/larger sample of \"interesting\" targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use ADS to find the refcode for this paper (ie., '2018blahblah')\n",
    "#Then use the NED astroquery interface to load the table from the paper with relevant target info\n",
    "#This loads the table into an astropy table\n",
    "\n",
    "CLAGN = Ned.query_refcode('2018ApJ...862..109Y')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#see what is included in this table\n",
    "CLAGN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a list of skycoords from target ra and dec\n",
    "#This is useful for some codes which understand that these are a set of coordinates\n",
    "coords_list = [\n",
    "    SkyCoord(ra, dec, frame='icrs', unit='deg')\n",
    "    for ra, dec in zip(CLAGN['RA'], CLAGN['DEC'])\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Find light curves for these targets in NASA catalogs\n",
    "- data access concerns:\n",
    "    - can't ask the archives to search their entire holdings\n",
    "        - not good enough meta data\n",
    "        - not clear that the data is all vetted and good enough to include for science\n",
    "        - all catalogs have differently named columns so how would we know which columns to keep\n",
    "    - instead work with a curated list of catalogs for each archive\n",
    "        - focus on general surveys\n",
    "        - try to ensure that this list is also appropriate for a generalization of this use case to other input catalogs\n",
    "        - could astroquery.NED be useful in finding a generalized curated list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 HEASARC: FERMI & Beppo SAX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mission_list = ['FERMIGTRIG', 'SAXGRBMGRB']\n",
    "radius = 0.1*u.degree\n",
    "#for testing\n",
    "for ccount in range(1):\n",
    "    #To get a fermigtrig source\n",
    "    coord = SkyCoord('03h41m21.2s -89d00m33.0s', frame='icrs')\n",
    "\n",
    "    #to get a bepposax source\n",
    "    #coord = SkyCoord('14h32m00.0s -88d00m00.0s', frame='icrs')\n",
    "\n",
    "#for ccount, coord in enumerate(coords_list):\n",
    "    #use astroquery to search that position for either a Fermi or Beppo Sax trigger\n",
    "    for mcount, mission in enumerate(mission_list):\n",
    "        try:\n",
    "            results = Heasarc.query_region(coord, mission = mission, radius = radius)#, sortvar = 'SEARCH_OFFSET_')\n",
    "            #really just need to save the one time of the Gamma ray detection\n",
    "            #time is already in MJD for both catalogs\n",
    "            if mission == 'FERMIGTRIG':\n",
    "                time_mjd = results['TRIGGER_TIME']\n",
    "                time_mjd = float(time_mjd)\n",
    "            else:\n",
    "                time_mjd = float(results['TIME'][0])\n",
    "                \n",
    "            type(time_mjd)\n",
    "            #build the storage for this result\n",
    "            #really just need to mark this spot with a vertical line in the plot\n",
    "            dfsingle = pd.DataFrame(dict(flux=[0.0001], err=[0.0001], time=[time_mjd], objectid=[ccount + 1], band=[mission])).set_index([\"objectid\", \"band\", \"time\"])\n",
    "            try:\n",
    "                df_lc\n",
    "            except NameError:\n",
    "                #df_lc doesn't exist (yet)\n",
    "                df_lc = dfsingle\n",
    "            else:\n",
    "                #df_lc exists\n",
    "                df_lc = pd.concat([df_lc, dfsingle])\n",
    "\n",
    "        except AttributeError:\n",
    "            print(\"no results at that location for \", mission)\n",
    "\n",
    "\n",
    "#**** These HEASARC searches are returning an attribute error because of an astroquery bug\n",
    "# bug submitted to astroquery Oct 18, waiting for a fix.\n",
    "# if that gets fixed, can probably change this cell "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 IRSA: ZTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python package ztfquery is not a good solution for this because it requires IRSA password\n",
    "#Instead will construct the URL for an API query\n",
    "#https://irsa.ipac.caltech.edu/docs/program_interface/ztf_lightcurve_api.html\n",
    "ztf_radius = 0.000278   #as suggested by Dave Shupe\n",
    "\n",
    "for ccount, coord in enumerate(coords_list):\n",
    "    #doesn't take SkyCoord\n",
    "    ra = CLAGN['RA'][ccount]\n",
    "    dec = CLAGN['DEC'][ccount]\n",
    "    #make the string for the URL query\n",
    "    #ask for all three bands (g, r, i)\n",
    "    #don't want data that is flagged as unusable by the ZTF pipeline\n",
    "    urlstr = 'https://irsa.ipac.caltech.edu/cgi-bin/ZTF/nph_light_curves?POS=CIRCLE %f %f %f&BANDNAME=g,r,i&FORMAT=ipac_table&BAD_CATFLAGS_MASK=32768'%(ra, dec,ztf_radius)\n",
    "\n",
    "    response = requests.get(urlstr)\n",
    "    if response.ok:\n",
    "        ztf_lc = ascii.read(response.text, format='ipac')\n",
    "        #print(count, len(ztf_lc))\n",
    "        #this could be up to 3 light curves because there are 3 filters\n",
    "        #need to sort by filtercode 'zg','zr','zi'\n",
    "        #and store the light curves\n",
    "    else:\n",
    "        print(ccount, \" There is no ZTF light curve at this position\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 IRSA:WISE\n",
    "\n",
    "- Dave Shupe has made a catalog of NEOWISE light curves of half the sky in a parquet file\n",
    "\n",
    "- Pandas is not a good option for reading in and working with this catalog because it is so large (2 billion rows?)\n",
    "\n",
    "- Instead we can use AXS to cross match the CLAGN sample with the NEOWISE catalog to find those rows in neowise which correspond to the CLAGN sample. AXS is a part of spark. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#could load the neowise light curves into pandas, but would need to severely\n",
    "# filter the catalog to get it to fit into memory.  Since these targets are all over the sky\n",
    "# it is not obvious how to filter the catalog\n",
    "\n",
    "#Here is one way it could work in Pandas if we had a way to filter significantly before matching\n",
    "#subset = pd.read_parquet('/stage/irsa-data-download10/parquet-work/NEOWISE-R/neowise_lc_half.parquet',\n",
    "#                    engine='pyarrow', \n",
    "#                    filters=[ ('ra', '<', 121) , ('ra', '>', 120) , \n",
    "#                            ('dec', '<', 68) , ('dec', '>', -9),\n",
    "#                            ('cw_w1mpro', '>', 15.0) ])\n",
    "#\n",
    "#len(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start up SPARK\n",
    "os.environ['SPARK_CONF_DIR'] = '/home/jkrick/axs_store/conf_alt'\n",
    "\n",
    "def spark_start(work_dir, database_dir, warehouse_dir):\n",
    "    from pyspark.sql import SparkSession\n",
    "    import os\n",
    "    \n",
    "    spark = (\n",
    "            SparkSession.builder\n",
    "            .appName(\"spark trial\")\n",
    "            .config(\"spark.sql.warehouse.dir\", warehouse_dir)\n",
    "            .config('spark.master', \"local[20]\")\n",
    "            .config('spark.driver.memory', '64G') # 128\n",
    "            .config('spark.executor.memory', '30G')\n",
    "            .config('spark.local.dir', work_dir)\n",
    "            .config('spark.memory.offHeap.enabled', 'true')\n",
    "            .config('spark.memory.offHeap.size', '128G') # 256\n",
    "            .config(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "            .config(\"spark.driver.maxResultSize\", \"60G\")\n",
    "            .config(\"spark.driver.extraJavaOptions\", \n",
    "                    f\"-Dderby.system.home={database_dir}\")\n",
    "            .config(\"spark.sql.hive.metastore.sharedPrefixes\",\n",
    "                    \"org.apache.derby\")\n",
    "            .enableHiveSupport()\n",
    "            .getOrCreate()\n",
    "                    )   \n",
    "\n",
    "    return spark\n",
    "\n",
    "spark_session = spark_start(\n",
    "    \"/stage/irsa-staff-jkrick/spark_work\",\n",
    "    \"/home/jkrick/axs_store\",\n",
    "    \"/stage/irsa-staff-jkrick/sp_axs_warehouse/warehouse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if the catalog we want is not yet available, import it\n",
    "catalog = axs.AxsCatalog(spark_session)\n",
    "catlist = catalog.list_table_names()\n",
    "\n",
    "if 'neowise_lc_half' not in catlist:\n",
    "    catalog.import_existing_table('neowise_lc_half', \n",
    "        '/stage/irsa-data-download10/parquet-work/NEOWISE-R/neowise_lc_half.parquet',\n",
    "        import_into_spark=True)\n",
    "    \n",
    "\n",
    "#now figure out how to get the CLAGN catalog into AXS\n",
    "#can't go direct from astropy table into AXS, so first need to convert astropy to pandas, then to AXS\n",
    "\n",
    "if 'axs_clagn' not in catlist:\n",
    "\n",
    "    pd_CLAGN = CLAGN.to_pandas()\n",
    "\n",
    "    #then pandas to spark dataframe\n",
    "    sp_CLAGN = spark_session.createDataFrame(pd_CLAGN)\n",
    "\n",
    "    #saving below can't handle capital \"RA\" and \"DEC\", so need to change that\n",
    "    #also can't handle column names with spaces in them so need to rename those as well.\n",
    "    sp_CLAGN2 = sp_CLAGN.withColumnRenamed(\"RA\",\"ra\").withColumnRenamed(\"DEC\",\"dec\").withColumnRenamed(\"Object Name\", \"Object_name\").withColumnRenamed(\"Redshift Flag\",\"redshift_flag\").withColumnRenamed(\"Magnitude and Filter\", \"magnitude_and_filter\").withColumnRenamed(\"Photometry Points\",\"photometry_points\").withColumnRenamed(\"Redshift Points\", \"redshift_points\").withColumnRenamed(\"Diameter Points\",\"diameter_points\")\n",
    "\n",
    "    #now save spark to AXS\n",
    "    catalog.save_axs_table(sp_CLAGN2, 'AXS_CLAGN', calculate_zone=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just confirm that worked:\n",
    "#catalog = axs.AxsCatalog(spark_session)\n",
    "#catlist = catalog.list_table_names()\n",
    "#catlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lazy load in the NEOWISE and CLAGN catalog\n",
    "neowise_lc_half = catalog.load('neowise_lc_half')\n",
    "axs_clagn = catalog.load('axs_clagn')\n",
    "\n",
    "#rename column name which is causing problems\n",
    "axs_clagn = axs_clagn.withColumnRenamed(\"No.\", \"objnum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lazy crossmatch CLAGN with NEOWISE\n",
    "\n",
    "neowise_CLAGN = neowise_lc_half.crossmatch(axs_clagn, 2*axs.Constants.ONE_ASEC, return_min = True, include_dist_col = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#lazy evaluation means the cross match won't happen until this cell gets executed\n",
    "#how many matches did we get?\n",
    "neowise_CLAGN.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#result is an AXS table, but want to get it into pandas\n",
    "#this is slow (18 min for limited set of columns) but is the fastest I have found\n",
    "\n",
    "#make a smaller version with just the light curve info to save\n",
    "neowise_lc = neowise_CLAGN.select('objnum','Object_name','w1pmag','w1pmagerr','w2pmag','w2pmagerr', 'mjd')\n",
    "\n",
    "#convert to Pandas dataframe\n",
    "pd_neowise_lc = neowise_lc.toPandas()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to convert those magnitudes into flux to be consistent in data structure.\n",
    "#go to Janskies\n",
    "#using zeropoints from here: https://wise2.ipac.caltech.edu/docs/release/allsky/expsup/sec4_4h.html\n",
    "def convert_WISEtoJanskies(mag, magerr, band):\n",
    "    if band == 'w1':\n",
    "        zpt = 309.54\n",
    "    elif band == 'w2':\n",
    "        zpt = 171.787\n",
    "            \n",
    "    flux_Jy = zpt*(10**(-mag/2.5))\n",
    "    \n",
    "    #calculate the error\n",
    "    magupper = mag + magerr\n",
    "    maglower = mag - magerr\n",
    "    flux_upper = abs(flux_Jy - (zpt*(10**(-magupper/2.5))))\n",
    "    flux_lower = abs(flux_Jy - (zpt*(10**(-maglower/2.5))))\n",
    "    \n",
    "    fluxerr_Jy = (flux_upper + flux_lower) / 2.0\n",
    "    \n",
    "    return flux_Jy, fluxerr_Jy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_decimal_float(changetype):\n",
    "\n",
    "    #columns which are themselves arrays\n",
    "    changetype['mjd'] = [changetype.mjd[i].astype(float) for i in range(len(changetype))]\n",
    "    changetype['w1mag'] = [changetype.w1pmag[i].astype(float) for i in range(len(changetype))]\n",
    "    changetype['w2mag'] = [changetype.w2pmag[i].astype(float) for i in range(len(changetype))]\n",
    "    changetype['w1error'] = [changetype.w1pmagerr[i].astype(float) for i in range(len(changetype))]\n",
    "    changetype['w2error'] = [changetype.w2pmagerr[i].astype(float) for i in range(len(changetype))]\n",
    "\n",
    "    return changetype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decimals are causing problems when trying to change the units of these columns,\n",
    "# so change their type to floats which are more universally useable\n",
    "floats = convert_decimal_float(pd_neowise_lc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ack, w2 errors don't have same lengths as flux arrays!!\n",
    "#well, we need to exclude w2 for now and figure this out\n",
    "#Dave thinks it was a problem with null values getting dropped when he made the catalog\n",
    "#need to re-make the catalog in order for this to get fixed\n",
    "for i in range(29):\n",
    "    print(i, floats['w2mag'][i].size, floats['w2error'][i].size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make new columns with fluxes instead of magnitudes for comparison with other bands\n",
    "w1flux, w1fluxerr = convert_WISEtoJanskies(floats['w1mag'],floats['w1error'] ,'w1')\n",
    "flux_lc = pd_neowise_lc.assign(w1flux = w1flux, w1fluxerr = w1fluxerr)\n",
    "\n",
    "#deprecated until we get w2 uncertainties of the same length as magnitudes\n",
    "#w2flux, w2fluxerr = convert_WISEtoJanskies(floats['w2mag'],floats['w2error'] ,'w2')\n",
    "#flux_lc = flux_lc.assign(w2flux = w2flux, w2fluxerr = w2fluxerr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the resulting (Jy) light curves to the pandas multiindex data frame\n",
    "#syntax isn't working and getting data type errors to convert the whole dataframe of arrays at once\n",
    "#instead try iterating over rows in df and making individual data frames which we then concatenate together.\n",
    "#this is crazy inefficient, but....I can't make it work any other way\n",
    "pd_neowise_lc.reset_index()\n",
    "for index, row in flux_lc.iterrows():\n",
    "    dfw1 = pd.DataFrame(dict(flux=row.w1flux, err=row.w1fluxerr, time=row.mjd, objectid=row.objnum, band='w1')).set_index([\"objectid\", \"band\", \"time\"])\n",
    "\n",
    "    #then concatenate each individual df together\n",
    "    #first make sure that df has been defined before\n",
    "    try:\n",
    "        df_lc\n",
    "    except NameError:\n",
    "        #df_lc doesn't exist (yet)\n",
    "        df_lc = dfw1\n",
    "    else:\n",
    "        #df_lc exists\n",
    "        df_lc = pd.concat([df_lc, dfw1])#, dfw2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 MAST: Pan-STARRS\n",
    "Code ideas taken from this website: https://ps1images.stsci.edu/ps1_dr2_api.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ps1cone(ra,dec,radius,table=\"mean\",release=\"dr1\",format=\"csv\",columns=None,\n",
    "           baseurl=\"https://catalogs.mast.stsci.edu/api/v0.1/panstarrs\", verbose=False,\n",
    "           **kw):\n",
    "    \"\"\"Do a cone search of the PS1 catalog\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ra (float): (degrees) J2000 Right Ascension\n",
    "    dec (float): (degrees) J2000 Declination\n",
    "    radius (float): (degrees) Search radius (<= 0.5 degrees)\n",
    "    table (string): mean, stack, or detection\n",
    "    release (string): dr1 or dr2\n",
    "    format: csv, votable, json\n",
    "    columns: list of column names to include (None means use defaults)\n",
    "    baseurl: base URL for the request\n",
    "    verbose: print info about request\n",
    "    **kw: other parameters (e.g., 'nDetections.min':2)\n",
    "    \"\"\"\n",
    "    \n",
    "    data = kw.copy()\n",
    "    data['ra'] = ra\n",
    "    data['dec'] = dec\n",
    "    data['radius'] = radius\n",
    "    return ps1search(table=table,release=release,format=format,columns=columns,\n",
    "                    baseurl=baseurl, verbose=verbose, **data)\n",
    "\n",
    "\n",
    "def ps1search(table=\"mean\",release=\"dr1\",format=\"csv\",columns=None,\n",
    "           baseurl=\"https://catalogs.mast.stsci.edu/api/v0.1/panstarrs\", verbose=False,\n",
    "           **kw):\n",
    "    \"\"\"Do a general search of the PS1 catalog (possibly without ra/dec/radius)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    table (string): mean, stack, or detection\n",
    "    release (string): dr1 or dr2\n",
    "    format: csv, votable, json\n",
    "    columns: list of column names to include (None means use defaults)\n",
    "    baseurl: base URL for the request\n",
    "    verbose: print info about request\n",
    "    **kw: other parameters (e.g., 'nDetections.min':2).  Note this is required!\n",
    "    \"\"\"\n",
    "    \n",
    "    data = kw.copy()\n",
    "    if not data:\n",
    "        raise ValueError(\"You must specify some parameters for search\")\n",
    "    checklegal(table,release)\n",
    "    if format not in (\"csv\",\"votable\",\"json\"):\n",
    "        raise ValueError(\"Bad value for format\")\n",
    "    url = f\"{baseurl}/{release}/{table}.{format}\"\n",
    "    if columns:\n",
    "        # check that column values are legal\n",
    "        # create a dictionary to speed this up\n",
    "        dcols = {}\n",
    "        for col in ps1metadata(table,release)['name']:\n",
    "            dcols[col.lower()] = 1\n",
    "        badcols = []\n",
    "        for col in columns:\n",
    "            if col.lower().strip() not in dcols:\n",
    "                badcols.append(col)\n",
    "        if badcols:\n",
    "            raise ValueError('Some columns not found in table: {}'.format(', '.join(badcols)))\n",
    "        # two different ways to specify a list of column values in the API\n",
    "        # data['columns'] = columns\n",
    "        data['columns'] = '[{}]'.format(','.join(columns))\n",
    "\n",
    "# either get or post works\n",
    "#    r = requests.post(url, data=data)\n",
    "    r = requests.get(url, params=data)\n",
    "\n",
    "    if verbose:\n",
    "        print(r.url)\n",
    "    r.raise_for_status()\n",
    "    if format == \"json\":\n",
    "        return r.json()\n",
    "    else:\n",
    "        return r.text\n",
    "\n",
    "\n",
    "def checklegal(table,release):\n",
    "    \"\"\"Checks if this combination of table and release is acceptable\n",
    "    \n",
    "    Raises a VelueError exception if there is problem\n",
    "    \"\"\"\n",
    "    \n",
    "    releaselist = (\"dr1\", \"dr2\")\n",
    "    if release not in (\"dr1\",\"dr2\"):\n",
    "        raise ValueError(\"Bad value for release (must be one of {})\".format(', '.join(releaselist)))\n",
    "    if release==\"dr1\":\n",
    "        tablelist = (\"mean\", \"stack\")\n",
    "    else:\n",
    "        tablelist = (\"mean\", \"stack\", \"detection\")\n",
    "    if table not in tablelist:\n",
    "        raise ValueError(\"Bad value for table (for {} must be one of {})\".format(release, \", \".join(tablelist)))\n",
    "\n",
    "\n",
    "def ps1metadata(table=\"mean\",release=\"dr1\",\n",
    "           baseurl=\"https://catalogs.mast.stsci.edu/api/v0.1/panstarrs\"):\n",
    "    \"\"\"Return metadata for the specified catalog and table\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    table (string): mean, stack, or detection\n",
    "    release (string): dr1 or dr2\n",
    "    baseurl: base URL for the request\n",
    "    \n",
    "    Returns an astropy table with columns name, type, description\n",
    "    \"\"\"\n",
    "    \n",
    "    checklegal(table,release)\n",
    "    url = f\"{baseurl}/{release}/{table}/metadata\"\n",
    "    r = requests.get(url)\n",
    "    r.raise_for_status()\n",
    "    v = r.json()\n",
    "    # convert to astropy table\n",
    "    tab = Table(rows=[(x['name'],x['type'],x['description']) for x in v],\n",
    "               names=('name','type','description'))\n",
    "    return tab\n",
    "\n",
    "\n",
    "def addfilter(dtab):\n",
    "    \"\"\"Add filter name as column in detection table by translating filterID\n",
    "    \n",
    "    This modifies the table in place.  If the 'filter' column already exists,\n",
    "    the table is returned unchanged.\n",
    "    \"\"\"\n",
    "    if 'filter' not in dtab.colnames:\n",
    "        # the filterID value goes from 1 to 5 for grizy\n",
    "        id2filter = np.array(list('grizy'))\n",
    "        dtab['filter'] = id2filter[(dtab['filterID']-1).data]\n",
    "    return dtab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do a panstarrs search\n",
    "radius = 1.0/3600.0 # radius = 1 arcsec\n",
    "#plt.rcParams.update({'font.size': 14})\n",
    "#plt.figure(1,(10,10))\n",
    "\n",
    "        \n",
    "#for all objects in our catalog\n",
    "for ccount, coord in enumerate(coords_list):\n",
    "    #doesn't take SkyCoord\n",
    "    ra = CLAGN['RA'][ccount]\n",
    "    dec = CLAGN['DEC'][ccount]\n",
    "\n",
    "    #see if there is an object in panSTARRS at this location\n",
    "    results = ps1cone(ra,dec,radius,release='dr2')\n",
    "    tab = ascii.read(results)\n",
    "    \n",
    "    # improve the format\n",
    "    # column names don't include which filter it is\n",
    "    for filter in 'grizy':\n",
    "        col = filter+'MeanPSFMag'\n",
    "        tab[col].format = \".4f\"\n",
    "        tab[col][tab[col] == -999.0] = np.nan\n",
    "        \n",
    "    #in case there is more than one object within 1 arcsec, sort them by match distance\n",
    "    tab.sort('distance')\n",
    "    \n",
    "    #if there is an object at that location\n",
    "    if len(tab) > 0:   \n",
    "        #got a live one\n",
    "        #print( 'for object', ccount + 1, 'there is ',len(tab), 'match in panSTARRS', tab['objID'])\n",
    "\n",
    "        #take the closest match as the best match\n",
    "        objid = tab['objID'][0]\n",
    "        \n",
    "        #setup to pull light curve info\n",
    "        dconstraints = {'objID': objid}\n",
    "        dcolumns = (\"\"\"objID,detectID,filterID,obsTime,ra,dec,psfFlux,psfFluxErr,psfMajorFWHM,psfMinorFWHM,\n",
    "                    psfQfPerfect,apFlux,apFluxErr,infoFlag,infoFlag2,infoFlag3\"\"\").split(',')\n",
    "        # strip blanks and weed out blank and commented-out values\n",
    "        dcolumns = [x.strip() for x in dcolumns]\n",
    "        dcolumns = [x for x in dcolumns if x and not x.startswith('#')]\n",
    "\n",
    "\n",
    "        #get the actual detections and light curve info for this target\n",
    "        dresults = ps1search(table='detection',release='dr2',columns=dcolumns,**dconstraints)\n",
    "        \n",
    "        #sometimes there isn't actually a light curve for the target???\n",
    "        try:\n",
    "            ascii.read(dresults)\n",
    "        except FileNotFoundError:\n",
    "            print(\"There is no light curve\")\n",
    "            #no need to store PanSTARRS data for this one\n",
    "        else:\n",
    "            #There is a light curve for this target\n",
    "            \n",
    "            #fix the column names to include filter names\n",
    "            dtab = addfilter(ascii.read(dresults))\n",
    "            dtab.sort('obsTime')\n",
    "\n",
    "            #here is the light curve mixed from all 5 bands\n",
    "            t_panstarrs = dtab['obsTime']\n",
    "            flux_panstarrs = dtab['psfFlux']  # in Jy\n",
    "            err_panstarrs = dtab['psfFluxErr']\n",
    "            filtername = dtab['filter']\n",
    "            \n",
    "            #put this single object light curves into a pandas multiindex dataframe\n",
    "            dfsingle = pd.DataFrame(dict(flux=flux_panstarrs, err=err_panstarrs, time=t_panstarrs, objectid=ccount + 1, band=filtername)).set_index([\"objectid\", \"band\", \"time\"])\n",
    "            \n",
    "            #then concatenate each individual df together\n",
    "            #first make sure that df has been defined before\n",
    "            try:\n",
    "                df_lc\n",
    "            except NameError:\n",
    "                #df_lc doesn't exist (yet)\n",
    "                df_lc = dfsingle\n",
    "            else:\n",
    "                #df_lc exists\n",
    "                df_lc = pd.concat([df_lc, dfsingle])\n",
    "                \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.set_option('display.max_rows',None)\n",
    "df_lc\n",
    "#pd.reset_option('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 MAST: ATLAS all-sky stellar reference catalog (g, r, i) < 19mag\n",
    " -  MAST has this catalog but it is not clear that it has the individual epoch photometry and it is only accessible with casjobs, not through python notebooks.  \n",
    "\n",
    " https://archive.stsci.edu/hlsp/atlas-refcat2#section-a737bc3e-2d56-4827-9ab4-838fbf8d67c1\n",
    " \n",
    " - if we really want to pursue this, we can put in a MAST helpdesk ticket to see if a) they do have the light curves, and b) they could switch the catalog to a searchable with python version.  There are some ways of accessing casjobs through python (<https://github.com/spacetelescope/notebooks/blob/master/notebooks/MAST/HSC/HCV_CASJOBS/HCV_casjobs_demo.ipynb), but apparently not this particular catalog.  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 MAST: TESS, Kepler and K2\n",
    " - use `lightKurve` to search all 3 missions and download light curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radius = 1.0  #arcseconds\n",
    "\n",
    "#for all objects\n",
    "#for ccount, coord in enumerate(coords_list):\n",
    "#for testing, this has 79 light curves between the three missions.\n",
    "for ccount in range(1):\n",
    "    coord = '19:02:43.1 +50:14:28.7'\n",
    "    \n",
    "    #use lightkurve to search TESS, Kepler and K2\n",
    "    search_result = lk.search_lightcurve(coord, radius = radius)\n",
    "    \n",
    "    #figure out what to do with the results\n",
    "    if len(search_result) < 1:\n",
    "        #there is no data in these missions at this location\n",
    "        print(ccount, ': no match')\n",
    "    else:\n",
    "        #https://docs.lightkurve.org/tutorials/1-getting-started/searching-for-data-products.html\n",
    "        print(ccount, 'got a live one')\n",
    "        #download all of the returned light curves from TESS, Kepler, and K2\n",
    "        lc_collection = search_result.download_all()\n",
    "\n",
    "        #can't get the whole collection directly into pandas multiindex\n",
    "        #pull out inidividual light curves, convert to uniform units, and put them in pandas\n",
    "        for numlc in range(len(search_result)):\n",
    "            \n",
    "            lc = lc_collection[numlc]  #for testing 0 is Kepler, #69 is TESS\n",
    "\n",
    "            #convert to Pandas\n",
    "            lcdf = lc.to_pandas().reset_index()\n",
    "        \n",
    "            #convert time to mjd\n",
    "            time_lc = lcdf.time #in units of time - 2457000 BTJD days\n",
    "            time_lc= time_lc + 2457000 - 2400000.5 #now in MJD days within a few minutes (except for the barycenter correction)\n",
    "\n",
    "            #convert flux from electrons/s to Jy\n",
    "            #can't figure out how to do this\n",
    "            #could take it into magnitudes, but that doesn't help get it back to flux density\n",
    "            #really we don't care about absolute scale, but want to scale the light curve to be on the same plot as other light curves\n",
    "            #save as electron/s now and think about how to scale when plotting\n",
    "            flux_lc = lcdf.flux #in electron/s\n",
    "            fluxerr_lc = lcdf.flux_err #in electron/s\n",
    "\n",
    "            #record band name\n",
    "            filtername = str(search_result[numlc].mission)\n",
    "            #clean this up a bit so all Kepler quarters etc., get the same filtername\n",
    "            #we don't need to track the individual names for the quarters, just need to know which mission it is\n",
    "            if 'Kepler' in filtername:\n",
    "                filtername = 'Kepler'\n",
    "            if 'TESS' in filtername:\n",
    "                filtername = 'TESS'\n",
    "            if 'K2' in filtername:\n",
    "                filtername = 'K2'\n",
    "\n",
    "            #put this single object light curves into a pandas multiindex dataframe\n",
    "            dfsingle = pd.DataFrame(dict(flux=flux_lc, err=fluxerr_lc, time=time_lc, objectid=ccount + 1, band=filtername)).set_index([\"objectid\", \"band\", \"time\"])\n",
    "\n",
    "            #then concatenate each individual df together\n",
    "            #first make sure that df has been defined before\n",
    "            try:\n",
    "                df_lc\n",
    "            except NameError:\n",
    "                #df_lc doesn't exist (yet)\n",
    "                df_lc = dfsingle\n",
    "            else:\n",
    "                #df_lc exists\n",
    "                df_lc = pd.concat([df_lc, dfsingle])\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 MAST: HCV\n",
    " - hubble catalog of variables (https://archive.stsci.edu/hlsp/hcv)\n",
    " - follow notebook here to know how to search and download light curves https://archive.stsci.edu/hst/hsc/help/HCV/HCV_API_demo.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hscapiurl = \"https://catalogs.mast.stsci.edu/api/v0.1/hsc\"\n",
    "\n",
    "def hcvcone(ra,dec,radius,table=\"hcvsummary\",release=\"v3\",format=\"csv\",magtype=\"magaper2\",\n",
    "            columns=None, baseurl=hscapiurl, verbose=False,\n",
    "            **kw):\n",
    "    \"\"\"Do a cone search of the HSC catalog (including the HCV)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ra (float): (degrees) J2000 Right Ascension\n",
    "    dec (float): (degrees) J2000 Declination\n",
    "    radius (float): (degrees) Search radius (<= 0.5 degrees)\n",
    "    table (string): hcvsummary, hcv, summary, detailed, propermotions, or sourcepositions\n",
    "    release (string): v3 or v2\n",
    "    magtype (string): magaper2 or magauto (only applies to summary table)\n",
    "    format: csv, votable, json\n",
    "    columns: list of column names to include (None means use defaults)\n",
    "    baseurl: base URL for the request\n",
    "    verbose: print info about request\n",
    "    **kw: other parameters (e.g., 'numimages.gte':2)\n",
    "    \"\"\"\n",
    "    \n",
    "    data = kw.copy()\n",
    "    data['ra'] = ra\n",
    "    data['dec'] = dec\n",
    "    data['radius'] = radius\n",
    "    return hcvsearch(table=table,release=release,format=format,magtype=magtype,\n",
    "                     columns=columns,baseurl=baseurl,verbose=verbose,**data)\n",
    "\n",
    "\n",
    "def hcvsearch(table=\"hcvsummary\",release=\"v3\",magtype=\"magaper2\",format=\"csv\",\n",
    "              columns=None, baseurl=hscapiurl, verbose=False,\n",
    "           **kw):\n",
    "    \"\"\"Do a general search of the HSC catalog (possibly without ra/dec/radius)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    table (string): hcvsummary, hcv, summary, detailed, propermotions, or sourcepositions\n",
    "    release (string): v3 or v2\n",
    "    magtype (string): magaper2 or magauto (only applies to summary table)\n",
    "    format: csv, votable, json\n",
    "    columns: list of column names to include (None means use defaults)\n",
    "    baseurl: base URL for the request\n",
    "    verbose: print info about request\n",
    "    **kw: other parameters (e.g., 'numimages.gte':2).  Note this is required!\n",
    "    \"\"\"\n",
    "    \n",
    "    data = kw.copy()\n",
    "    if not data:\n",
    "        raise ValueError(\"You must specify some parameters for search\")\n",
    "    if format not in (\"csv\",\"votable\",\"json\"):\n",
    "        raise ValueError(\"Bad value for format\")\n",
    "    url = \"{}.{}\".format(cat2url(table,release,magtype,baseurl=baseurl),format)\n",
    "    if columns:\n",
    "        # check that column values are legal\n",
    "        # create a dictionary to speed this up\n",
    "        dcols = {}\n",
    "        for col in hcvmetadata(table,release,magtype)['name']:\n",
    "            dcols[col.lower()] = 1\n",
    "        badcols = []\n",
    "        for col in columns:\n",
    "            if col.lower().strip() not in dcols:\n",
    "                badcols.append(col)\n",
    "        if badcols:\n",
    "            raise ValueError('Some columns not found in table: {}'.format(', '.join(badcols)))\n",
    "        # two different ways to specify a list of column values in the API\n",
    "        # data['columns'] = columns\n",
    "        data['columns'] = '[{}]'.format(','.join(columns))\n",
    "\n",
    "    # either get or post works\n",
    "    # r = requests.post(url, data=data)\n",
    "    r = requests.get(url, params=data)\n",
    "\n",
    "    if verbose:\n",
    "        print(r.url)\n",
    "    r.raise_for_status()\n",
    "    if format == \"json\":\n",
    "        return r.json()\n",
    "    else:\n",
    "        return r.text\n",
    "\n",
    "\n",
    "def hcvmetadata(table=\"hcvsummary\",release=\"v3\",magtype=\"magaper2\",baseurl=hscapiurl):\n",
    "    \"\"\"Return metadata for the specified catalog and table\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    table (string): hcvsummary, hcv, summary, detailed, propermotions, or sourcepositions\n",
    "    release (string): v3 or v2\n",
    "    magtype (string): magaper2 or magauto (only applies to summary table)\n",
    "    baseurl: base URL for the request\n",
    "    \n",
    "    Returns an astropy table with columns name, type, description\n",
    "    \"\"\"\n",
    "    url = \"{}/metadata\".format(cat2url(table,release,magtype,baseurl=baseurl))\n",
    "    r = requests.get(url)\n",
    "    r.raise_for_status()\n",
    "    v = r.json()\n",
    "    # convert to astropy table\n",
    "    tab = Table(rows=[(x['name'],x['type'],x['description']) for x in v],\n",
    "               names=('name','type','description'))\n",
    "    return tab\n",
    "\n",
    "\n",
    "def cat2url(table=\"hcvsummary\",release=\"v3\",magtype=\"magaper2\",baseurl=hscapiurl):\n",
    "    \"\"\"Return URL for the specified catalog and table\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    table (string): hcvsummary, hcv, summary, detailed, propermotions, or sourcepositions\n",
    "    release (string): v3 or v2\n",
    "    magtype (string): magaper2 or magauto (only applies to summary table)\n",
    "    baseurl: base URL for the request\n",
    "    \n",
    "    Returns a string with the base URL for this request\n",
    "    \"\"\"\n",
    "    checklegal_hcv(table,release,magtype)\n",
    "    if table == \"summary\":\n",
    "        url = \"{baseurl}/{release}/{table}/{magtype}\".format(**locals())\n",
    "    else:\n",
    "        url = \"{baseurl}/{release}/{table}\".format(**locals())\n",
    "    return url\n",
    "\n",
    "\n",
    "def checklegal_hcv(table,release,magtype):\n",
    "    \"\"\"Checks if this combination of table, release and magtype is acceptable\n",
    "    \n",
    "    Raises a ValueError exception if there is problem\n",
    "    \"\"\"\n",
    "    \n",
    "    releaselist = (\"v2\", \"v3\")\n",
    "    if release not in releaselist:\n",
    "        raise ValueError(\"Bad value for release (must be one of {})\".format(\n",
    "            ', '.join(releaselist)))\n",
    "    if release==\"v2\":\n",
    "        tablelist = (\"summary\", \"detailed\")\n",
    "    else:\n",
    "        tablelist = (\"summary\", \"detailed\", \"propermotions\", \"sourcepositions\",\n",
    "                    \"hcvsummary\", \"hcv\")\n",
    "    if table not in tablelist:\n",
    "        raise ValueError(\"Bad value for table (for {} must be one of {})\".format(\n",
    "            release, \", \".join(tablelist)))\n",
    "    if table == \"summary\":\n",
    "        magtypelist = (\"magaper2\", \"magauto\")\n",
    "        if magtype not in magtypelist:\n",
    "            raise ValueError(\"Bad value for magtype (must be one of {})\".format(\n",
    "                \", \".join(magtypelist)))\n",
    "\n",
    "\n",
    "def resolve(name):\n",
    "    \"\"\"Get the RA and Dec for an object using the MAST name resolver\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    name (str): Name of object\n",
    "\n",
    "    Returns RA, Dec tuple with position\n",
    "    \"\"\"\n",
    "\n",
    "    resolverRequest = {'service':'Mast.Name.Lookup',\n",
    "                       'params':{'input':name,\n",
    "                                 'format':'json'\n",
    "                                },\n",
    "                      }\n",
    "    resolvedObjectString = mastQuery(resolverRequest)\n",
    "    resolvedObject = json.loads(resolvedObjectString)\n",
    "    # The resolver returns a variety of information about the resolved object, \n",
    "    # however for our purposes all we need are the RA and Dec\n",
    "    try:\n",
    "        objRa = resolvedObject['resolvedCoordinate'][0]['ra']\n",
    "        objDec = resolvedObject['resolvedCoordinate'][0]['decl']\n",
    "    except IndexError as e:\n",
    "        raise ValueError(\"Unknown object '{}'\".format(name))\n",
    "    return (objRa, objDec)\n",
    "\n",
    "\n",
    "def mastQuery(request, url='https://mast.stsci.edu/api/v0/invoke'):\n",
    "    \"\"\"Perform a MAST query.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    request (dictionary): The MAST request json object\n",
    "    url (string): The service URL\n",
    "\n",
    "    Returns the returned data content\n",
    "    \"\"\"\n",
    "    \n",
    "    # Encoding the request as a json string\n",
    "    requestString = json.dumps(request)\n",
    "    r = requests.post(url, data={'request': requestString})\n",
    "    r.raise_for_status()\n",
    "    return r.text\n",
    "\n",
    "def convertACSmagtoflux(date, filterstring, mag, magerr):\n",
    "    \n",
    "    #date is currently in MJD and needs to be in ISO Format (YYYY-MM-DD)\n",
    "    #use astropy to handle this properly\n",
    "    t = Time(date, format = 'mjd')\n",
    "    \n",
    "    #t.iso has this, but also the hrs, min, sec, so need to truncate those\n",
    "    tiso = t.iso[0:10]\n",
    "    q = acszpt.Query(date=tiso, detector='WFC', filt=filterstring)\n",
    "    zpt_table = q.fetch()\n",
    "    print(zpt_table)\n",
    "    zpt = zpt_table['VEGAmag'].value\n",
    "    \n",
    "    #ACS provides conversion to erg/s/cm^2/angstrom\n",
    "    flux = 10**((mag - zpt)/(-2.5))  #now in erg/s/cm^2/angstrom\n",
    "    #calculate the error\n",
    "    magupper = mag + magerr\n",
    "    maglower = mag - magerr\n",
    "    flux_upper = abs(flux - (10**((magupper - zpt)/(-2.5))))\n",
    "    flux_lower =  abs(flux - (10**((maglower - zpt)/(-2.5))))\n",
    "    \n",
    "    fluxerr = (flux_upper + flux_lower) / 2.0\n",
    "\n",
    "    #now convert from erg/s/cm^2/angstrom to Jy\n",
    "    #first convert angstrom to Hz using c\n",
    "    c = 3E8\n",
    "    flux = flux * c /(1E-10) #now in erg/s/cm^2/Hz\n",
    "    fluxerr = fluxerr * c /(1E-10) #now in erg/s/cm^2/Hz\n",
    "\n",
    "    flux = flux * (1E-23) # now in Jy\n",
    "    \n",
    "    fluxerr = fluxerr * (1E-23) # now in Jy\n",
    "\n",
    "    return flux, fluxerr\n",
    "            \n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do an HCV search\n",
    "radius = 1.0/3600.0 # radius = 1 arcsec\n",
    "\n",
    "\n",
    "\n",
    "for ccount, coord in enumerate(coords_list):\n",
    "\n",
    "    #doesn't take SkyCoord\n",
    "    ra = CLAGN['RA'][ccount]\n",
    "    dec = CLAGN['DEC'][ccount]\n",
    "\n",
    "    #IC 1613 from the demo for testing\n",
    "    #ra = 16.19913\n",
    "    #dec = 2.11778\n",
    "    \n",
    "    #look in the summary table for anything within a radius of our targets\n",
    "    tab = hcvcone(ra,dec,radius,table=\"hcvsummary\")\n",
    "    if tab == '':\n",
    "        print (ccount, 'no matches')\n",
    "    else:\n",
    "        print(ccount, 'got a live one')\n",
    "        \n",
    "        tab = ascii.read(tab)\n",
    "                \n",
    "        matchid = tab['MatchID'][0]  #take the first one, assuming it is the nearest match\n",
    "        \n",
    "        #just pulling one filter for an example (more filters are available)\n",
    "        try:\n",
    "            src_814 = ascii.read(hcvsearch(table='hcv',MatchID=matchid,Filter='ACS_F814W'))\n",
    "        except FileNotFoundError:\n",
    "            #that filter doesn't exist for this target\n",
    "            print(\"no F814W filter info for this target\")\n",
    "        else:        \n",
    "            time_814 = src_814['MJD']\n",
    "            mag_814 = src_814['CorrMag']  #need to convert this to flux\n",
    "            magerr_814 = src_814['MagErr']\n",
    "\n",
    "            filterstring = 'F814W'\n",
    "\n",
    "            #uggg, ACS has time dependent flux zero points.....\n",
    "            #going to cheat for now and only use one time, but could imagine this as a loop\n",
    "            #https://www.stsci.edu/hst/instrumentation/acs/data-analysis/zeropoints\n",
    "            flux, fluxerr = convertACSmagtoflux(time_814[0], filterstring, mag_814, magerr_814)\n",
    "            \n",
    "            #put this single object light curves into a pandas multiindex dataframe\n",
    "            dfsingle_814 = pd.DataFrame(dict(flux=flux, err=fluxerr, time=time_814, objectid=ccount + 1, band='F814W')).set_index([\"objectid\", \"band\", \"time\"])\n",
    "\n",
    "            #then concatenate each individual df together\n",
    "            #first make sure that df has been defined before\n",
    "            try:\n",
    "                df_lc\n",
    "            except NameError:\n",
    "                #df_lc doesn't exist (yet)\n",
    "                df_lc = dfsingle_606\n",
    "            else:\n",
    "                #df_lc exists\n",
    "                df_lc = pd.concat([df_lc,  dfsingle_814])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Find light curves for these targets in relevant, non-NASA catalogs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaia (Faisst)\n",
    "- astroquery.gaia will presumably work out of the box for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ EXTRACT GAIA DATA FOR OBJECTS ##########\n",
    "\n",
    "## Select Gaia table (DR3)\n",
    "Gaia.MAIN_GAIA_TABLE = \"gaiaedr3.gaia_source\"\n",
    "\n",
    "## Define search radius\n",
    "radius = u.Quantity(20, u.arcsec)\n",
    "\n",
    "## Search and Cross match.\n",
    "# This can be done in a smarter way by matching catalogs on the Gaia server, or grouping the\n",
    "# sources and search a larger area.\n",
    "\n",
    "# get catalog\n",
    "gaia_table = Table()\n",
    "t1 = time.time()\n",
    "for cc,coord in enumerate(coords_list):\n",
    "    print(len(coords_list)-cc , end=\" \")\n",
    "\n",
    "    gaia_search = Gaia.cone_search_async(coordinate=coord, radius=radius , background=True)\n",
    "    gaia_search.get_data()[\"dist\"].unit = \"deg\"\n",
    "    gaia_search.get_data()[\"dist\"] = gaia_search.get_data()[\"dist\"].to(u.arcsec) # Change distance unit from degrees to arcseconds\n",
    "    \n",
    "    \n",
    "    # match\n",
    "    if len(gaia_search.get_data()[\"dist\"]) > 0:\n",
    "        gaia_search.get_data()[\"input_object_name\"] = CLAGN[\"Object Name\"][cc] # add input object name to catalog\n",
    "        sel_min = np.where( (gaia_search.get_data()[\"dist\"] < 1*u.arcsec) & (gaia_search.get_data()[\"dist\"] == np.nanmin(gaia_search.get_data()[\"dist\"]) ) )[0]\n",
    "    else:\n",
    "        sel_min = []\n",
    "        \n",
    "    #print(\"Number of sources matched: {}\".format(len(sel_min)) )\n",
    "    \n",
    "    if len(sel_min) > 0:\n",
    "        gaia_table = vstack( [gaia_table , gaia_search.get_data()[sel_min]] )\n",
    "    else:\n",
    "        gaia_table = vstack( [gaia_table , gaia_search.get_data()[sel_min]] )\n",
    "\n",
    "print(\"\\nSearch completed in {:.2f} seconds\".format((time.time()-t1) ) )\n",
    "print(\"Number of objects mached: {} out of {}.\".format(len(gaia_table),len(CLAGN) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## EXTRACT PHOTOMETRY #########\n",
    "# Note that the fluxes are in e/s, not very useful. However, there are magnitudes (what unit??) but without errors.\n",
    "# We can get the errors from the flux errors?\n",
    "\n",
    "## Define keys (columns) that will be used later. Also add wavelength in angstroms for each filter\n",
    "mag_keys = [\"phot_bp_mean_mag\" , \"phot_g_mean_mag\" , \"phot_rp_mean_mag\"]\n",
    "magerr_keys = [\"phot_bp_mean_mag_error\" , \"phot_g_mean_mag_error\" , \"phot_rp_mean_mag_error\"]\n",
    "flux_keys = [\"phot_bp_mean_flux\" , \"phot_g_mean_flux\" , \"phot_rp_mean_flux\"]\n",
    "fluxerr_keys = [\"phot_bp_mean_flux_error\" , \"phot_g_mean_flux_error\" , \"phot_rp_mean_flux_error\"]\n",
    "mag_lambda = [\"5319.90\" , \"6735.42\" , \"7992.90\"]\n",
    "\n",
    "## Get photometry. Note that this includes only objects that are \n",
    "# matched to the catalog. We have to add the missing ones later.\n",
    "_phot = gaia_table[mag_keys]\n",
    "_err = hstack( [ 2.5/np.log(10) * gaia_table[e]/gaia_table[f] for e,f in zip(fluxerr_keys,flux_keys) ] )\n",
    "gaia_phot2 = hstack( [_phot , _err] )\n",
    "\n",
    "## Clean up (change units and column names)\n",
    "_ = [gaia_phot2.rename_column(f,m) for m,f in zip(magerr_keys,fluxerr_keys)]\n",
    "for key in magerr_keys:\n",
    "    gaia_phot2[key].unit = \"mag\"\n",
    "gaia_phot2[\"input_object_name\"] = gaia_table[\"input_object_name\"].copy()\n",
    "\n",
    "## Also add object for which we don't have photometry.\n",
    "# Add Nan for now, need to think about proper format. Also, there are probably smarter ways to do this.\n",
    "# We do this by matching the object names from the original catalog to the photometry catalog. Then add\n",
    "# an entry [np.nan, ...] if it does not exist. To make life easier, we add a dummy entry as the first\n",
    "# row so we can compy all the \n",
    "gaia_phot = Table( names=gaia_phot2.keys() , dtype=gaia_phot2.dtype )\n",
    "for ii in range(len(CLAGN)):\n",
    "    sel = np.where( CLAGN[\"Object Name\"][ii] == gaia_phot2[\"input_object_name\"] )[0]\n",
    "    if len(sel) > 0:\n",
    "        gaia_phot = vstack([gaia_phot , gaia_phot2[sel] ])\n",
    "    else:\n",
    "        tmp = Table( np.repeat(np.NaN , len(gaia_phot2.keys())) , names=gaia_phot2.keys() , dtype=gaia_phot2.dtype )\n",
    "        gaia_phot = vstack([gaia_phot , tmp ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaia_phot.pprint_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASAS-SN (all sky automated survey for supernovae) has a website that can be manually searched (Faisst)\n",
    "- see if astroquery.vizier can find it\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### icecube has a 2008 - 2018 catalog which we can download and is small (Faisst)\n",
    "- https://icecube.wisc.edu/data-releases/2021/01/all-sky-point-source-icecube-data-years-2008-2018/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Make plots of luminosity as a function of time\n",
    "- model plots after https://arxiv.org/pdf/2111.09391.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#pick one to look at for now\n",
    "obj = 1  \n",
    "singleobj = df_lc.loc[(obj),:,:]\n",
    "\n",
    "#set up for plotting\n",
    "fig, ax = plt.subplots(figsize=(9,5))\n",
    "\n",
    "#first check to see which bands we have in the dataframe\n",
    "availband = singleobj.index.unique('band')\n",
    "max_list = []\n",
    "\n",
    "#plot the bands one at a time\n",
    "for l in range(len(availband)):\n",
    "    band_lc = singleobj.loc[(obj, availband[l]), :]\n",
    "    band_lc.reset_index(inplace = True)\n",
    "\n",
    "    #first clean dataframe to remove erroneous rows\n",
    "    band_lc_clean = band_lc[band_lc['time'] < 65000]\n",
    "\n",
    "    #before plotting need to scale the Kepler, K2, and TESS fluxes to the other available fluxes\n",
    "    if availband[l] not in ['Kepler', 'K2', 'TESS']:\n",
    "        #find the maximum value of 'other bands'\n",
    "        max_list.append(max(band_lc_clean.flux))\n",
    "        \n",
    "        ax.errorbar(band_lc_clean.time, band_lc_clean.flux, band_lc_clean.err, capsize = 3.0, label = availband[l])\n",
    "    else:\n",
    "        max_electrons = max(band_lc_clean.flux)\n",
    "        factor = np.mean(max_list)/ max_electrons\n",
    "        ax.errorbar(band_lc_clean.time, band_lc_clean.flux * factor, band_lc_clean.err* factor, capsize = 3.0, label = availband[l])\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.set_xlabel('Time(MJD)')\n",
    "ax.set_ylabel('Flux(Jy)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image extension: look for archival images of these targets\n",
    "- NASA NAVO use cases should help us to learn how to do this\n",
    "- can use the cutout service now in astropy from the first fornax use case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Extension \n",
    "Consider training a ML model to do light curve classification based on this sample of CLAGN\n",
    " - once we figure out which bands these are likely to be observed in, could then have a optical + IR light curve classifier\n",
    " - what would the features of the light curve be?\n",
    " - what models are reasonable to test as light curve classifiers?\n",
    " - could we make also a sample of TDEs, SNe, flaring AGN? - then train the model to distinguish between these things?\n",
    " - need a sample of non-flaring light curves\n",
    " \n",
    "After training the model:\n",
    " - would then need a sample of optical + IR light curves for \"all\" galaxies = big data to run the model on.\n",
    "\n",
    "Some resources to consider:\n",
    "- https://github.com/dirac-institute/ZTF_Boyajian\n",
    "- https://ui.adsabs.harvard.edu/abs/2022AJ....164...68S/abstract\n",
    "- https://ui.adsabs.harvard.edu/abs/2019ApJ...881L...9F/abstract\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

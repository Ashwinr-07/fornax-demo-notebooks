{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make multiwavelength light curves using archival data\n",
    "***\n",
    "\n",
    "## Learning Goals    \n",
    "By the end of this tutorial, you will be able to:\n",
    " - automatically load a catalog of sources\n",
    " - automatically search NASA and non-NASA resources for light curves\n",
    " - store light curves in a Pandas multiindex dataframe\n",
    " - plot all light curves on the same plot\n",
    " \n",
    " \n",
    "## Introduction:\n",
    " - A user has a sample of interesting targets for which they would like to see a plot of available archival light curves.  We start with a small set of changing look AGN from Yang et al., 2018, which are automatically downloaded. Changing look AGN are cases where the broad emission lines appear or disappear (and not just that the flux is variable). \n",
    " - We model light curve plots after van Velzen et al. 2021.  We search through a curated list of time-domain NASA holdings as well as non-NASA sources.  HEASARC catalogs used are Fermi and Beppo-Sax, IRSA catalogs used are ZTF and WISE, and MAST catalogs used are Pan-Starrs, TESS, Kepler, and K2.  Non-NASA sources are Gaia and IceCube. This list is generalized enough to include many types of targets to make this notebook interesting for many types of science.  All of these time-domain archives are searched in an automated fashion using astroquery or APIs.\n",
    " - Light curve data storage is a tricky problem.  Currently we are using a multi-index Pandas dataframe, as the best existing choice for right now.  One downside is that we need to manually track the units of flux and time instead of relying on an astropy storage scheme which would be able to do some of the units worrying for us (even astropy can't do all magnitude to flux conversions).  Astropy does not currently have a good option for multi-band light curve storage.\n",
    " - We intend to explore a ML classifier for these changing look AGN light curves.\n",
    " \n",
    "## Input:\n",
    " - choose from a list of known changing look AGN from the literature\n",
    " \n",
    "  OR - \n",
    " - input your own sample\n",
    "\n",
    "## Output:\n",
    " - an archival optical + IR + neutrino light curve\n",
    " \n",
    "## Technical Goals:\n",
    " - should be able to run from a clean checkout from github\n",
    " - should be able to automatically download all catalogs & images used\n",
    " - need to have all photometry in the same physical unit\n",
    " - need to have a data structure that is easy to use but holds light curve information (time and units) and is extendable to ML applications\n",
    " - need to have a curated list of catalogs to search for photometry that is generalizeable to other input catalogs\n",
    " \n",
    "## Non-standard Imports:\n",
    "- `astroquery` to interface with archives APIs\n",
    "- `astropy` to work with coordinates/units and data structures\n",
    "- `AXS` (astronomy extensions for Spark) to handle large catalog cross matching\n",
    "- `lightkurve` to search TESSS, Kepler, and K2 archives\n",
    "- `urllib` to handle archive searches with website interface\n",
    "- `acstools` to work with HST magnitude to flux conversion\n",
    "- `unTimely` to retrieve WISE light curves\n",
    "- `alerce` to convert ZTF object names into coordinates\n",
    "\n",
    "## Authors:\n",
    "IPAC SP team\n",
    "\n",
    "## Acknowledgements:\n",
    "Suvi Gezari, Antara Basu-zych,Stephanie LaMassa\\\n",
    "MAST, HEASARC, & IRSA Fornax teams\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/lightkurve/config/__init__.py:119: UserWarning: The default Lightkurve cache directory, used by download(), etc., has been moved to /home/jkrick/.lightkurve/cache. Please move all the files in the legacy directory /home/jkrick/.lightkurve-cache to the new location and remove the legacy directory. Refer to https://docs.lightkurve.org/reference/config.html#default-cache-directory-migration for more information.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: alerce in /opt/conda/lib/python3.8/site-packages (1.2.0)\n",
      "Requirement already satisfied: pandas>=1.1.2 in /opt/conda/lib/python3.8/site-packages (from alerce) (1.3.5)\n",
      "Requirement already satisfied: astropy>=4.0.1 in /opt/conda/lib/python3.8/site-packages (from alerce) (5.0.1)\n",
      "Requirement already satisfied: requests>=2.24.0 in /opt/conda/lib/python3.8/site-packages (from alerce) (2.24.0)\n",
      "Requirement already satisfied: numpy>=1.18 in /opt/conda/lib/python3.8/site-packages (from astropy>=4.0.1->alerce) (1.21.5)\n",
      "Requirement already satisfied: packaging>=19.0 in /opt/conda/lib/python3.8/site-packages (from astropy>=4.0.1->alerce) (20.4)\n",
      "Requirement already satisfied: PyYAML>=3.13 in /opt/conda/lib/python3.8/site-packages (from astropy>=4.0.1->alerce) (6.0)\n",
      "Requirement already satisfied: pyerfa>=2.0 in /opt/conda/lib/python3.8/site-packages (from astropy>=4.0.1->alerce) (2.0.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas>=1.1.2->alerce) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas>=1.1.2->alerce) (2021.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.24.0->alerce) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.24.0->alerce) (2022.12.7)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from requests>=2.24.0->alerce) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.24.0->alerce) (1.25.11)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=19.0->astropy>=4.0.1->alerce) (2.4.7)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from packaging>=19.0->astropy>=4.0.1->alerce) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.3; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import axs\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import json\n",
    "import requests\n",
    "import pickle\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "from astroquery.ipac.ned import Ned\n",
    "from astroquery.heasarc import Heasarc\n",
    "from astroquery.gaia import Gaia\n",
    "\n",
    "from astropy.coordinates import SkyCoord, name_resolve\n",
    "import astropy.units as u\n",
    "from astropy.table import Table, vstack, hstack, unique\n",
    "from astropy.io import fits,ascii\n",
    "from astropy.time import Time\n",
    "from astropy.timeseries import TimeSeries\n",
    "\n",
    "\n",
    "try: # Python 3.x\n",
    "    from urllib.parse import quote as urlencode\n",
    "    from urllib.request import urlretrieve\n",
    "except ImportError:  # Python 2.x\n",
    "    from urllib import pathname2url as urlencode\n",
    "    from urllib import urlretrieve\n",
    "\n",
    "try:\n",
    "    import lightkurve as lk\n",
    "except ImportError:\n",
    "    !pip install lightkurve --upgrade\n",
    "    import lightkurve as lk\n",
    "        \n",
    "try:\n",
    "    from acstools import acszpt\n",
    "except ImportError:\n",
    "    !pip install acstools\n",
    "    from acstools import acszpt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "try:\n",
    "    from unTimely_Catalog_tools import unTimelyCatalogExplorer\n",
    "except ImportError:\n",
    "    if not os.path.exists('./unTimely_Catalog_explorer'):\n",
    "        !git clone https://github.com/fkiwy/unTimely_Catalog_explorer.git\n",
    "    sys.path.append('./unTimely_Catalog_explorer')\n",
    "    from unTimely_Catalog_tools import unTimelyCatalogExplorer\n",
    "    \n",
    "    \n",
    "!pip install alerce\n",
    "\n",
    "\n",
    "import tempfile\n",
    "\n",
    "# Local code imports\n",
    "sys.path.append('code/')\n",
    "from fluxconversions import convert_WISEtoJanskies, convertACSmagtoflux\n",
    "from panstarrs import ps1cone, ps1search, checklegal, ps1metadata, addfilter, improve_filter_format, search_lightcurve\n",
    "from clean_filternames import clean_filternames\n",
    "from gaia_functions import Gaia_retrieve_EPOCH_PHOTOMETRY, Gaia_mk_lightcurves, Gaia_mk_MultiIndex\n",
    "from HCV_functions import get_hscapiurl, hcvcone, hcvsearch, hcvmetadata, cat2url, checklegal_hcv\n",
    "from mast_functions import resolve, mastQuery\n",
    "from icecube_functions import get_icecube_catalog\n",
    "from sample_selection import get_lamassa_sample, get_macleod16_sample, get_ruan_sample, get_macleod19_sample, get_sheng_sample, \\\n",
    "    get_green_sample, get_lyu_sample, get_lopeznavas_sample, get_hon_sample, get_yang_sample,  get_SDSS_sample, remove_duplicate_coords\n",
    "from ztf_functions import ZTF_id2coord,ZTF_get_lightcurve\n",
    "from data_structures import MultiIndexDFObject\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "## Plotting stuff\n",
    "mpl.rcParams['font.size'] = 14\n",
    "mpl.rcParams['axes.labelpad'] = 7\n",
    "mpl.rcParams['xtick.major.pad'] = 7\n",
    "mpl.rcParams['ytick.major.pad'] = 7\n",
    "mpl.rcParams['xtick.minor.visible'] = True\n",
    "mpl.rcParams['ytick.minor.visible'] = True\n",
    "mpl.rcParams['xtick.minor.top'] = True\n",
    "mpl.rcParams['xtick.minor.bottom'] = True\n",
    "mpl.rcParams['ytick.minor.left'] = True\n",
    "mpl.rcParams['ytick.minor.right'] = True\n",
    "mpl.rcParams['xtick.major.size'] = 5\n",
    "mpl.rcParams['ytick.major.size'] = 5\n",
    "mpl.rcParams['xtick.minor.size'] = 3\n",
    "mpl.rcParams['ytick.minor.size'] = 3\n",
    "mpl.rcParams['xtick.direction'] = 'in'\n",
    "mpl.rcParams['ytick.direction'] = 'in'\n",
    "#mpl.rc('text', usetex=True)\n",
    "mpl.rc('font', family='serif')\n",
    "mpl.rcParams['xtick.top'] = True\n",
    "mpl.rcParams['ytick.right'] = True\n",
    "mpl.rcParams['hatch.linewidth'] = 1\n",
    "\n",
    "\n",
    "\n",
    "## NEW! LOAD/SAVE PICKLE FILE! ##\n",
    "SAVEDF = True # if set to True, pickle file will be saved. If False, pickle file will be loaded.\n",
    "pickle_file_name = \"data/dflc_Shooby_March102023.pkl\" # name of pickle file (to be loaded or saved)\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Initialize Pandas MultiIndex data frame for storing the light curve\n",
    "df_lc = MultiIndexDFObject()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define the Sample\n",
    " We define here a \"gold\" sample of spectroscopically confirmed changing look AGN and quasars. This sample includes both objects which change from type 1 to type 2 and also the opposite.  Future studies may want to treat these as seperate objects or seperate QSOs from AGN.\n",
    " \n",
    " Bibcodes for the samples used are listed next to their functions for reference.  \n",
    " \n",
    " Functions used to grab the samples from the papers use Astroquery, NED, SIMBAD, Vizier, and in a few cases grab the tables from the html versions of the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changing Look AGN- Yang et al:  31\n",
      "after duplicates removal, sample size: 30\n",
      "sample size: 30\n"
     ]
    }
   ],
   "source": [
    "#build up the sample\n",
    "coords =[]\n",
    "labels = []\n",
    "\n",
    "#choose your own adventure:\n",
    "\n",
    "#get_lamassa_sample(coords, labels)  #2015ApJ...800..144L\n",
    "#get_macleod16_sample(coords, labels) #2016MNRAS.457..389M\n",
    "#get_ruan_sample(coords, labels) #2016ApJ...826..188R\n",
    "#get_macleod19_sample(coords, labels)  #2019ApJ...874....8M\n",
    "#get_sheng_sample(coords, labels)  #2020ApJ...889...46S\n",
    "#get_green_sample(coords, labels)  #2022ApJ...933..180G\n",
    "#get_lyu_sample(coords, labels)  #z32022ApJ...927..227L\n",
    "#get_lopeznavas_sample(coords, labels)  #2022MNRAS.513L..57L\n",
    "#get_hon_sample(coords, labels)  #2022MNRAS.511...54H\n",
    "get_yang_sample(coords, labels)   #2018ApJ...862..109Y\n",
    "\n",
    "#now get some \"normal\" QSOs for use in the classifier\n",
    "#there are ~500K of these, so choose the number based on\n",
    "#a balance between speed of running the light curves and whatever \n",
    "#the ML algorithms would like to have\n",
    "\n",
    "num_normal_QSO = 100 \n",
    "#get_SDSS_sample(coords, labels, num_normal_QSO)\n",
    "\n",
    "#remove duplicates from the list if combining multiple references\n",
    "coords_list, labels_list = remove_duplicate_coords(coords, labels)\n",
    "\n",
    "#add an index-like list with our own object name\n",
    "object_name = [['CLAGN' + str(i)] for i in range(len(coords_list))]\n",
    "print('sample size: '+str(len(coords_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Build your own Sample\n",
    "\n",
    "To build your own sample, you can follow the examples of functions above to grab coordinates from your favorite literature resource, \n",
    "\n",
    "or\n",
    "\n",
    "You can use [astropy's read](https://docs.astropy.org/en/stable/io/ascii/read.html) function to read in an input table\n",
    "and then convert that table into a list of [skycoords](https://docs.astropy.org/en/stable/api/astropy.coordinates.SkyCoord.html) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Find light curves for these targets in NASA catalogs\n",
    "  - this list is appropriate for a generalization of this use case to other input catalogs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 HEASARC: FERMI & Beppo SAX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mission_list = ['FERMIGTRIG', 'SAXGRBMGRB']\n",
    "radius = 0.1*u.degree\n",
    "#for testing\n",
    "#for ccount in range(1):\n",
    "    #To get a fermigtrig source\n",
    "    #coord = SkyCoord('03h41m21.2s -89d00m33.0s', frame='icrs')\n",
    "\n",
    "    #to get a bepposax source\n",
    "    #coord = SkyCoord('14h32m00.0s -88d00m00.0s', frame='icrs')\n",
    "\n",
    "for ccount, coord in enumerate(tqdm(coords_list)):\n",
    "    #use astroquery to search that position for either a Fermi or Beppo Sax trigger\n",
    "    for mcount, mission in enumerate(mission_list):\n",
    "        try:\n",
    "            results = Heasarc.query_region(coord, mission = mission, radius = radius)#, sortvar = 'SEARCH_OFFSET_')\n",
    "            #really just need to save the one time of the Gamma ray detection\n",
    "            #time is already in MJD for both catalogs\n",
    "            if mission == 'FERMIGTRIG':\n",
    "                time_mjd = float(results['TRIGGER_TIME'])\n",
    "            else:\n",
    "                time_mjd = float(results['TIME'][0])\n",
    "                \n",
    "            type(time_mjd)\n",
    "            lab = labels_list[ccount]\n",
    "\n",
    "            #really just need to mark this spot with a vertical line in the plot\n",
    "            dfsingle = pd.DataFrame(dict(flux=[0.1], err=[0.1], time=[time_mjd], objectid=[ccount + 1], band=[mission], label=lab)).set_index([\"objectid\", \"label\", \"band\", \"time\"])\n",
    "\n",
    "            # Append to existing MultiIndex light curve object\n",
    "            df_lc.append(dfsingle)\n",
    "\n",
    "        except AttributeError:\n",
    "            print(\"no results at that location for \", mission)\n",
    "\n",
    "\n",
    "#**** These HEASARC searches are returning an attribute error because of an astroquery bug\n",
    "# bug submitted to astroquery Oct 18, waiting for a fix.\n",
    "# if that gets fixed, can probably change this cell "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 IRSA: ZTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lc = ztf_lightcurve(df_lc,coords_list,labels_list,plotprint=1) ## number of plots to show to be set by plotprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 IRSA:WISE\n",
    "\n",
    "- use the unTimely catalog which ties together all WISE & NEOWISE 2010 - 2020 epochs.  Specifically it combined all observations at a single epoch to achieve deeper mag limits than individual observations alone.\n",
    "- https://github.com/fkiwy/unTimely_Catalog_explorer\n",
    "- https://iopscience-iop-org.caltech.idm.oclc.org/article/10.3847/1538-3881/aca2ab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup the explorer\n",
    "ucx = unTimelyCatalogExplorer(directory=os.getcwd(), cache=True, show_progress=True, timeout=300,\n",
    "                              catalog_base_url='http://unwise.me/data/neo7/untimely-catalog/',\n",
    "                              catalog_index_file='untimely_index-neo7.fits')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "bandlist = ['w1', 'w2']\n",
    "\n",
    "#for ccount in range(1):#enumerate(coords_list):\n",
    "for ccount, coord in enumerate(tqdm(coords_list)):\n",
    "    #doesn't take SkyCoord, convert to floats\n",
    "    ra = coords_list.ra.deg[ccount]\n",
    "    dec = coords_list.dec.deg[ccount]\n",
    "    lab = labels_list[ccount]\n",
    "\n",
    "    #search the untimely catalog\n",
    "    result_table = ucx.search_by_coordinates(ra, dec, box_size=100, cone_radius=1.0, show_result_table_in_browser=False,\n",
    "                                         save_result_table=False)#, suppress_console_output=True)\n",
    "\n",
    "    if (len(result_table) > 0):\n",
    "        #got a live one\n",
    "        for bcount, band in enumerate(bandlist):\n",
    "            #sort by band\n",
    "            mask = result_table['band'] == (bcount + 1)\n",
    "            result_table_band = result_table[mask]\n",
    "    \n",
    "            mag = result_table_band['mag']\n",
    "            magerr = result_table_band['dmag']\n",
    "    \n",
    "            wiseflux, wisefluxerr = convert_WISEtoJanskies(mag,magerr ,band)\n",
    "\n",
    "            time_mjd = result_table_band['mjdmean']\n",
    "            \n",
    "            #plt.figure(figsize=(8, 4))\n",
    "            #plt.errorbar(time_mjd, wiseflux, wisefluxerr)\n",
    "            dfsingle = pd.DataFrame(dict(flux=wiseflux, err=wisefluxerr, time=time_mjd, objectid=ccount + 1, band=band,label=lab)).set_index([\"objectid\",\"label\", \"band\", \"time\"])\n",
    "\n",
    "            #then concatenate each individual df together\n",
    "            df_lc.append(dfsingle)\n",
    "            \n",
    "    else:\n",
    "        print(\"There is no WISE light curve for this object\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 MAST: Pan-STARRS\n",
    "Query the Pan-STARRS API; based on this [example](https://ps1images.stsci.edu/ps1_dr2_api.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:40<00:00,  1.35s/it]\n"
     ]
    }
   ],
   "source": [
    "#Do a panstarrs search\n",
    "radius = 1.0/3600.0 # radius = 1 arcsec\n",
    "#plt.rcParams.update({'font.size': 14})\n",
    "#plt.figure(1,(10,10))\n",
    "\n",
    "        \n",
    "#for all objects in our catalog\n",
    "for ccount, coord in enumerate(tqdm(coords_list)):\n",
    "    #doesn't take SkyCoord, convert to floats\n",
    "    ra = coords_list.ra.deg[ccount]\n",
    "    dec = coords_list.dec.deg[ccount]\n",
    "    lab = labels_list[ccount]\n",
    "\n",
    "    #see if there is an object in panSTARRS at this location\n",
    "    results = ps1cone(ra,dec,radius,release='dr2')\n",
    "    tab = ascii.read(results)\n",
    "    \n",
    "    # improve the format of the table\n",
    "    tab = improve_filter_format(tab)\n",
    "        \n",
    "    #in case there is more than one object within 1 arcsec, sort them by match distance\n",
    "    tab.sort('distance')\n",
    "    \n",
    "    #if there is an object at that location\n",
    "    if len(tab) > 0:   \n",
    "        #got a live one\n",
    "        #print( 'for object', ccount + 1, 'there is ',len(tab), 'match in panSTARRS', tab['objID'])\n",
    "\n",
    "        #take the closest match as the best match\n",
    "        objid = tab['objID'][0]\n",
    "        \n",
    "        #get the actual detections and light curve info for this target\n",
    "        dresults = search_lightcurve(objid)\n",
    "        \n",
    "        #sometimes there isn't actually a light curve for the target???\n",
    "        try:\n",
    "            ascii.read(dresults)\n",
    "        except FileNotFoundError:\n",
    "            print(\"There is no light curve\")\n",
    "            #no need to store PanSTARRS data for this one\n",
    "        else:\n",
    "            #There is a light curve for this target\n",
    "            \n",
    "            #fix the column names to include filter names\n",
    "            dtab = addfilter(ascii.read(dresults))\n",
    "            dtab.sort('obsTime')\n",
    "\n",
    "            #here is the light curve mixed from all 5 bands\n",
    "            t_panstarrs = dtab['obsTime']\n",
    "            flux_panstarrs = dtab['psfFlux']*1E3  # in mJy\n",
    "            err_panstarrs = dtab['psfFluxErr'] *1E3\n",
    "            filtername = dtab['filter']\n",
    "            \n",
    "            #put this single object light curves into a pandas multiindex dataframe\n",
    "            dfsingle = pd.DataFrame(dict(flux=flux_panstarrs, err=err_panstarrs, time=t_panstarrs, objectid=ccount + 1, band=filtername, label=lab)).set_index([\"objectid\",\"label\", \"band\", \"time\"])\n",
    "\n",
    "            #then concatenate each individual df together\n",
    "            df_lc.append(dfsingle)\n",
    "                \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>flux</th>\n",
       "      <th>err</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>objectid</th>\n",
       "      <th>label</th>\n",
       "      <th>band</th>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">Yang 18</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">panstarrs r</th>\n",
       "      <th>55088.484765</th>\n",
       "      <td>0.062549</td>\n",
       "      <td>0.001499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55088.495212</th>\n",
       "      <td>0.060602</td>\n",
       "      <td>0.001487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55094.521832</th>\n",
       "      <td>0.089942</td>\n",
       "      <td>0.001906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55104.427380</th>\n",
       "      <td>0.062854</td>\n",
       "      <td>0.001912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55104.437309</th>\n",
       "      <td>0.066443</td>\n",
       "      <td>0.002166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">30</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">Yang 18</th>\n",
       "      <th>panstarrs y</th>\n",
       "      <th>56868.625712</th>\n",
       "      <td>0.094400</td>\n",
       "      <td>0.009570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">panstarrs i</th>\n",
       "      <th>56912.473952</th>\n",
       "      <td>0.056277</td>\n",
       "      <td>0.001978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56912.485421</th>\n",
       "      <td>0.056711</td>\n",
       "      <td>0.001889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56912.496921</th>\n",
       "      <td>0.060259</td>\n",
       "      <td>0.002196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56912.508399</th>\n",
       "      <td>0.060392</td>\n",
       "      <td>0.002198</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2086 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               flux       err\n",
       "objectid label   band        time                            \n",
       "1        Yang 18 panstarrs r 55088.484765  0.062549  0.001499\n",
       "                             55088.495212  0.060602  0.001487\n",
       "                             55094.521832  0.089942  0.001906\n",
       "                             55104.427380  0.062854  0.001912\n",
       "                             55104.437309  0.066443  0.002166\n",
       "...                                             ...       ...\n",
       "30       Yang 18 panstarrs y 56868.625712  0.094400  0.009570\n",
       "                 panstarrs i 56912.473952  0.056277  0.001978\n",
       "                             56912.485421  0.056711  0.001889\n",
       "                             56912.496921  0.060259  0.002196\n",
       "                             56912.508399  0.060392  0.002198\n",
       "\n",
       "[2086 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get a look at what that multiindex data frame looks like so far\n",
    "df_lc.data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 MAST: Asteroid Terrestrial-impact Last Alert System (ATLAS)\n",
    " - All-sky stellar reference catalog \n",
    " -  MAST hosts this catalog but there are three barriers to using it\n",
    "     1. it is unclear if the MAST [holdings]( https://archive.stsci.edu/hlsp/atlas-refcat2#section-a737bc3e-2d56-4827-9ab4-838fbf8d67c1) include the individual epoch photometry and \n",
    "     2. it is only accessible with casjobs, not through python notebooks.  \n",
    "     3. magnitude range (g, r, i) < 19mag makes it not relevant for this use case\n",
    " \n",
    "One path forward if this catalog becomes scientifically interesting is to put in a MAST helpdesk ticket to see if 1) they do have the light curves, and 2) they could switch the catalog to a searchable with python version.  There are some ways of [accessing casjobs with python](<https://github.com/spacetelescope/notebooks/blob/master/notebooks/MAST/HSC/HCV_CASJOBS/HCV_casjobs_demo.ipynb), but not this particular catalog.  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 MAST: TESS, Kepler and K2\n",
    " - use [`lightKurve`](https://docs.lightkurve.org/index.html) to search all 3 missions and download light curves\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radius = 1.0  #arcseconds\n",
    "\n",
    "#for all objects\n",
    "for ccount, coord in enumerate(tqdm(coords_list)):\n",
    "#for testing, this has 79 light curves between the three missions.\n",
    "#for ccount in range(1):\n",
    "#    coord = '19:02:43.1 +50:14:28.7'\n",
    "    \n",
    "    #use lightkurve to search TESS, Kepler and K2\n",
    "    search_result = lk.search_lightcurve(coord, radius = radius)\n",
    "    lab = labels_list[ccount]\n",
    "\n",
    "    #figure out what to do with the results\n",
    "    if len(search_result) < 1:\n",
    "        #there is no data in these missions at this location\n",
    "        print(ccount, ': no match')\n",
    "    else:\n",
    "        #https://docs.lightkurve.org/tutorials/1-getting-started/searching-for-data-products.html\n",
    "        print(ccount, 'got a live one')\n",
    "        #download all of the returned light curves from TESS, Kepler, and K2\n",
    "        lc_collection = search_result.download_all()\n",
    "\n",
    "        #can't get the whole collection directly into pandas multiindex\n",
    "        #pull out inidividual light curves, convert to uniform units, and put them in pandas\n",
    "        for numlc in range(len(search_result)):\n",
    "            \n",
    "            lc = lc_collection[numlc]  #for testing 0 is Kepler, #69 is TESS\n",
    "\n",
    "            #convert to Pandas\n",
    "            lcdf = lc.to_pandas().reset_index()\n",
    "        \n",
    "            #these light curves are too highly sampled for our AGN use case, so reduce their size\n",
    "            #by choosing only to keep every nth sample\n",
    "            nsample = 30\n",
    "            lcdf_small = lcdf[lcdf.index % nsample ==0]  #selects every nth row starting with row 0\n",
    "            \n",
    "            #convert time to mjd\n",
    "            time_lc = lcdf_small.time #in units of time - 2457000 BTJD days\n",
    "            time_lc= time_lc + 2457000 - 2400000.5 #now in MJD days within a few minutes (except for the barycenter correction)\n",
    "\n",
    "            #TESS, Kepler, and K2 report flux in units of electrons/s \n",
    "             #- there is no good way to convert this to anything more useful because the bandpasses are very wide and nonstandard\n",
    "             #- really we don't care about absolute scale, but want to scale the light curve to be on the same plot as other light curves\n",
    "             #- save as electron/s here and scale when plotting\n",
    "            flux_lc = lcdf_small.flux #in electron/s\n",
    "            fluxerr_lc = lcdf_small.flux_err #in electron/s\n",
    "\n",
    "            #record band name\n",
    "            filtername = clean_filternames(search_result, numlc)\n",
    "\n",
    "            #put this single object light curves into a pandas multiindex dataframe\n",
    "            # fluxes are in units of electrons/s and will be scaled to fit the other fluxes when plotting\n",
    "            dfsingle = pd.DataFrame(dict(flux=flux_lc, err=fluxerr_lc, time=time_lc, objectid=ccount + 1, band=filtername,label=lab)).set_index([\"objectid\", \"label\", \"band\", \"time\"])\n",
    "\n",
    "            #then concatenate each individual df together\n",
    "            df_lc.append(dfsingle)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 MAST: HCV\n",
    " - [hubble catalog of variables](https://archive.stsci.edu/hlsp/hcv) \n",
    " - using [this notebook](https://archive.stsci.edu/hst/hsc/help/HCV/HCV_API_demo.html) as a reference to search and download light curves via API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do an HCV search\n",
    "radius = 1.0/3600.0 # radius = 1 arcsec\n",
    "\n",
    "for ccount, coord in enumerate(coords_list):\n",
    "\n",
    "    #doesn't take SkyCoord, convert to floats\n",
    "    ra = coords_list.ra.deg[ccount]\n",
    "    dec = coords_list.dec.deg[ccount]\n",
    "    lab = labels_list[ccount]\n",
    "\n",
    "    #IC 1613 from the demo for testing\n",
    "    #ra = 16.19913\n",
    "    #dec = 2.11778\n",
    "    \n",
    "    #look in the summary table for anything within a radius of our targets\n",
    "    tab = hcvcone(ra,dec,radius,table=\"hcvsummary\")\n",
    "    if tab == '':\n",
    "        print (ccount, 'no matches')\n",
    "    else:\n",
    "        print(ccount, 'got a live one')\n",
    "        \n",
    "        tab = ascii.read(tab)\n",
    "                \n",
    "        matchid = tab['MatchID'][0]  #take the first one, assuming it is the nearest match\n",
    "        \n",
    "        #just pulling one filter for an example (more filters are available)\n",
    "        try:\n",
    "            src_814 = ascii.read(hcvsearch(table='hcv',MatchID=matchid,Filter='ACS_F814W'))\n",
    "        except FileNotFoundError:\n",
    "            #that filter doesn't exist for this target\n",
    "            print(\"no F814W filter info for this target\")\n",
    "        else:        \n",
    "            time_814 = src_814['MJD']\n",
    "            mag_814 = src_814['CorrMag']  #need to convert this to flux\n",
    "            magerr_814 = src_814['MagErr']\n",
    "\n",
    "            filterstring = 'F814W'\n",
    "\n",
    "            #uggg, ACS has time dependent flux zero points.....\n",
    "            #going to cheat for now and only use one time, but could imagine this as a loop\n",
    "            #https://www.stsci.edu/hst/instrumentation/acs/data-analysis/zeropoints\n",
    "            flux, fluxerr = convertACSmagtoflux(time_814[0], filterstring, mag_814, magerr_814)\n",
    "            flux = flux *1E3 #convert to mJy\n",
    "            fluxerr = fluxerr*1E3 #convert to mJy\n",
    "            \n",
    "            #put this single object light curves into a pandas multiindex dataframe\n",
    "            dfsingle_814 = pd.DataFrame(dict(flux=flux, err=fluxerr, time=time_814, objectid=ccount + 1, band='F814W',label=lab)).set_index([\"objectid\", \"label\", \"band\", \"time\"])\n",
    "\n",
    "            #then concatenate each individual df together\n",
    "            df_lc.append(dfsingle_814)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Find light curves for these targets in relevant, non-NASA catalogs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Gaia \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ EXTRACT GAIA DATA FOR OBJECTS ##########\n",
    "## Note: This is very slow. Can probably make faster with direct SQL search?\n",
    "\n",
    "## Select Gaia table (DR3)\n",
    "Gaia.MAIN_GAIA_TABLE = \"gaiaedr3.gaia_source\"\n",
    "\n",
    "## Define search radius\n",
    "radius = u.Quantity(20, u.arcsec)\n",
    "\n",
    "## Search and Cross match.\n",
    "# This can be done in a smarter way by matching catalogs on the Gaia server, or grouping the\n",
    "# sources and search a larger area.\n",
    "\n",
    "# get catalog\n",
    "gaia_table = Table()\n",
    "t1 = time.time()\n",
    "for cc,coord in enumerate(coords_list):\n",
    "    print(len(coords_list)-cc , end=\" \")\n",
    "\n",
    "    gaia_search = Gaia.cone_search_async(coordinate=coord, radius=radius , background=True)\n",
    "    gaia_search.get_data()[\"dist\"].unit = \"deg\"\n",
    "    gaia_search.get_data()[\"dist\"] = gaia_search.get_data()[\"dist\"].to(u.arcsec) # Change distance unit from degrees to arcseconds\n",
    "\n",
    "    \n",
    "    # match\n",
    "    if len(gaia_search.get_data()[\"dist\"]) > 0:\n",
    "        #gaia_search.get_data()[\"input_object_name\"] = CLAGN[\"Object Name\"][cc] # add input object name to catalog\n",
    "        gaia_search.get_data()[\"input_object_name\"] = object_name[cc] # add input object name to catalog\n",
    "        gaia_search.get_data()[\"input_object_id\"] = object_name[cc] # add input object name to catalog\n",
    "        sel_min = np.where( (gaia_search.get_data()[\"dist\"] < 1*u.arcsec) & (gaia_search.get_data()[\"dist\"] == np.nanmin(gaia_search.get_data()[\"dist\"]) ) )[0]\n",
    "    else:\n",
    "        sel_min = []\n",
    "        \n",
    "    #print(\"Number of sources matched: {}\".format(len(sel_min)) )\n",
    "    \n",
    "    if len(sel_min) > 0:\n",
    "        gaia_table = vstack( [gaia_table , gaia_search.get_data()[sel_min]] )\n",
    "    else:\n",
    "        gaia_table = vstack( [gaia_table , gaia_search.get_data()[sel_min]] )\n",
    "\n",
    "print(\"\\nSearch completed in {:.2f} seconds\".format((time.time()-t1) ) )\n",
    "print(\"Number of objects mached: {} out of {}.\".format(len(gaia_table),len(object_name) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "########## EXTRACT PHOTOMETRY #########\n",
    "# Once we matched the objects, we have to extract the photometry for them. Here we extract\n",
    "# the mean photometry (later we will do the time series).\n",
    "# Note that the fluxes are in e/s, not very useful. However, there are magnitudes (what unit??) but without errors.\n",
    "# We can get the errors from the flux errors?\n",
    "# Also note that we should include the source_id in order to search for epoch photometry\n",
    "\n",
    "## Define keys (columns) that will be used later. Also add wavelength in angstroms for each filter\n",
    "other_keys = [\"source_id\",\"phot_g_n_obs\",\"phot_rp_n_obs\",\"phot_bp_n_obs\"] # some other useful info\n",
    "mag_keys = [\"phot_bp_mean_mag\" , \"phot_g_mean_mag\" , \"phot_rp_mean_mag\"]\n",
    "magerr_keys = [\"phot_bp_mean_mag_error\" , \"phot_g_mean_mag_error\" , \"phot_rp_mean_mag_error\"]\n",
    "flux_keys = [\"phot_bp_mean_flux\" , \"phot_g_mean_flux\" , \"phot_rp_mean_flux\"]\n",
    "fluxerr_keys = [\"phot_bp_mean_flux_error\" , \"phot_g_mean_flux_error\" , \"phot_rp_mean_flux_error\"]\n",
    "mag_lambda = [\"5319.90\" , \"6735.42\" , \"7992.90\"]\n",
    "\n",
    "## Get photometry. Note that this includes only objects that are \n",
    "# matched to the catalog. We have to add the missing ones later.\n",
    "_phot = gaia_table[mag_keys]\n",
    "_err = hstack( [ 2.5/np.log(10) * gaia_table[e]/gaia_table[f] for e,f in zip(fluxerr_keys,flux_keys) ] )\n",
    "gaia_phot2 = hstack( [_phot , _err] )\n",
    "\n",
    "## Clean up (change units and column names)\n",
    "_ = [gaia_phot2.rename_column(f,m) for m,f in zip(magerr_keys,fluxerr_keys)]\n",
    "for key in magerr_keys:\n",
    "    gaia_phot2[key].unit = \"mag\"\n",
    "gaia_phot2[\"input_object_name\"] = gaia_table[\"input_object_name\"].copy()\n",
    "\n",
    "## Add Some other useful information\n",
    "for key in other_keys:\n",
    "    gaia_phot2[key] = gaia_table[key]\n",
    "\n",
    "\n",
    "## Also add object for which we don't have photometry.\n",
    "# Add Nan for now, need to think about proper format. Also, there are probably smarter ways to do this.\n",
    "# We do this by matching the object names from the original catalog to the photometry catalog. Then add\n",
    "# an entry [np.nan, ...] if it does not exist. To make life easier, we add a dummy entry as the first\n",
    "# row so we can compy all the \n",
    "gaia_phot = Table( names=gaia_phot2.keys() , dtype=gaia_phot2.dtype )\n",
    "for ii in range(len(object_name)):\n",
    "    #sel = np.where( CLAGN[\"Object Name\"][ii] == gaia_phot2[\"input_object_name\"] )[0]\n",
    "    sel = np.where( object_name[ii] == gaia_phot2[\"input_object_name\"] )[0]\n",
    "    if len(sel) > 0:\n",
    "        gaia_phot = vstack([gaia_phot , gaia_phot2[sel] ])\n",
    "    else:\n",
    "        tmp = Table( np.repeat(np.NaN , len(gaia_phot2.keys())) , names=gaia_phot2.keys() , dtype=gaia_phot2.dtype )\n",
    "        gaia_phot = vstack([gaia_phot , tmp ])\n",
    "        \n",
    "## Some cleanup:\n",
    "gaia_phot[\"source_id\"][gaia_phot[\"source_id\"] < 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "######## EXTRACT LIGHT CURVES ##########\n",
    "# Now since we have matched the objects to the Gaia catalog, we can also extract the full light curves using\n",
    "# the Gaia IDs.\n",
    "\n",
    "## Log in (apparently not necessary for small queries) =========\n",
    "#Gaia.login(user=None , password=None)\n",
    "\n",
    "\n",
    "## For each of the objects, request the EPOCH_PHOTOMETRY from the Gaia DataLink Service =======\n",
    "\n",
    "## Run search\n",
    "ids = list(gaia_phot[\"source_id\"])\n",
    "prod_tab = Gaia_retrieve_EPOCH_PHOTOMETRY(ids=ids , verbose=False)\n",
    "\n",
    "## Create light curves\n",
    "gaia_epoch_phot = Gaia_mk_lightcurves(prod_tab)\n",
    "\n",
    "## Create MultiBandTimeSeries photometry object (not used anymore)\n",
    "#gaia_multibandTS_phot = Gaia_mk_MultibandTimeSeries(epoch_phot = gaia_epoch_phot)\n",
    "\n",
    "## Create Gaia Pandas MultiIndex object and append to existing data frame.\n",
    "df_lc_gaia = Gaia_mk_MultiIndex(data=object_name,labels_list=labels_list , gaia_phot=gaia_phot , gaia_epoch_phot=gaia_epoch_phot , verbose = 1)\n",
    "\n",
    "## Append to existing MultiIndex object (if exists)\n",
    "#df_lc = MultiIndexDFObject()\n",
    "df_lc.append(df_lc_gaia)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MAKE FIGURE OF ONE LIGHT CURVE FOR GAIA ###\n",
    "\n",
    "## First get the ids/names of sources that have Gaia multi-epoch observations.\n",
    "object_ids = list(df_lc.data.index.levels[0]) # get list of objectids in multiIndex table\n",
    "print(object_ids)\n",
    "\n",
    "fig = plt.figure(figsize=(12,4))\n",
    "plt.subplots_adjust(wspace=0.3)\n",
    "axs = [ fig.add_subplot(1,3,ii+1) for ii in range(3) ]\n",
    "cmap = plt.get_cmap(\"Spectral\")\n",
    "\n",
    "for dd in object_ids:\n",
    "    try:\n",
    "    \n",
    "        for bb,band in enumerate([\"G\",\"BP\",\"RP\"]):\n",
    "        \n",
    "            this_tab = df_lc.data.loc[dd,:,\"Gaia {}\".format(band.lower()),:].reset_index(inplace=False)\n",
    "            t = Time(this_tab[\"time\"] , format=\"mjd\") # convert to time object\n",
    "            #axs[bb].plot(t.mjd , this_tab[\"flux\"] , \"-\" , linewidth=1 , markersize=0.1)\n",
    "            axs[bb].errorbar(t.mjd , this_tab[\"flux\"] , yerr=this_tab[\"err\"] , fmt=\"-o\",linewidth=0.5 , markersize=3 , label=\"{}\".format(dd))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "for ii in range(3):\n",
    "    axs[ii].set_title(np.asarray([\"G\",\"BP\",\"RP\"])[ii])\n",
    "    axs[ii].legend(fontsize=6 , ncol=3)\n",
    "    axs[ii].set_xlabel(\"MJD (Days)\" , fontsize=10)\n",
    "    axs[ii].set_ylabel(r\"Flux ($\\mu$Jy)\", fontsize=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 ASAS-SN (all sky automated survey for supernovae) \n",
    "- Has a [website](https://asas-sn.osu.edu/photometry) that can be manually searched; but no API which would allow automatic searches from within this notebook\n",
    "- Magnitude range of this survey is not consistent with the magnitude range of our CLAGN.  If this catalog becomes scientifically interesting, one path forward would be to ask ASAS-SN team about implementing an API\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Icecube Neutrinos\n",
    "\n",
    "There are several [catalogs](https://icecube.wisc.edu/data-releases/2021/01/all-sky-point-source-icecube-data-years-2008-2018) (basically one for each year of IceCube data from 2008 - 2018). The following code creates a large catalog by combining\n",
    "all the yearly catalogs.\n",
    "The IceCube catalog contains Neutrino detections with associated energy and time and approximate direction (which is uncertain by half-degree scales....). Usually, for active events only one or two Neutrinos are detected, which makes matching quite different compared to \"photons\". For our purpose, we will list the top 3 events in energy that are within a given distance to the target.\n",
    "\n",
    "This time series (time vs. neutrino energy) information is similar to photometry. We choose to storing time and energy in our data structure, leaving error = 0. What is __not__ stored in this format is the distance or angular uncertainty of the event direction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#### LOAD EVENTS ####\n",
    "# This loads the IceCube catalog, which is dispersed in different files.\n",
    "# Each file has a list of events with their energy, time, and approximate direction.\n",
    "icecube_events , _ = get_icecube_catalog(path=\"./data/icecube/icecube_10year_ps/\")\n",
    "\n",
    "# sort by Neutrino energy\n",
    "icecube_events.sort(keys=\"energy_logGeV\" , reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MATCH OBJECTS ###\n",
    "# Here we match the objects to the IceCube catalog to extract the N highest energy events close\n",
    "# to the objects' coordinates. We also want to include the errors in position of the IceCube\n",
    "# events.\n",
    "\n",
    "## Top N (in energy) events to selected\n",
    "icecube_select_topN = 3\n",
    "\n",
    "## create SkyCoord objects from event coordinates\n",
    "c2 = SkyCoord(icecube_events[\"ra\"], icecube_events[\"dec\"], unit=\"deg\", frame='icrs')\n",
    "\n",
    "## Match\n",
    "icecube_matches = []\n",
    "icecube_matched = []\n",
    "ii = 0\n",
    "for cc,coord in enumerate(coords_list):\n",
    "\n",
    "    # get all distances\n",
    "    dist_angle =  coord.separation(c2)\n",
    "    \n",
    "    # make selection: here we have to also include errors on the\n",
    "    # angles somehow.\n",
    "    sel = np.where( (dist_angle.to(u.degree).value - icecube_events[\"AngErr\"]) <= 0.0)[0]\n",
    "    #print(len(sel))\n",
    "\n",
    "    # select the top N events in energy. Note that we already sorted the table\n",
    "    # by energy_logGeV. Hence we only have to pick the top N here.\n",
    "    if len(sel) < icecube_select_topN:\n",
    "        this_topN = len(sel)\n",
    "    else:\n",
    "        this_topN = icecube_select_topN * 1\n",
    "    \n",
    "    if len(sel) > 0:\n",
    "        icecube_matches.append(icecube_events[sel[0:this_topN]])\n",
    "        icecube_matches[ii][\"Ang_match\"] = dist_angle.to(u.degree).value[sel[0:this_topN]]\n",
    "        icecube_matches[ii][\"Ang_match\"].unit = u.degree\n",
    "        icecube_matched.append(cc)\n",
    "        \n",
    "        ii += 1\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        pass # no match found\n",
    "        print(\"No match found.\")\n",
    "\n",
    "        \n",
    "## Add to lightcurve object:\n",
    "ii = 0\n",
    "for cc,coord in enumerate(tqdm(coords_list)):\n",
    "    lab = labels_list[cc]\n",
    "    if cc in icecube_matched:\n",
    "        ## Create single instance\n",
    "        dfsingle = pd.DataFrame(\n",
    "                                dict(flux=np.asarray(icecube_matches[ii][\"energy_logGeV\"]), # in log GeV\n",
    "                                 err=np.repeat(0,len(icecube_matches[ii])), # in mJy\n",
    "                                 time=np.asarray(icecube_matches[ii][\"mjd\"]), # in MJD\n",
    "                                 #objectid=gaia_phot[\"input_object_name\"][ii],\n",
    "                                 objectid=np.repeat(cc+1, len(icecube_matches[ii])),label=lab,\n",
    "                                 band=\"IceCube\"\n",
    "                                    )\n",
    "                    ).set_index([\"objectid\", \"label\", \"band\", \"time\"])\n",
    "\n",
    "        ## Append\n",
    "        df_lc.append(dfsingle)\n",
    "        \n",
    "        ii += 1\n",
    "    \n",
    "print(\"IceCube Matched and added to lightcurve object.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save or load the data frame\n",
    "if SAVEDF:\n",
    "    df_lc.pickle(pickle_file_name)\n",
    "    print(\"Pickle file saved!\")\n",
    "else:\n",
    "    df_lc = MultiIndexDFObject()\n",
    "    #df_lc.load_pickle(\"data/dflc.pkl\")\n",
    "    df_lc.load_pickle(pickle_file_name)\n",
    "    print(\"Pickle file loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Make plots of luminosity as a function of time\n",
    "- model plots after [van Velzen et al., 2021](https://arxiv.org/pdf/2111.09391.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for ccount, coord in enumerate(coords_list):\n",
    "    singleobj = df_lc.data.loc[(ccount+1),:,:]\n",
    "\n",
    "    # Set up for plotting. We use the \"mosaic\" method so we can plot\n",
    "    # the ZTF data in a subplot for better visibility.\n",
    "    fig, axes = plt.subplot_mosaic(mosaic=[[\"A\"],[\"A\"],[\"B\"]] , figsize=(10,8))\n",
    "    plt.subplots_adjust(hspace=0.3 , wspace=0.3)\n",
    "\n",
    "    # First check to see which bands we have in the dataframe\n",
    "    availband = singleobj.index.unique('band')\n",
    "    \n",
    "\n",
    "    # Plot all the bands in the *main plot* (A)\n",
    "    leg_handles_A = []\n",
    "    max_list = [] # store maximum flux for each band\n",
    "    ztf_minmax_tab = Table(names=[\"tmin\",\"tmax\",\"fluxmin\",\"fluxmax\"]) # store the min and max of the ZTF band fluxes and time\n",
    "    has_ztf = False # flag to set to True if ZTF data is available.\n",
    "    for l in range(len(availband)):\n",
    "        band_lc = singleobj.loc[ccount+1,:, availband[l], :]\n",
    "        #band_lc = singleobj.loc[availband[l], :] # above line doesn't work for me [ALF]\n",
    "        band_lc.reset_index(inplace = True)\n",
    "\n",
    "        # first clean dataframe to remove erroneous rows\n",
    "        band_lc_clean = band_lc[band_lc['time'] < 65000]\n",
    "\n",
    "        #before plotting need to scale the Kepler, K2, and TESS fluxes to the other available fluxes\n",
    "        if availband[l] in ['Kepler', 'K2', 'TESS']:\n",
    "            #remove outliers in the dataset\n",
    "            bandlc_clip = band_lc_clean[(np.abs(stats.zscore(band_lc_clean['flux'])) < 3.0)]\n",
    "\n",
    "            #find the maximum value of 'other bands'\n",
    "            max_electrons = max(band_lc_clean.flux)\n",
    "            factor = np.mean(max_list)/ max_electrons\n",
    "            lh = axes[\"A\"].errorbar(bandlc_clip.time, bandlc_clip.flux * factor, bandlc_clip.err* factor,\n",
    "                                    capsize = 3.0,label = availband[l])\n",
    "        elif availband[l] in ['zg','zr','zi']:\n",
    "            max_list.append(max(band_lc_clean.flux)) \n",
    "            lh = axes[\"A\"].errorbar(band_lc_clean.time, band_lc_clean.flux, band_lc_clean.err,\n",
    "                                    capsize = 1.0, elinewidth=0.5,marker='o',markersize=2,linestyle='', label = 'ZTF '+str(availband[l]))\n",
    "            ztf_minmax_tab.add_row( [np.min(band_lc_clean.time) , np.max(band_lc_clean.time) , np.min(band_lc_clean.flux) , np.max(band_lc_clean.flux) ] )\n",
    "            has_ztf = True\n",
    "            \n",
    "            axes[\"B\"].errorbar(band_lc_clean.time, band_lc_clean.flux, band_lc_clean.err,\n",
    "                                    capsize = 1.0, elinewidth=0.5, marker='o',linestyle='',markersize=0.5,  label = 'ZTF '+str(availband[l]))\n",
    "            \n",
    "        elif availband[l] in [\"IceCube\"]:\n",
    "            y = axes[\"A\"].get_ylim()[0] + np.diff(axes[\"A\"].get_ylim())*0.7\n",
    "            dy = np.diff(axes[\"A\"].get_ylim())/20\n",
    "            lh = axes[\"A\"].errorbar(band_lc_clean.time , np.repeat(y , len(band_lc_clean.time)) , yerr=dy, uplims=True ,\n",
    "                                    fmt=\"o\"  , label=availband[l] , color=\"black\")\n",
    "            \n",
    "        else:\n",
    "            max_list.append(max(band_lc_clean.flux)) \n",
    "            lh = axes[\"A\"].errorbar(band_lc_clean.time, band_lc_clean.flux, band_lc_clean.err,\n",
    "                                    capsize = 3.0, label = availband[l])\n",
    "\n",
    "        leg_handles_A.append(lh)\n",
    "    \n",
    "    # Plot the ZTF bands in a separate plot to show their variability\n",
    "    # more clearly. Can still also plot the rest, just change the x and\n",
    "    # y axis limits. Only do this if ZTF is available for source.\n",
    "\n",
    "    ## Do Axes\n",
    "    \n",
    "    #axes[\"A\"].spines['top'].set_visible(False)\n",
    "    #axes[\"A\"].spines['right'].set_visible(False)\n",
    "    axes[\"A\"].set_ylabel('Flux(mJy)')\n",
    "    \n",
    "    axes[\"B\"].set_ylabel('Flux(mJy)')\n",
    "    axes[\"B\"].set_xlabel('Time(MJD)')\n",
    "    \n",
    "    axes[\"B\"].set_xlim( np.min(ztf_minmax_tab[\"tmin\"])-100 , np.max(ztf_minmax_tab[\"tmax\"])+100 )\n",
    "    \n",
    "    \n",
    "    plt.legend(handles=leg_handles_A , bbox_to_anchor=(1.4,3.5))\n",
    "    plt.tight_layout()\n",
    "    #save the plot to data/*.pdf\n",
    "    savename = \"data/lightcurve_{}.pdf\".format(ccount+1)\n",
    "    plt.savefig(savename, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    \n",
    "## TODO:\n",
    "## - add running median to ZTF lightcurve. [ALF]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Extension \n",
    "Consider training a ML model to do light curve classification based on this sample of CLAGN\n",
    " - once we figure out which bands these are likely to be observed in, could then have a optical + IR light curve classifier\n",
    " - what would the features of the light curve be?\n",
    " - what models are reasonable to test as light curve classifiers?\n",
    " - could we make also a sample of TDEs, SNe, flaring AGN? - then train the model to distinguish between these things?\n",
    " - need a sample of non-flaring light curves\n",
    " \n",
    "After training the model:\n",
    " - would then need a sample of optical + IR light curves for \"all\" galaxies = big data to run the model on.\n",
    "\n",
    "Some resources to consider:\n",
    "- https://github.com/dirac-institute/ZTF_Boyajian\n",
    "- https://ui.adsabs.harvard.edu/abs/2022AJ....164...68S/abstract\n",
    "- https://ui.adsabs.harvard.edu/abs/2019ApJ...881L...9F/abstract\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citations\n",
    "\n",
    "to the various codes used (astroquery etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
